{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vavv6xxd73V2",
        "outputId": "c6940ba5-ab24-43be-9d92-ecc4cd5de76f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pandoc is already the newest version (2.9.2.1-3ubuntu2).\n",
            "texlive-fonts-recommended is already the newest version (2021.20220204-1).\n",
            "texlive-plain-generic is already the newest version (2021.20220204-1).\n",
            "texlive-xetex is already the newest version (2021.20220204-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "pandoc 2.9.2.1\n",
            "Compiled with pandoc-types 1.20, texmath 0.12.0.2, skylighting 0.8.5\n",
            "Default user data directory: /root/.local/share/pandoc or /root/.pandoc\n",
            "Copyright (C) 2006-2020 John MacFarlane\n",
            "Web:  https://pandoc.org\n",
            "This is free software; see the source for copying conditions.\n",
            "There is no warranty, not even for merchantability or fitness\n",
            "for a particular purpose.\n"
          ]
        }
      ],
      "source": [
        "# Instalación de dependencias necesarias (se recomienda ejecutar estas líneas solo una vez)\n",
        "!pip install -q backoff python-docx nltk PyPDF2 markdown-it-py pypandoc\n",
        "!apt-get install -y pandoc texlive-xetex texlive-fonts-recommended texlive-plain-generic\n",
        "!pandoc --version\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWM2yceO_-hg"
      },
      "source": [
        "# Configuración inicial\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx-YoSqz7JJm",
        "outputId": "98bf3dd4-2ed9-4237-f2c3-d35e6bd76c5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Es recomendable almacenar la API Key en Secrets de Colab y no exponerla directamente en el código.\n",
        "os.environ['GOOGLE_API_KEY'] = 'AIzaSyD8Vp2fT5rKPgTJV40ekUgShJd7vkHWqHA'\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Clase de configuración para el agente educativo con IA\n",
        "class Config:\n",
        "    \"\"\"\n",
        "    Configuración para el modelo y parámetros del agente educativo.\n",
        "\n",
        "    Atributos:\n",
        "        gemini_model (str): Modelo de generación a utilizar.\n",
        "        llm_tokens_per_minute (int): Cantidad de tokens procesados por minuto.\n",
        "        llm_max_tokens_per_request (int): Máximo de tokens permitidos por solicitud.\n",
        "        prompt_templates_path (str): Ruta al archivo de plantillas de prompts.\n",
        "        readability_threshold (float): Umbral de legibilidad para el contenido generado.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        gemini_model: str = 'gemini-2.0-flash-001',\n",
        "        llm_tokens_per_minute: int = 50000,\n",
        "        llm_max_tokens_per_request: int = 4000,\n",
        "        prompt_templates_path: str = 'prompts.json',\n",
        "        readability_threshold: float = 60.0\n",
        "    ):\n",
        "        self.gemini_model = gemini_model\n",
        "        self.llm_tokens_per_minute = llm_tokens_per_minute\n",
        "        self.llm_max_tokens_per_request = llm_max_tokens_per_request\n",
        "        self.prompt_templates_path = prompt_templates_path\n",
        "        self.readability_threshold = readability_threshold\n",
        "\n",
        "# Inicialización del modelo utilizando la configuración predeterminada\n",
        "config = Config()\n",
        "model = genai.GenerativeModel(config.gemini_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmYEf6JW8YNl"
      },
      "source": [
        "# Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ZO_0R2kL8YAl"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Módulo para gestionar la interacción con la API de Google Gemini,\n",
        "optimizado para un agente educativo inteligente.\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict\n",
        "import google.generativeai as genai\n",
        "import backoff\n",
        "\n",
        "# Constante para el período de reinicio del contador (en segundos)\n",
        "RATE_LIMIT_RESET = 60\n",
        "\n",
        "# Configuración básica del logging para el módulo\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# =============================================================================\n",
        "# Configuración del LLM\n",
        "# =============================================================================\n",
        "@dataclass\n",
        "class Config:\n",
        "    gemini_model: str\n",
        "    llm_tokens_per_minute: int\n",
        "    llm_max_tokens_per_request: int\n",
        "    prompt_templates_path: str\n",
        "\n",
        "# =============================================================================\n",
        "# Limitador de Tasa (RateLimiter)\n",
        "# =============================================================================\n",
        "class RateLimiter:\n",
        "    \"\"\"\n",
        "    Controla la tasa de llamadas a la API para evitar exceder el límite\n",
        "    de tokens permitidos por petición.\n",
        "    \"\"\"\n",
        "    def __init__(self, tokens_per_minute: int, max_tokens_per_request: int):\n",
        "        self.tokens_per_minute = tokens_per_minute\n",
        "        self.max_tokens_per_request = max_tokens_per_request\n",
        "        self.tokens_used_in_minute = 0\n",
        "        self.last_reset = time.time()\n",
        "\n",
        "    def wait_if_needed(self, tokens: int) -> None:\n",
        "        \"\"\"\n",
        "        Espera si la suma de tokens usados y los solicitados excede el límite por minuto.\n",
        "\n",
        "        Args:\n",
        "            tokens (int): Número de tokens que se desean consumir en la petición.\n",
        "        \"\"\"\n",
        "        # Validar que no se soliciten más tokens de los permitidos en una petición\n",
        "        if tokens > self.max_tokens_per_request:\n",
        "            raise ValueError(\n",
        "                f\"El número de tokens solicitados ({tokens}) excede el máximo permitido por petición \"\n",
        "                f\"({self.max_tokens_per_request}).\"\n",
        "            )\n",
        "\n",
        "        now = time.time()\n",
        "        # Reiniciar el contador si ha pasado el período definido\n",
        "        if now - self.last_reset >= RATE_LIMIT_RESET:\n",
        "            self.tokens_used_in_minute = 0\n",
        "            self.last_reset = now\n",
        "\n",
        "        # Si se excede el límite, se espera hasta que se reinicie el contador\n",
        "        if self.tokens_used_in_minute + tokens > self.tokens_per_minute:\n",
        "            time_to_wait = RATE_LIMIT_RESET - (now - self.last_reset)\n",
        "            if time_to_wait > 0:\n",
        "                logger.info(\n",
        "                    f\"Limitador de tasa: esperando {time_to_wait:.2f} segundos para cumplir con el límite de tokens.\"\n",
        "                )\n",
        "                time.sleep(time_to_wait)\n",
        "                # Reiniciar el contador tras la espera\n",
        "                self.tokens_used_in_minute = tokens\n",
        "                self.last_reset = time.time()\n",
        "        else:\n",
        "            self.tokens_used_in_minute += tokens\n",
        "\n",
        "# =============================================================================\n",
        "# LLMEngine: Interfaz para la API de Google Gemini\n",
        "# =============================================================================\n",
        "class LLMEngine:\n",
        "    \"\"\"\n",
        "    Interfaz para interactuar con la API de Google Gemini, generando contenido educativo\n",
        "    de alta calidad.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        \"\"\"\n",
        "        Inicializa el motor LLM con la configuración proporcionada.\n",
        "\n",
        "        Args:\n",
        "            config (Config): Instancia de Config con los parámetros necesarios.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logger.getChild(\"llm_engine\")\n",
        "\n",
        "        # Verificar la existencia de la variable de entorno con la clave API\n",
        "        api_key = os.environ.get('GOOGLE_API_KEY')\n",
        "        if not api_key:\n",
        "            self.logger.error(\"La variable de entorno 'GOOGLE_API_KEY' no está definida.\")\n",
        "            raise EnvironmentError(\"La variable de entorno 'GOOGLE_API_KEY' es requerida.\")\n",
        "\n",
        "        # Configurar la API de Google Gemini\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel(self.config.gemini_model)\n",
        "\n",
        "        # Inicializar el limitador de tasa\n",
        "        self.rate_limiter = RateLimiter(\n",
        "            tokens_per_minute=self.config.llm_tokens_per_minute,\n",
        "            max_tokens_per_request=self.config.llm_max_tokens_per_request\n",
        "        )\n",
        "\n",
        "        # Cargar las plantillas de prompts\n",
        "        self.prompt_templates = self._load_prompt_templates()\n",
        "\n",
        "        # Instrucción base del sistema, mejorada para la generación de contenido educativo\n",
        "        self.system_instruction = (\n",
        "            \"Eres un asistente educativo experto en la creación de materiales didácticos de alta calidad para cursos universitarios. \"\n",
        "            \"Tu tarea es generar contenido claro, preciso y bien estructurado, con rigor académico y ejemplos pertinentes, \"\n",
        "            \"adaptado al nivel de profundidad que se solicita. Responde siempre de forma organizada y autoexplicativa, \"\n",
        "            \"para que sea fácil de entender y aplicar.\"\n",
        "        )\n",
        "\n",
        "    def _load_prompt_templates(self) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Carga las plantillas de prompts desde un archivo JSON.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, str]: Diccionario con las plantillas de prompts.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(self.config.prompt_templates_path, 'r', encoding='utf-8') as f:\n",
        "                templates = json.load(f)\n",
        "                self.logger.info(\"Plantillas de prompts cargadas correctamente.\")\n",
        "                return templates\n",
        "        except Exception as e:\n",
        "            self.logger.warning(\n",
        "                f\"No se pudieron cargar las plantillas desde {self.config.prompt_templates_path}: {str(e)}. \"\n",
        "                \"Se usarán plantillas por defecto.\"\n",
        "            )\n",
        "            return {\n",
        "                \"lecture_notes\": (\n",
        "                    \"Genera notas de clase detalladas para el tema: {topic_title}. \"\n",
        "                    \"Incluye los siguientes subtemas: {subtopics}. \"\n",
        "                    \"Las notas deben incluir definiciones, explicaciones claras, ejemplos y casos de aplicación.\"\n",
        "                ),\n",
        "                \"practice_problems\": (\n",
        "                    \"Crea problemas de práctica con soluciones paso a paso para el tema: {topic_title}. \"\n",
        "                    \"Los problemas deben cubrir: {subtopics}. \"\n",
        "                    \"Incluye problemas de distintos niveles de dificultad.\"\n",
        "                ),\n",
        "                \"discussion_questions\": (\n",
        "                    \"Genera preguntas para discusión sobre el tema: {topic_title}, considerando: {subtopics}. \"\n",
        "                    \"Las preguntas deben promover el pensamiento crítico y el análisis profundo.\"\n",
        "                ),\n",
        "                \"learning_objectives\": (\n",
        "                    \"Crea objetivos de aprendizaje específicos y medibles para el tema: {topic_title}. \"\n",
        "                    \"Considera los siguientes subtemas: {subtopics}. \"\n",
        "                    \"Usa verbos de la taxonomía de Bloom apropiados.\"\n",
        "                ),\n",
        "                \"suggested_resources\": (\n",
        "                    \"Sugiere recursos de aprendizaje adicionales para el tema: {topic_title}. \"\n",
        "                    \"Incluye libros, artículos, videos, cursos en línea y otros materiales relevantes para: {subtopics}.\"\n",
        "                )\n",
        "            }\n",
        "\n",
        "    @backoff.on_exception(\n",
        "        backoff.expo,\n",
        "        Exception,\n",
        "        max_tries=3,\n",
        "        jitter=backoff.full_jitter,\n",
        "        logger=logger.getChild(\"llm_engine\")\n",
        "    )\n",
        "    def generate_content(self, prompt: str, max_tokens: int = 1000) -> str:\n",
        "        \"\"\"\n",
        "        Genera contenido educativo llamando a la API de Google Gemini.\n",
        "\n",
        "        Args:\n",
        "            prompt (str): Instrucción detallada para el modelo.\n",
        "            max_tokens (int, optional): Número máximo de tokens para la respuesta. Por defecto es 1000.\n",
        "\n",
        "        Returns:\n",
        "            str: Texto generado por el modelo.\n",
        "        \"\"\"\n",
        "        self.logger.debug(f\"Generando contenido con prompt (primeros 100 caracteres): {prompt[:100]}...\")\n",
        "\n",
        "        # Estimar tokens del prompt (aproximación simple)\n",
        "        prompt_tokens = len(prompt.split())\n",
        "\n",
        "        # Calcular el total de tokens solicitados (prompt + respuesta)\n",
        "        total_tokens = prompt_tokens + max_tokens\n",
        "\n",
        "        # Ajuste dinámico: si total_tokens supera el límite, reducir max_tokens\n",
        "        if total_tokens > self.rate_limiter.max_tokens_per_request:\n",
        "            available_tokens = self.rate_limiter.max_tokens_per_request - prompt_tokens\n",
        "            if available_tokens <= 0:\n",
        "                raise ValueError(\"El prompt es demasiado largo y no deja espacio para la respuesta.\")\n",
        "            self.logger.info(\n",
        "                f\"Ajustando max_tokens de {max_tokens} a {available_tokens} para cumplir el límite.\"\n",
        "            )\n",
        "            max_tokens = available_tokens\n",
        "            total_tokens = prompt_tokens + max_tokens\n",
        "\n",
        "        # Esperar si es necesario para cumplir con los límites de tokens\n",
        "        self.rate_limiter.wait_if_needed(total_tokens)\n",
        "\n",
        "        try:\n",
        "            # Combinar la instrucción del sistema con el prompt del usuario\n",
        "            full_prompt = f\"{self.system_instruction}\\n\\n{prompt}\"\n",
        "\n",
        "            # Configuración para la generación de contenido\n",
        "            generation_config = {\n",
        "                \"max_output_tokens\": max_tokens,\n",
        "                \"temperature\": 0.7,\n",
        "                \"top_p\": 0.9,\n",
        "                \"top_k\": 40\n",
        "            }\n",
        "\n",
        "            self.logger.debug(\"Llamando a la API de Gemini con la configuración definida.\")\n",
        "            response = self.model.generate_content(\n",
        "                contents=[{\"role\": \"user\", \"parts\": [{\"text\": full_prompt}]}],\n",
        "                generation_config=generation_config\n",
        "            )\n",
        "\n",
        "            # Extraer y retornar el texto generado\n",
        "            generated_text = response.text.strip() if response.text else \"\"\n",
        "            self.logger.debug(\"Contenido generado exitosamente.\")\n",
        "            return generated_text\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error al generar contenido con Gemini: {str(e)}\", exc_info=True)\n",
        "            raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrYmFgJEAU3F"
      },
      "source": [
        "# Generador de contenido educativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nf8Ai4PG8eBg"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import textwrap\n",
        "from typing import Dict, Any\n",
        "\n",
        "class ContentGenerator:\n",
        "    \"\"\"\n",
        "    Genera contenido educativo utilizando un motor LLM.\n",
        "    \"\"\"\n",
        "    # Constante para definir límites específicos de tokens por tipo de contenido\n",
        "    TOKEN_LIMITS: Dict[str, int] = {\n",
        "        \"lecture_notes\": 4000,\n",
        "        \"practice_problems\": 3000,\n",
        "        \"discussion_questions\": 2000,\n",
        "        \"learning_objectives\": 1500,\n",
        "        \"suggested_resources\": 2000\n",
        "    }\n",
        "\n",
        "    def __init__(self, llm_engine: Any, config: Dict[str, Any]) -> None:\n",
        "        \"\"\"\n",
        "        Inicializa el generador de contenido.\n",
        "\n",
        "        Args:\n",
        "            llm_engine: Instancia del motor LLM para generación de contenido.\n",
        "            config: Configuración general del sistema.\n",
        "        \"\"\"\n",
        "        self.llm_engine = llm_engine\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(\"educational_agent.content_generator\")\n",
        "\n",
        "    def generate_all_materials(self, syllabus_data: Dict[str, Any]) -> Dict[str, Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Genera todos los materiales didácticos para cada tema del programa.\n",
        "\n",
        "        Args:\n",
        "            syllabus_data: Datos estructurados del programa del curso.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario que asocia cada tema (por ID) a un sub-diccionario con los diferentes tipos de contenido.\n",
        "        \"\"\"\n",
        "        all_content: Dict[str, Dict[str, str]] = {}\n",
        "\n",
        "        try:\n",
        "            course_context = {\n",
        "                \"course_title\": syllabus_data[\"course_title\"],\n",
        "                \"course_code\": syllabus_data[\"course_code\"],\n",
        "                \"course_description\": syllabus_data[\"description\"],\n",
        "                \"course_objectives\": syllabus_data[\"objectives\"]\n",
        "            }\n",
        "        except KeyError as e:\n",
        "            self.logger.error(f\"Clave faltante en syllabus_data: {e}\")\n",
        "            raise\n",
        "\n",
        "        for topic in syllabus_data.get(\"topics\", []):\n",
        "            topic_id = topic.get(\"id\")\n",
        "            topic_title = topic.get(\"title\", \"Tema sin título\")\n",
        "            self.logger.info(f\"Generando contenido para el tema {topic_id}: {topic_title}\")\n",
        "\n",
        "            # Se genera el contenido para cada tipo utilizando métodos auxiliares\n",
        "            topic_content = {\n",
        "                \"lecture_notes\": self._generate_content_with_context(\n",
        "                    topic, course_context, \"lecture_notes\", self._lecture_notes_context(course_context)\n",
        "                ),\n",
        "                \"practice_problems\": self._generate_content_with_context(\n",
        "                    topic, course_context, \"practice_problems\", self._practice_problems_context(course_context)\n",
        "                ),\n",
        "                \"discussion_questions\": self._generate_content_with_context(\n",
        "                    topic, course_context, \"discussion_questions\", self._discussion_questions_context(course_context)\n",
        "                ),\n",
        "                \"learning_objectives\": self._generate_content_with_context(\n",
        "                    topic, course_context, \"learning_objectives\", self._learning_objectives_context(course_context)\n",
        "                ),\n",
        "                \"suggested_resources\": self._generate_content_with_context(\n",
        "                    topic, course_context, \"suggested_resources\", self._suggested_resources_context(course_context)\n",
        "                )\n",
        "            }\n",
        "            all_content[topic_id] = topic_content\n",
        "\n",
        "        return all_content\n",
        "\n",
        "    def _generate_content_with_context(\n",
        "        self,\n",
        "        topic: Dict[str, Any],\n",
        "        course_context: Dict[str, Any],\n",
        "        template_key: str,\n",
        "        additional_context: str,\n",
        "        default_max_tokens: int = 2000\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Construye y genera contenido a partir de una plantilla y un contexto adicional.\n",
        "\n",
        "        Args:\n",
        "            topic: Datos del tema.\n",
        "            course_context: Datos generales del curso.\n",
        "            template_key: Clave para obtener la plantilla desde llm_engine.prompt_templates.\n",
        "            additional_context: Texto adicional que se agregará al prompt.\n",
        "            default_max_tokens: Límite de tokens en caso de que no se defina uno específico.\n",
        "\n",
        "        Returns:\n",
        "            Contenido generado como cadena de texto.\n",
        "        \"\"\"\n",
        "        prompt_template: str = self.llm_engine.prompt_templates.get(template_key, \"\")\n",
        "        if not prompt_template:\n",
        "            self.logger.warning(f\"No se encontró plantilla para {template_key}.\")\n",
        "\n",
        "        subtopics = topic.get(\"subtopics\", [])\n",
        "        subtopics_text = \", \".join(subtopics)\n",
        "        prompt = prompt_template.format(\n",
        "            topic_title=topic.get(\"title\", \"\"),\n",
        "            subtopics=subtopics_text\n",
        "        )\n",
        "\n",
        "        # Se combina el contexto adicional con la plantilla ya completada\n",
        "        final_prompt = textwrap.dedent(f\"\"\"\n",
        "            {additional_context.strip()}\n",
        "\n",
        "            {prompt.strip()}\n",
        "        \"\"\")\n",
        "        self.logger.debug(f\"Prompt para {template_key}: {final_prompt[:150]}...\")\n",
        "\n",
        "        max_tokens = self.TOKEN_LIMITS.get(template_key, default_max_tokens)\n",
        "\n",
        "        return self.llm_engine.generate_content(final_prompt, max_tokens=max_tokens)\n",
        "\n",
        "    def _lecture_notes_context(self, course_context: Dict[str, Any]) -> str:\n",
        "        \"\"\"Retorna el contexto adicional para generar notas de clase.\"\"\"\n",
        "        return textwrap.dedent(f\"\"\"\n",
        "            Información del curso:\n",
        "            - Título: {course_context.get('course_title', '')}\n",
        "            - Código: {course_context.get('course_code', '')}\n",
        "            - Descripción: {course_context.get('course_description', '')}\n",
        "\n",
        "            Genera notas de clase completas y detalladas que cubran el tema a profundidad.\n",
        "            Emplea rigor académico, definiciones precisas y ejemplos concretos para cada concepto.\n",
        "            Asegúrate de explicar la relevancia práctica y posibles aplicaciones.\n",
        "            Organiza la información de manera clara y estructurada, de modo que sea fácilmente entendible.\n",
        "        \"\"\")\n",
        "\n",
        "    def _practice_problems_context(self, course_context: Dict[str, Any]) -> str:\n",
        "        \"\"\"Retorna el contexto adicional para generar problemas de práctica.\"\"\"\n",
        "        return textwrap.dedent(f\"\"\"\n",
        "            Para el curso: {course_context.get('course_title', '')} ({course_context.get('course_code', '')})\n",
        "\n",
        "            Genera un conjunto de problemas de práctica que cubran todos los aspectos importantes del tema.\n",
        "            Asegúrate de incluir:\n",
        "            1. Al menos 5 problemas con distintos niveles de dificultad (básico, intermedio, avanzado).\n",
        "            2. Cada problema con una solución paso a paso detallada y justificada.\n",
        "            3. Preguntas conceptuales y aplicadas, integrando el contexto del curso.\n",
        "            4. Conexión a casos de la vida real o de la industria cuando sea pertinente.\n",
        "        \"\"\")\n",
        "\n",
        "    def _discussion_questions_context(self, course_context: Dict[str, Any]) -> str:\n",
        "        \"\"\"Retorna el contexto adicional para generar preguntas de discusión.\"\"\"\n",
        "        return textwrap.dedent(f\"\"\"\n",
        "            Para el curso: {course_context.get('course_title', '')}\n",
        "\n",
        "            Genera un conjunto de preguntas para discusión que:\n",
        "            1. Promuevan el pensamiento crítico y reflexivo.\n",
        "            2. Estimulen el debate y el intercambio de ideas entre los estudiantes.\n",
        "            3. Conecten el tema con problemas actuales o aplicaciones reales.\n",
        "            4. Exploren implicaciones éticas, sociales o económicas cuando sea oportuno.\n",
        "            5. Fomenten la conexión entre este tema y otros contenidos del curso.\n",
        "\n",
        "            Para cada pregunta, incluye una breve nota para el instructor sobre los puntos clave que podrían surgir en el debate.\n",
        "        \"\"\")\n",
        "\n",
        "    def _learning_objectives_context(self, course_context: Dict[str, Any]) -> str:\n",
        "        \"\"\"Retorna el contexto adicional para generar objetivos de aprendizaje.\"\"\"\n",
        "        course_objectives = course_context.get(\"course_objectives\", [])\n",
        "        objectives_str = \", \".join(course_objectives)\n",
        "        return textwrap.dedent(f\"\"\"\n",
        "            Para el curso: {course_context.get('course_title', '')}\n",
        "\n",
        "            Genera objetivos de aprendizaje que:\n",
        "            1. Sean específicos, medibles, alcanzables, relevantes y con un tiempo definido (SMART).\n",
        "            2. Utilicen verbos de acción de la taxonomía de Bloom apropiados para el nivel universitario.\n",
        "            3. Cubran diferentes niveles cognitivos (recordar, comprender, aplicar, analizar, evaluar, crear).\n",
        "            4. Se alineen con los objetivos generales del curso.\n",
        "            5. Sean claras y comprensibles para los estudiantes.\n",
        "\n",
        "            Los objetivos generales del curso incluyen: {objectives_str}.\n",
        "        \"\"\")\n",
        "\n",
        "    def _suggested_resources_context(self, course_context: Dict[str, Any]) -> str:\n",
        "        \"\"\"Retorna el contexto adicional para generar recursos sugeridos.\"\"\"\n",
        "        return textwrap.dedent(f\"\"\"\n",
        "            Para el curso: {course_context.get('course_title', '')}\n",
        "\n",
        "            Genera una lista de recursos de aprendizaje que incluya:\n",
        "            1. Libros de texto principales y complementarios (proporciona autores y años de publicación).\n",
        "            2. Artículos académicos relevantes y actualizados, si es posible de acceso abierto.\n",
        "            3. Recursos en línea de calidad (MOOCs, tutoriales, videos).\n",
        "            4. Herramientas o software relevantes cuando sea aplicable.\n",
        "            5. Recursos para diferentes niveles de conocimiento previo y enfoques de aprendizaje.\n",
        "\n",
        "            Para cada recurso, describe su relevancia y utilidad en relación al tema y al curso.\n",
        "        \"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZuu3gb6AbUc"
      },
      "source": [
        "# Extraer informacion de documentos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2C9mNYX29-gF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Módulo para procesar y extraer información estructurada de documentos\n",
        "(PDF, DOCX, TXT) que contienen programas de curso.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "from typing import Dict, List, Any\n",
        "import PyPDF2\n",
        "import docx\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Configuración del logger para este módulo\n",
        "logger = logging.getLogger(\"educational_agent.document_processor\")\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"\n",
        "    Procesa y extrae información estructurada de documentos que contienen programas de curso.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: \"Config\") -> None:\n",
        "        \"\"\"\n",
        "        Inicializa el procesador de documentos y descarga los recursos de NLTK necesarios.\n",
        "\n",
        "        Args:\n",
        "            config: Objeto de configuración con parámetros del sistema.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "\n",
        "        # Verificar y descargar el tokenizador 'punkt' de NLTK si no se encuentra disponible\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            self.logger.info(\"Descargando recursos NLTK necesarios...\")\n",
        "            nltk.download('punkt')\n",
        "\n",
        "    def process_file(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Procesa un archivo de programa de curso y extrae su estructura.\n",
        "\n",
        "        Args:\n",
        "            file_path: Ruta al archivo del programa.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con la información estructurada del curso.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"No se encontró el archivo: {file_path}\")\n",
        "\n",
        "        file_ext = os.path.splitext(file_path)[1].lower()\n",
        "        if file_ext == '.pdf':\n",
        "            text = self._extract_text_from_pdf(file_path)\n",
        "        elif file_ext == '.docx':\n",
        "            text = self._extract_text_from_docx(file_path)\n",
        "        elif file_ext == '.txt':\n",
        "            text = self._extract_text_from_txt(file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Formato de archivo no soportado: {file_ext}\")\n",
        "\n",
        "        syllabus_data = self._parse_syllabus(text)\n",
        "        return syllabus_data\n",
        "\n",
        "    def _extract_text_from_pdf(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Extrae y retorna el texto completo de un archivo PDF.\n",
        "\n",
        "        Args:\n",
        "            file_path: Ruta del archivo PDF.\n",
        "\n",
        "        Returns:\n",
        "            Texto extraído del PDF.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Extrayendo texto de PDF: {file_path}\")\n",
        "        texts = []\n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "                for page in reader.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        texts.append(page_text)\n",
        "            return \"\\n\".join(texts)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error al extraer texto de PDF: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _extract_text_from_docx(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Extrae y retorna el texto completo de un archivo DOCX.\n",
        "\n",
        "        Args:\n",
        "            file_path: Ruta del archivo DOCX.\n",
        "\n",
        "        Returns:\n",
        "            Texto extraído del DOCX.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Extrayendo texto de DOCX: {file_path}\")\n",
        "        try:\n",
        "            doc = docx.Document(file_path)\n",
        "            texts = [paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()]\n",
        "            return \"\\n\".join(texts)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error al extraer texto de DOCX: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _extract_text_from_txt(self, file_path: str) -> str:\n",
        "        \"\"\"\n",
        "        Extrae y retorna el texto completo de un archivo TXT.\n",
        "\n",
        "        Args:\n",
        "            file_path: Ruta del archivo TXT.\n",
        "\n",
        "        Returns:\n",
        "            Texto extraído del TXT.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Extrayendo texto de TXT: {file_path}\")\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                return file.read()\n",
        "        except UnicodeDecodeError:\n",
        "            with open(file_path, 'r', encoding='latin-1') as file:\n",
        "                return file.read()\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error al extraer texto de TXT: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _parse_syllabus(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analiza el texto completo del programa y extrae su estructura.\n",
        "\n",
        "        Args:\n",
        "            text: Texto completo del documento.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con información estructurada del curso.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Analizando la estructura del programa de curso\")\n",
        "        syllabus_data: Dict[str, Any] = {\n",
        "            \"course_title\": \"\",\n",
        "            \"course_code\": \"\",\n",
        "            \"instructor\": \"\",\n",
        "            \"description\": \"\",\n",
        "            \"objectives\": [],\n",
        "            \"topics\": [],\n",
        "            \"evaluation_methods\": [],\n",
        "            \"bibliography\": []\n",
        "        }\n",
        "\n",
        "        sections = self._split_into_sections(text)\n",
        "\n",
        "        syllabus_data[\"course_title\"] = self._extract_course_title(sections)\n",
        "        syllabus_data[\"course_code\"] = self._extract_course_code(sections)\n",
        "        syllabus_data[\"instructor\"] = self._extract_instructor(sections)\n",
        "        syllabus_data[\"description\"] = self._extract_description(sections)\n",
        "        syllabus_data[\"objectives\"] = self._extract_objectives(sections)\n",
        "        syllabus_data[\"topics\"] = self._extract_topics(sections)\n",
        "        syllabus_data[\"evaluation_methods\"] = self._extract_evaluation_methods(sections)\n",
        "        syllabus_data[\"bibliography\"] = self._extract_bibliography(sections)\n",
        "\n",
        "        return syllabus_data\n",
        "\n",
        "    def _split_into_sections(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Divide el texto en secciones basadas en encabezados comunes.\n",
        "\n",
        "        Args:\n",
        "            text: Texto completo del documento.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario donde las claves son los encabezados y los valores, el contenido asociado.\n",
        "        \"\"\"\n",
        "        section_headers = [\n",
        "            r\"(?:TÍTULO|NOMBRE)\\s+DEL\\s+CURSO\",\n",
        "            r\"(?:CÓDIGO|CLAVE)\",\n",
        "            r\"(?:PROFESOR|INSTRUCTOR|DOCENTE)\",\n",
        "            r\"(?:DESCRIPCIÓN|DESCRIPCION)\",\n",
        "            r\"(?:OBJETIVOS|METAS)\",\n",
        "            r\"(?:TEMARIO|CONTENIDO|PROGRAMA|UNIDADES)\",\n",
        "            r\"(?:EVALUACIÓN|EVALUACION|CALIFICACIÓN)\",\n",
        "            r\"(?:BIBLIOGRAFÍA|BIBLIOGRAFIA|REFERENCIAS)\"\n",
        "        ]\n",
        "\n",
        "        sections: Dict[str, str] = {\"preamble\": \"\"}\n",
        "        current_section = \"preamble\"\n",
        "        for line in text.splitlines():\n",
        "            stripped_line = line.strip()\n",
        "            is_header = False\n",
        "            for pattern in section_headers:\n",
        "                if re.search(pattern, stripped_line, re.IGNORECASE):\n",
        "                    current_section = stripped_line\n",
        "                    sections[current_section] = \"\"\n",
        "                    is_header = True\n",
        "                    break\n",
        "            if not is_header:\n",
        "                sections[current_section] += stripped_line + \"\\n\"\n",
        "        return sections\n",
        "\n",
        "    def _extract_course_title(self, sections: Dict[str, str]) -> str:\n",
        "        \"\"\"\n",
        "        Extrae el título del curso buscando encabezados o líneas en el preámbulo.\n",
        "\n",
        "        Args:\n",
        "            sections: Secciones del documento.\n",
        "\n",
        "        Returns:\n",
        "            Título del curso.\n",
        "        \"\"\"\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:TÍTULO|NOMBRE)\\s+DEL\\s+CURSO\", header, re.IGNORECASE):\n",
        "                return content.strip()\n",
        "        # Buscar en el preámbulo las primeras líneas no vacías\n",
        "        preamble_lines = sections.get(\"preamble\", \"\").splitlines()\n",
        "        for line in preamble_lines[:5]:\n",
        "            if line.strip():\n",
        "                return line.strip()\n",
        "        return \"No se pudo determinar el título del curso\"\n",
        "\n",
        "    def _extract_course_code(self, sections: Dict[str, str]) -> str:\n",
        "        \"\"\"\n",
        "        Extrae el código del curso mediante encabezados o patrones en el texto.\n",
        "\n",
        "        Args:\n",
        "            sections: Secciones del documento.\n",
        "\n",
        "        Returns:\n",
        "            Código del curso.\n",
        "        \"\"\"\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:CÓDIGO|CLAVE)\", header, re.IGNORECASE):\n",
        "                return content.strip()\n",
        "        for content in sections.values():\n",
        "            code_match = re.search(r\"\\b[A-Z]{2,4}\\s*\\d{3,4}\\b\", content)\n",
        "            if code_match:\n",
        "                return code_match.group(0).strip()\n",
        "        return \"No se pudo determinar el código del curso\"\n",
        "\n",
        "    def _extract_instructor(self, sections: Dict[str, str]) -> str:\n",
        "        \"\"\"\n",
        "        Extrae el nombre del instructor del curso.\n",
        "\n",
        "        Args:\n",
        "            sections: Secciones del documento.\n",
        "\n",
        "        Returns:\n",
        "            Nombre del instructor.\n",
        "        \"\"\"\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:PROFESOR|INSTRUCTOR|DOCENTE)\", header, re.IGNORECASE):\n",
        "                return content.strip()\n",
        "        return \"No se pudo determinar el instructor del curso\"\n",
        "\n",
        "    def _extract_description(self, sections: Dict[str, str]) -> str:\n",
        "        \"\"\"\n",
        "        Extrae la descripción del curso.\n",
        "\n",
        "        Args:\n",
        "            sections: Secciones del documento.\n",
        "\n",
        "        Returns:\n",
        "            Descripción del curso.\n",
        "        \"\"\"\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:DESCRIPCIÓN|DESCRIPCION)\", header, re.IGNORECASE):\n",
        "                return content.strip()\n",
        "        return \"No se encontró descripción del curso\"\n",
        "\n",
        "    def _extract_objectives(self, sections: Dict[str, str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extrae los objetivos del curso identificando listas o frases relevantes.\n",
        "\n",
        "        Args:\n",
        "            sections: Secciones del documento.\n",
        "\n",
        "        Returns:\n",
        "            Lista de objetivos.\n",
        "        \"\"\"\n",
        "        objectives: List[str] = []\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:OBJETIVOS|METAS)\", header, re.IGNORECASE):\n",
        "                for line in content.splitlines():\n",
        "                    line_clean = line.strip()\n",
        "                    if line_clean:\n",
        "                        # Detectar listas con viñetas, guiones o numeración\n",
        "                        if line_clean[0] in \"-•\" or re.match(r\"^\\d+\\.\", line_clean):\n",
        "                            objectives.append(line_clean.lstrip(\"-•0123456789. \").strip())\n",
        "                        elif len(line_clean) > 20:\n",
        "                            # Dividir en oraciones y agregar las suficientemente largas\n",
        "                            for sentence in sent_tokenize(line_clean):\n",
        "                                if len(sentence.strip()) > 20:\n",
        "                                    objectives.append(sentence.strip())\n",
        "        return objectives\n",
        "\n",
        "    def _extract_topics(self, sections: Dict[str, str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Extrae el temario o unidades del curso utilizando patrones formales y métodos alternativos.\n",
        "\n",
        "        Args:\n",
        "            sections: Secciones del documento.\n",
        "\n",
        "        Returns:\n",
        "            Lista de diccionarios, cada uno representando un tema o unidad.\n",
        "        \"\"\"\n",
        "        topics: List[Dict[str, Any]] = []\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:TEMARIO|CONTENIDO|PROGRAMA|UNIDADES)\", header, re.IGNORECASE):\n",
        "                # Intento mediante patrones formales: Unidad, Tema, Módulo o Capítulo\n",
        "                unit_pattern = re.compile(\n",
        "                    r\"(?:Unidad|Tema|Módulo|Capítulo)\\s+(\\d+|[IVXLCDM]+)[\\s:.]+(.+?)(?=(?:Unidad|Tema|Módulo|Capítulo)\\s+\\d+|$)\",\n",
        "                    re.IGNORECASE | re.DOTALL\n",
        "                )\n",
        "                for match in unit_pattern.finditer(content):\n",
        "                    unit_num = match.group(1)\n",
        "                    unit_text = match.group(2).strip()\n",
        "                    subtopics: List[str] = []\n",
        "                    lines = [l.strip() for l in unit_text.splitlines() if l.strip()]\n",
        "                    if lines:\n",
        "                        title = re.sub(r\"^\\d+\\.\\s*\", \"\", lines[0])\n",
        "                        for line in lines[1:]:\n",
        "                            if line[0] in \"-•\" or re.match(r\"^\\d+\\.\\d+\", line):\n",
        "                                subtopics.append(line.lstrip(\"-•0123456789. \").strip())\n",
        "                    else:\n",
        "                        title = unit_text.splitlines()[0] if unit_text else \"Sin título\"\n",
        "                        subtopics = [s.strip() for s in sent_tokenize(unit_text)[1:] if len(s.strip()) > 10]\n",
        "\n",
        "                    topics.append({\n",
        "                        \"id\": unit_num,\n",
        "                        \"title\": title,\n",
        "                        \"subtopics\": subtopics\n",
        "                    })\n",
        "                # Método alternativo si no se detectaron temas con el patrón formal\n",
        "                if not topics:\n",
        "                    numbered_lines = re.finditer(r\"^\\s*(\\d+)\\.\\s*(.+)$\", content, re.MULTILINE)\n",
        "                    current_topic = None\n",
        "                    for match in numbered_lines:\n",
        "                        num, text_line = match.group(1), match.group(2).strip()\n",
        "                        if len(num) == 1:\n",
        "                            current_topic = {\"id\": num, \"title\": text_line, \"subtopics\": []}\n",
        "                            topics.append(current_topic)\n",
        "                        elif current_topic is not None:\n",
        "                            current_topic[\"subtopics\"].append(text_line)\n",
        "                # Fallback: cada línea no vacía se considera un tema\n",
        "                if not topics:\n",
        "                    for idx, line in enumerate(content.splitlines(), start=1):\n",
        "                        line_clean = line.strip()\n",
        "                        if line_clean and len(line_clean) > 5:\n",
        "                            topics.append({\n",
        "                                \"id\": str(idx),\n",
        "                                \"title\": line_clean,\n",
        "                                \"subtopics\": []\n",
        "                            })\n",
        "        return topics\n",
        "\n",
        "    def _extract_evaluation_methods(self, sections: Dict[str, str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Extrae los métodos de evaluación y sus porcentajes.\n",
        "\n",
        "        Args:\n",
        "            sections: Secciones del documento.\n",
        "\n",
        "        Returns:\n",
        "            Lista de diccionarios con el método y su porcentaje.\n",
        "        \"\"\"\n",
        "        evaluation_methods: List[Dict[str, Any]] = []\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:EVALUACIÓN|EVALUACION|CALIFICACIÓN)\", header, re.IGNORECASE):\n",
        "                # Buscar patrones del tipo \"Examen: 30%\"\n",
        "                for match in re.finditer(r\"([^:]+):\\s*(\\d+)%\", content):\n",
        "                    method = match.group(1).strip()\n",
        "                    percentage = int(match.group(2))\n",
        "                    evaluation_methods.append({\"method\": method, \"percentage\": percentage})\n",
        "                # Alternativa: buscar líneas que contengan '%'\n",
        "                if not evaluation_methods:\n",
        "                    for line in content.splitlines():\n",
        "                        if '%' in line:\n",
        "                            parts = line.split('%')\n",
        "                            percentage_match = re.search(r\"(\\d+)\\s*$\", parts[0])\n",
        "                            if percentage_match:\n",
        "                                percentage = int(percentage_match.group(1))\n",
        "                                method = re.sub(r\"\\d+\\s*$\", \"\", parts[0]).strip()\n",
        "                                evaluation_methods.append({\"method\": method, \"percentage\": percentage})\n",
        "        return evaluation_methods\n",
        "\n",
        "    def _extract_bibliography(self, sections: Dict[str, str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extrae la bibliografía o referencias del curso.\n",
        "\n",
        "        Args:\n",
        "            sections: Secciones del documento.\n",
        "\n",
        "        Returns:\n",
        "            Lista de entradas bibliográficas.\n",
        "        \"\"\"\n",
        "        bibliography: List[str] = []\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:BIBLIOGRAFÍA|BIBLIOGRAFIA|REFERENCIAS)\", header, re.IGNORECASE):\n",
        "                entries = []\n",
        "                current_entry = \"\"\n",
        "                for line in content.splitlines():\n",
        "                    line = line.strip()\n",
        "                    if not line:\n",
        "                        if current_entry:\n",
        "                            entries.append(current_entry)\n",
        "                            current_entry = \"\"\n",
        "                    else:\n",
        "                        if not current_entry and (line[0] in \"-•\" or re.match(r\"^\\d+\\.\", line)):\n",
        "                            current_entry = line.lstrip(\"-•0123456789. \").strip()\n",
        "                        elif not current_entry:\n",
        "                            current_entry = line\n",
        "                        else:\n",
        "                            current_entry += \" \" + line\n",
        "                if current_entry:\n",
        "                    entries.append(current_entry)\n",
        "                # Fallback: usar líneas no vacías con longitud suficiente\n",
        "                bibliography = entries if entries else [l.strip() for l in content.splitlines() if l.strip() and len(l.strip()) > 10]\n",
        "        return bibliography\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ENrMH70A-BkF",
        "outputId": "ee1e88cd-9eaa-4005-cf19-f10cdd29d5a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:__main__.llm_engine:No se pudieron cargar las plantillas desde prompts.json: [Errno 2] No such file or directory: 'prompts.json'. Se usarán plantillas por defecto.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Por favor, sube el archivo del programa de curso (ej. PROGRAMA_DE_CURSO.pdf)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4245403d-3830-47e3-a646-e630a80050c2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4245403d-3830-47e3-a646-e630a80050c2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Programa estadistica.pdf to Programa estadistica (1).pdf\n",
            "Datos extraídos del syllabus:\n",
            "{\n",
            "    \"course_title\": \"Área o componente curricular:  Matemáticas\\nTipo de curso:  Teórico - práctico  Créditos académicos:  3\",\n",
            "    \"course_code\": \"\",\n",
            "    \"instructor\": \"2. INFORMACIÓN ESPECÍFICA\",\n",
            "    \"description\": \"La mayoría de los datos disponibles en la amplia gama de áreas del conocimiento, entre las cuales se encuentran las\\nciencias económicas, corresponden a datos observados que provienen de un fenómeno o ley aleatoria, la cual es de\\ngran importancia conocer co n el objetivo de obtener conclusiones, realizar contrastes de hipótesis, hacer\\npredicciones, tomar decisiones óptimas, entre muchas otras. No obstante, para poder afrontar dichos fines es\\nnecesario conocer y familiarizarse primero con los conceptos provist os por la teoría de la probabilidad y la estadística\\nmatemática. En este sentido, este curso está diseñado para proveer al estudiante con un sólido y bien balanceado\\nentendimiento de estos conceptos, tales como las nociones de probabilidad clásica, condici onamiento,\\nindependencia, variables aleatorias, funciones de distribución, esperanza matemática, entre otras. Aunque, el curso\",\n",
            "    \"objectives\": [\n",
            "        \"Que el estudiante adquiera los elementos básicos de la teoría de la probabilidad y sus aplicaciones.\",\n",
            "        \"las técnicas de la estadística descriptiva por el medio de manejo de calcula dora y el paquete estadístico (R ) en el\",\n",
            "        \"Lo anterior servirá como base para desarrollar posteriormente en la estadística II, la estadística\",\n",
            "        \"inferencial (estimación, prueba de hipótesis, predicción) y de algunas técnicas usadas en el muestreo estadístico.\",\n",
            "        \"Trabajar la estadística descriptiva por medio de las distribuciones de frecuencias y de gráficos. Utilizar el\",\n",
            "        \"computador  y la calculadora como herramientas adicionales y a la vez saber interpretar los resultados que estos\",\n",
            "        \"instrumentos arrojan.\",\n",
            "        \"Conocer las técnicas de conteo (análisis combinatorio) que facilitan el cálculo de las probabilidades.\",\n",
            "        \"Conocer las distribuciones de  probabilidades tanto discretas como continuas, proporcionando áreas de\",\n",
            "        \"aplicaciones a cada una de ellas.\"\n",
            "    ],\n",
            "    \"topics\": [\n",
            "        {\n",
            "            \"id\": \"1\",\n",
            "            \"title\": \"INFORMACIÓN GENERAL\",\n",
            "            \"subtopics\": []\n",
            "        },\n",
            "        {\n",
            "            \"id\": \"1\",\n",
            "            \"title\": \"y 2.\",\n",
            "            \"subtopics\": []\n",
            "        },\n",
            "        {\n",
            "            \"id\": \"3\",\n",
            "            \"title\": \" El tercer  examen, con un valor del 30 %, se realizará en la duodécima  semana. Se evaluará el tema de la\",\n",
            "            \"subtopics\": []\n",
            "        },\n",
            "        {\n",
            "            \"id\": \"4\",\n",
            "            \"title\": \"y 5.\",\n",
            "            \"subtopics\": []\n",
            "        },\n",
            "        {\n",
            "            \"id\": \"1\",\n",
            "            \"title\": \"5\",\n",
            "            \"subtopics\": []\n",
            "        }\n",
            "    ],\n",
            "    \"evaluation_methods\": [\n",
            "        {\n",
            "            \"method\": \"Parcial No.1\",\n",
            "            \"percentage\": 20\n",
            "        },\n",
            "        {\n",
            "            \"method\": \"Parcial No.2\",\n",
            "            \"percentage\": 20\n",
            "        },\n",
            "        {\n",
            "            \"method\": \"Parcial No.3\",\n",
            "            \"percentage\": 30\n",
            "        },\n",
            "        {\n",
            "            \"method\": \"Parcial No. 4\",\n",
            "            \"percentage\": 30\n",
            "        }\n",
            "    ],\n",
            "    \"bibliography\": [\n",
            "        \"R. Walpole, R. Myers, S. Myers, and K. Ye. Probabilidad y Estadística para Ingeniería y Ciencias. Pearson, New Jersey, 9na Edición, 2012\",\n",
            "        \"METODOLOGÍA\"\n",
            "    ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "# Definir los valores necesarios para la configuración\n",
        "gemini_model = 'gemini-2.0-flash-001'\n",
        "llm_tokens_per_minute = 50000\n",
        "llm_max_tokens_per_request = 4000\n",
        "prompt_templates_path = 'prompts.json'\n",
        "\n",
        "# Crear la instancia de Config con los argumentos requeridos\n",
        "config = Config(\n",
        "    gemini_model=gemini_model,\n",
        "    llm_tokens_per_minute=llm_tokens_per_minute,\n",
        "    llm_max_tokens_per_request=llm_max_tokens_per_request,\n",
        "    prompt_templates_path=prompt_templates_path\n",
        ")\n",
        "\n",
        "# Instanciar los componentes del sistema\n",
        "processor = DocumentProcessor(config)\n",
        "llm_engine = LLMEngine(config)\n",
        "generador = ContentGenerator(llm_engine, config)\n",
        "\n",
        "# Solicitar al usuario la carga del archivo\n",
        "print(\"Por favor, sube el archivo del programa de curso (ej. PROGRAMA_DE_CURSO.pdf)\")\n",
        "uploaded = files.upload()  # Abre el selector de archivos en Colab\n",
        "\n",
        "# Verificar que se haya subido al menos un archivo\n",
        "if not uploaded:\n",
        "    print(\"No se ha subido ningún archivo. Saliendo...\")\n",
        "\n",
        "    # Tomar el primer archivo subido\n",
        "file_path = list(uploaded.keys())[0]\n",
        "\n",
        "try:\n",
        "    # Procesar el archivo y extraer la información del syllabus\n",
        "    syllabus_data = processor.process_file(file_path)\n",
        "\n",
        "    # Mostrar los datos extraídos en formato JSON para mayor claridad\n",
        "    print(\"Datos extraídos del syllabus:\")\n",
        "    print(json.dumps(syllabus_data, indent=4, ensure_ascii=False))\n",
        "except Exception as e:\n",
        "    print(f\"Error al procesar el archivo: {e}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "toXPfNl0-mPN",
        "outputId": "ad534ece-3d07-4f01-cc29-ae380037bc53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:educational_agent.document_processor.llm_engine:No se pudieron cargar las plantillas desde prompts.json: [Errno 2] No such file or directory: 'prompts.json'. Se usarán plantillas por defecto.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Por favor, seleccione el archivo del programa de curso (ej. PROGRAMA_DE_CURSO.pdf)\n",
            "Error al procesar el archivo: no display name and no $DISPLAY environment variable\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import tkinter as tk\n",
        "from tkinter import filedialog\n",
        "\n",
        "def main():\n",
        "    # Configuración inicial\n",
        "    gemini_model = 'gemini-2.0-flash-001'\n",
        "    llm_tokens_per_minute = 50000\n",
        "    llm_max_tokens_per_request = 4000\n",
        "    prompt_templates_path = 'prompts.json'\n",
        "\n",
        "    # Crear la instancia de Config con los parámetros requeridos\n",
        "    config = Config(\n",
        "        gemini_model=gemini_model,\n",
        "        llm_tokens_per_minute=llm_tokens_per_minute,\n",
        "        llm_max_tokens_per_request=llm_max_tokens_per_request,\n",
        "        prompt_templates_path=prompt_templates_path\n",
        "    )\n",
        "\n",
        "    # Instanciar los componentes del sistema\n",
        "    processor = DocumentProcessor(config)\n",
        "    llm_engine = LLMEngine(config)\n",
        "    generador = ContentGenerator(llm_engine, config)\n",
        "\n",
        "    try:\n",
        "        print(\"Por favor, seleccione el archivo del programa de curso (ej. PROGRAMA_DE_CURSO.pdf)\")\n",
        "\n",
        "        # Crear y ocultar la ventana raíz de Tkinter\n",
        "        root = tk.Tk()\n",
        "        root.withdraw()\n",
        "\n",
        "        # Abrir el diálogo de selección de archivo\n",
        "        file_path = filedialog.askopenfilename(\n",
        "            title=\"Seleccione el archivo del programa de curso\",\n",
        "            filetypes=[(\"Archivos PDF\", \"*.pdf\"), (\"Todos los archivos\", \"*.*\")]\n",
        "        )\n",
        "\n",
        "        if not file_path:\n",
        "            print(\"No se ha seleccionado ningún archivo. Saliendo...\")\n",
        "            return\n",
        "\n",
        "        # Procesar el archivo y extraer la información del syllabus\n",
        "        syllabus_data = processor.process_file(file_path)\n",
        "\n",
        "        # Mostrar los datos extraídos de forma formateada\n",
        "        print(\"Datos extraídos del syllabus:\")\n",
        "        print(json.dumps(syllabus_data, indent=4, ensure_ascii=False))\n",
        "\n",
        "        # Generar los materiales didácticos basados en el syllabus extraído\n",
        "        contenido = generador.generate_all_materials(syllabus_data)\n",
        "\n",
        "        print(\"\\nMateriales generados:\")\n",
        "        print(json.dumps(contenido, indent=4, ensure_ascii=False))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error al procesar el archivo: {e}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "TrAyUfTu-wP2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from typing import Dict\n",
        "import pypandoc\n",
        "import markdown\n",
        "\n",
        "# Configuración centralizada del logger para este módulo\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "def dict_to_pdf(materials: Dict[str, Dict[str, str]], output_pdf: str = \"materiales_curso.pdf\") -> None:\n",
        "    \"\"\"\n",
        "    Convierte un diccionario de materiales a un archivo PDF.\n",
        "\n",
        "    El diccionario se transforma a un string en formato Markdown, se convierte a HTML y finalmente a PDF\n",
        "    usando pypandoc.\n",
        "\n",
        "    Args:\n",
        "        materials (Dict[str, Dict[str, str]]): Diccionario con la estructura de temas y secciones.\n",
        "        output_pdf (str): Nombre del archivo PDF de salida (por defecto \"materiales_curso.pdf\").\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Construir el contenido en Markdown usando una lista para mayor eficiencia\n",
        "        md_lines = []\n",
        "        for topic_id, sections in materials.items():\n",
        "            md_lines.append(f\"# Tema {topic_id}\\n\")\n",
        "            for section, text in sections.items():\n",
        "                section_title = section.replace('_', ' ').title()\n",
        "                md_lines.append(f\"## {section_title}\\n\")\n",
        "                md_lines.append(f\"{text}\\n\")\n",
        "            md_lines.append(\"\\n---\\n\")  # Separador entre temas\n",
        "        md_content = \"\\n\".join(md_lines)\n",
        "        logger.info(\"Contenido en Markdown construido correctamente.\")\n",
        "\n",
        "        # Convertir el Markdown a HTML\n",
        "        html = markdown.markdown(md_content)\n",
        "        logger.info(\"Conversión de Markdown a HTML completada.\")\n",
        "\n",
        "        # Opciones adicionales para pypandoc: usar XeLaTeX y una fuente compatible con Unicode\n",
        "        extra_args = [\n",
        "            '--pdf-engine=xelatex',\n",
        "            '-V', 'mainfont=Times New Roman'\n",
        "        ]\n",
        "\n",
        "        # Convertir HTML a PDF y guardar el archivo\n",
        "        pypandoc.convert_text(html, to='pdf', format='html', outputfile=output_pdf, extra_args=extra_args)\n",
        "        logger.info(f\"Archivo PDF guardado: {output_pdf}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error durante la conversión a PDF: {e}\", exc_info=True)\n",
        "        raise\n",
        "\n",
        "\n",
        "def save_materials_as_text_files(materials: Dict[str, Dict[str, str]], output_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Guarda los materiales generados en archivos de texto.\n",
        "\n",
        "    Cada tema se guarda en una subcarpeta, y para cada tipo de contenido se crea un archivo.\n",
        "\n",
        "    Args:\n",
        "        materials (Dict[str, Dict[str, str]]): Diccionario con la estructura de temas y secciones.\n",
        "        output_dir (str): Directorio base donde se guardarán los archivos.\n",
        "    \"\"\"\n",
        "    # Crear el directorio base (y sus subdirectorios) si no existen\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for topic_id, sections in materials.items():\n",
        "        # Crear una subcarpeta para cada tema\n",
        "        topic_dir = os.path.join(output_dir, f\"tema_{topic_id}\")\n",
        "        os.makedirs(topic_dir, exist_ok=True)\n",
        "\n",
        "        # Guardar cada tipo de material en un archivo distinto\n",
        "        for content_type, text in sections.items():\n",
        "            filename = os.path.join(topic_dir, f\"{content_type}.txt\")\n",
        "            try:\n",
        "                with open(filename, 'w', encoding='utf-8') as f:\n",
        "                    f.write(text)\n",
        "                logger.info(f\"Archivo guardado: {filename}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error al escribir el archivo {filename}: {e}\", exc_info=True)\n",
        "\n",
        "    logger.info(f\"Materiales guardados en: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eta2WdK8BCds"
      },
      "source": [
        "# Generar contenido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ribt5_eQA_CM"
      },
      "outputs": [],
      "source": [
        "contenido = generador.generate_all_materials(syllabus_data)\n",
        "save_materials_as_text_files(contenido, \"materiales_generados\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZDTbdPt--Od",
        "outputId": "098dae6c-6740-432e-d3cf-8bf6d93a7829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'topic_scores': {'1': {'lecture_notes': 0.7575757575757576, 'practice_problems': 0.65, 'discussion_questions': 0.4166666666666667, 'learning_objectives': 0.4047619047619048, 'suggested_resources': 0.6, 'average': 0.5658008658008659}, '3': {'lecture_notes': 0.6369230769230769, 'practice_problems': 0.65, 'discussion_questions': 0.36904761904761907, 'learning_objectives': 0.30952380952380953, 'suggested_resources': 0.6, 'average': 0.5130989010989011}, '4': {'lecture_notes': 0.6, 'practice_problems': 0.65, 'discussion_questions': 0.4166666666666667, 'learning_objectives': 0.4047619047619048, 'suggested_resources': 0.6, 'average': 0.5342857142857144}}, 'content_type_scores': {'lecture_notes': 0.6648329448329449, 'practice_problems': 0.65, 'discussion_questions': 0.40079365079365087, 'learning_objectives': 0.3730158730158731, 'suggested_resources': 0.6}, 'overall_metrics': {'relevance_score': 0.0, 'consistency_score': 0.8360108101779099, 'readability_score': 0.12667554371948503, 'domain_terminology_score': 0.4264796474434914}, 'average_score': 0.4425099970318577}\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Asegurarse de que NLTK cuente con los recursos necesarios\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Configuración centralizada del logger para este módulo\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Evaluator:\n",
        "    \"\"\"\n",
        "    Evalúa la calidad del contenido educativo generado utilizando diversas métricas:\n",
        "      - Relevancia (cobertura de subtemas)\n",
        "      - Consistencia (similitud de coseno entre temas)\n",
        "      - Legibilidad (longitud promedio de oraciones)\n",
        "      - Uso de terminología específica (basada en TF-IDF)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: Any) -> None:\n",
        "        \"\"\"\n",
        "        Inicializa el evaluador con la configuración dada.\n",
        "\n",
        "        Args:\n",
        "            config: Configuración general del sistema.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(\"educational_agent.evaluator\")\n",
        "\n",
        "    def evaluate_content(\n",
        "        self,\n",
        "        generated_content: Dict[str, Dict[str, str]],\n",
        "        syllabus_data: Dict[str, Any]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evalúa el contenido generado utilizando diversas métricas y retorna un resumen.\n",
        "\n",
        "        Args:\n",
        "            generated_content: Diccionario con los materiales generados, organizados por tema y tipo.\n",
        "            syllabus_data: Datos estructurados del syllabus del curso.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con los resultados de la evaluación.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Iniciando evaluación del contenido generado\")\n",
        "\n",
        "        evaluation_results: Dict[str, Any] = {\n",
        "            \"topic_scores\": {},\n",
        "            \"content_type_scores\": {\n",
        "                \"lecture_notes\": 0,\n",
        "                \"practice_problems\": 0,\n",
        "                \"discussion_questions\": 0,\n",
        "                \"learning_objectives\": 0,\n",
        "                \"suggested_resources\": 0\n",
        "            },\n",
        "            \"overall_metrics\": {\n",
        "                \"relevance_score\": 0,\n",
        "                \"consistency_score\": 0,\n",
        "                \"readability_score\": 0,\n",
        "                \"domain_terminology_score\": 0\n",
        "            },\n",
        "            \"average_score\": 0\n",
        "        }\n",
        "\n",
        "        # Extraer terminología clave del syllabus para evaluar su uso\n",
        "        course_terminology = self._extract_domain_terminology(syllabus_data)\n",
        "\n",
        "        # Evaluar cada tema individualmente\n",
        "        for topic_id, topic_content in generated_content.items():\n",
        "            # Buscar información del tema en el syllabus (por ID)\n",
        "            topic_info = next((t for t in syllabus_data.get(\"topics\", []) if t.get(\"id\") == topic_id), None)\n",
        "            if not topic_info:\n",
        "                self.logger.warning(f\"No se encontró información para el tema {topic_id} en el syllabus\")\n",
        "                continue\n",
        "\n",
        "            # Extraer textos por tipo de contenido\n",
        "            lecture = topic_content.get(\"lecture_notes\", \"\")\n",
        "            practice = topic_content.get(\"practice_problems\", \"\")\n",
        "            discussion = topic_content.get(\"discussion_questions\", \"\")\n",
        "            objectives = topic_content.get(\"learning_objectives\", \"\")\n",
        "            resources = topic_content.get(\"suggested_resources\", \"\")\n",
        "\n",
        "            # Calcular puntuaciones para cada tipo de contenido\n",
        "            topic_scores: Dict[str, float] = {\n",
        "                \"lecture_notes\": self._evaluate_lecture_notes(lecture, topic_info, course_terminology),\n",
        "                \"practice_problems\": self._evaluate_practice_problems(practice, topic_info),\n",
        "                \"discussion_questions\": self._evaluate_discussion_questions(discussion, topic_info),\n",
        "                \"learning_objectives\": self._evaluate_learning_objectives(objectives, topic_info),\n",
        "                \"suggested_resources\": self._evaluate_suggested_resources(resources, topic_info)\n",
        "            }\n",
        "            # Calcular la puntuación promedio para el tema\n",
        "            topic_avg = sum(topic_scores.values()) / len(topic_scores)\n",
        "            topic_scores[\"average\"] = topic_avg\n",
        "            evaluation_results[\"topic_scores\"][topic_id] = topic_scores\n",
        "\n",
        "            # Acumular las puntuaciones por tipo de contenido\n",
        "            for key, score in topic_scores.items():\n",
        "                if key != \"average\" and key in evaluation_results[\"content_type_scores\"]:\n",
        "                    evaluation_results[\"content_type_scores\"][key] += score\n",
        "\n",
        "        # Promediar las puntuaciones por tipo de contenido\n",
        "        num_topics = len(generated_content)\n",
        "        if num_topics > 0:\n",
        "            for key in evaluation_results[\"content_type_scores\"]:\n",
        "                evaluation_results[\"content_type_scores\"][key] /= num_topics\n",
        "\n",
        "        # Métricas globales a partir de todo el contenido generado\n",
        "        all_texts = self._join_all_texts(generated_content)\n",
        "        evaluation_results[\"overall_metrics\"][\"consistency_score\"] = self._evaluate_consistency(generated_content)\n",
        "        evaluation_results[\"overall_metrics\"][\"readability_score\"] = self._calculate_readability(all_texts)\n",
        "        evaluation_results[\"overall_metrics\"][\"domain_terminology_score\"] = self._calculate_terminology_usage(all_texts, course_terminology)\n",
        "\n",
        "        # Relevancia: cobertura promedio de subtemas en las notas de clase\n",
        "        relevance_scores = [\n",
        "            self._calculate_subtopic_coverage(topic_content.get(\"lecture_notes\", \"\"), topic_info.get(\"subtopics\", []))\n",
        "            for topic_id, topic_content in generated_content.items()\n",
        "            for topic_info in [next((t for t in syllabus_data.get(\"topics\", []) if t.get(\"id\") == topic_id), {})]\n",
        "        ]\n",
        "        overall_relevance = np.mean(relevance_scores) if relevance_scores else 0\n",
        "        evaluation_results[\"overall_metrics\"][\"relevance_score\"] = overall_relevance\n",
        "\n",
        "        # Puntuación global promedio\n",
        "        content_type_avg = np.mean(list(evaluation_results[\"content_type_scores\"].values()))\n",
        "        overall_metrics_avg = np.mean(list(evaluation_results[\"overall_metrics\"].values()))\n",
        "        evaluation_results[\"average_score\"] = (content_type_avg + overall_metrics_avg) / 2\n",
        "\n",
        "        self.logger.info(f\"Evaluación completada. Puntuación promedio: {evaluation_results['average_score']:.2f}\")\n",
        "        return evaluation_results\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Métodos auxiliares para análisis global\n",
        "    # -------------------------------------------------------------------------\n",
        "    def _join_all_texts(self, generated_content: Dict[str, Dict[str, str]]) -> str:\n",
        "        \"\"\"Une todo el texto de los temas y secciones para un análisis global.\"\"\"\n",
        "        texts = [text for topic in generated_content.values() for text in topic.values()]\n",
        "        return \"\\n\".join(texts)\n",
        "\n",
        "    def _calculate_readability(self, text: str) -> float:\n",
        "        \"\"\"\n",
        "        Calcula la legibilidad en función de la longitud promedio de las oraciones.\n",
        "        Se asume que oraciones más cortas facilitan la lectura.\n",
        "\n",
        "        Returns:\n",
        "            Valor entre 0 y 1.\n",
        "        \"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        words = word_tokenize(text)\n",
        "        if not sentences:\n",
        "            return 0.0\n",
        "        avg_sentence_length = len(words) / len(sentences)\n",
        "        if avg_sentence_length <= 15:\n",
        "            return 1.0\n",
        "        elif avg_sentence_length >= 30:\n",
        "            return 0.0\n",
        "        else:\n",
        "            return (30 - avg_sentence_length) / 15\n",
        "\n",
        "    def _extract_domain_terminology(self, syllabus_data: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extrae términos clave del syllabus combinando descripción, objetivos, temario y bibliografía.\n",
        "\n",
        "        Returns:\n",
        "            Lista de términos extraídos mediante TF-IDF.\n",
        "        \"\"\"\n",
        "        combined_text = \" \".join([\n",
        "            syllabus_data.get(\"description\", \"\"),\n",
        "            \" \".join(syllabus_data.get(\"objectives\", [])),\n",
        "            \" \".join([t.get(\"title\", \"\") + \" \" + \" \".join(t.get(\"subtopics\", [])) for t in syllabus_data.get(\"topics\", [])]),\n",
        "            \" \".join(syllabus_data.get(\"bibliography\", []))\n",
        "        ])\n",
        "\n",
        "        vectorizer = TfidfVectorizer(max_features=50, stop_words='english', ngram_range=(1, 2))\n",
        "        try:\n",
        "            tfidf_matrix = vectorizer.fit_transform([combined_text])\n",
        "            return list(vectorizer.get_feature_names_out())\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"Error extrayendo terminología: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _calculate_terminology_usage(self, text: str, terminology: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calcula la densidad de uso de la terminología específica en el contenido.\n",
        "\n",
        "        Returns:\n",
        "            Valor entre 0 y 1.\n",
        "        \"\"\"\n",
        "        content_lower = text.lower()\n",
        "        term_count = sum(1 for term in terminology if term.lower() in content_lower)\n",
        "        word_count = len(word_tokenize(text))\n",
        "        term_density = (term_count * 1000) / word_count if word_count else 0\n",
        "        # Se asume que 5 términos por cada 1000 palabras es ideal\n",
        "        return min(1.0, term_density / 5)\n",
        "\n",
        "    def _calculate_subtopic_coverage(self, content: str, subtopics: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calcula la proporción de subtemas cubiertos en el contenido.\n",
        "\n",
        "        Returns:\n",
        "            Valor entre 0 y 1.\n",
        "        \"\"\"\n",
        "        content_lower = content.lower()\n",
        "        covered = 0\n",
        "        for sub in subtopics:\n",
        "            sub_terms = [w.lower() for w in word_tokenize(sub) if len(w) > 3]\n",
        "            if sub_terms and (sum(1 for term in sub_terms if term in content_lower) / len(sub_terms)) >= 0.5:\n",
        "                covered += 1\n",
        "        return covered / len(subtopics) if subtopics else 0\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Métodos de evaluación específicos para cada tipo de contenido\n",
        "    # -------------------------------------------------------------------------\n",
        "    def _evaluate_lecture_notes(self, lecture_notes: str, topic_info: Dict[str, Any], course_terminology: List[str]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(lecture_notes, topic_info.get(\"subtopics\", [])),\n",
        "            self._calculate_terminology_usage(lecture_notes, course_terminology),\n",
        "            self._calculate_readability(lecture_notes),\n",
        "            self._evaluate_content_structure(lecture_notes),\n",
        "            self._evaluate_examples_presence(lecture_notes)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    def _evaluate_practice_problems(self, practice_problems: str, topic_info: Dict[str, Any]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(practice_problems, topic_info.get(\"subtopics\", [])),\n",
        "            self._evaluate_solutions_presence(practice_problems),\n",
        "            self._evaluate_difficulty_variety(practice_problems),\n",
        "            self._evaluate_problem_clarity(practice_problems)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    def _evaluate_discussion_questions(self, discussion_questions: str, topic_info: Dict[str, Any]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(discussion_questions, topic_info.get(\"subtopics\", [])),\n",
        "            self._evaluate_critical_thinking(discussion_questions),\n",
        "            self._evaluate_open_ended_questions(discussion_questions)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    def _evaluate_learning_objectives(self, learning_objectives: str, topic_info: Dict[str, Any]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(learning_objectives, topic_info.get(\"subtopics\", [])),\n",
        "            self._evaluate_bloom_taxonomy_usage(learning_objectives),\n",
        "            self._evaluate_measurable_objectives(learning_objectives)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    def _evaluate_suggested_resources(self, suggested_resources: str, topic_info: Dict[str, Any]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(suggested_resources, topic_info.get(\"subtopics\", [])),\n",
        "            self._evaluate_resource_variety(suggested_resources),\n",
        "            self._evaluate_resource_detail(suggested_resources)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Métodos auxiliares \"stub\" o simples para evaluación adicional\n",
        "    # -------------------------------------------------------------------------\n",
        "    def _evaluate_content_structure(self, text: str) -> float:\n",
        "        paragraphs = [p for p in text.split(\"\\n\\n\") if p.strip()]\n",
        "        num_paragraphs = len(paragraphs)\n",
        "        return 1.0 if num_paragraphs >= 5 else (num_paragraphs / 5.0 if num_paragraphs else 0.0)\n",
        "\n",
        "    def _evaluate_examples_presence(self, text: str) -> float:\n",
        "        return 1.0 if \"ejemplo\" in text.lower() else 0.0\n",
        "\n",
        "    def _evaluate_solutions_presence(self, text: str) -> float:\n",
        "        return 1.0 if (\"solución\" in text.lower() or \"solución:\" in text.lower()) else 0.0\n",
        "\n",
        "    def _evaluate_difficulty_variety(self, text: str) -> float:\n",
        "        return 0.8\n",
        "\n",
        "    def _evaluate_problem_clarity(self, text: str) -> float:\n",
        "        return 0.8\n",
        "\n",
        "    def _evaluate_critical_thinking(self, text: str) -> float:\n",
        "        keywords = [\"analiza\", \"discute\", \"reflexiona\", \"argumenta\"]\n",
        "        count = sum(1 for kw in keywords if kw in text.lower())\n",
        "        return min(1.0, count / len(keywords))\n",
        "\n",
        "    def _evaluate_open_ended_questions(self, text: str) -> float:\n",
        "        questions = [line for line in text.splitlines() if \"?\" in line]\n",
        "        if not questions:\n",
        "            return 0.0\n",
        "        open_count = sum(1 for q in questions if not re.search(r'\\b(?:sí|no)\\b', q.lower()))\n",
        "        return open_count / len(questions)\n",
        "\n",
        "    def _evaluate_bloom_taxonomy_usage(self, text: str) -> float:\n",
        "        bloom_verbs = [\"analiza\", \"aplica\", \"compara\", \"evalúa\", \"crea\", \"sintetiza\", \"interpreta\"]\n",
        "        count = sum(1 for verb in bloom_verbs if verb in text.lower())\n",
        "        return min(1.0, count / len(bloom_verbs))\n",
        "\n",
        "    def _evaluate_measurable_objectives(self, text: str) -> float:\n",
        "        return 0.8 if any(kw in text.lower() for kw in [\"porcentaje\", \"número\", \"cuantifica\"]) else 0.5\n",
        "\n",
        "    def _evaluate_resource_variety(self, text: str) -> float:\n",
        "        lines = [l for l in text.splitlines() if l.strip()]\n",
        "        return min(1.0, len(lines) / 5.0)\n",
        "\n",
        "    def _evaluate_resource_detail(self, text: str) -> float:\n",
        "        return 0.8\n",
        "\n",
        "    def _evaluate_consistency(self, generated_content: Dict[str, Dict[str, str]]) -> float:\n",
        "        \"\"\"\n",
        "        Evalúa la consistencia semántica entre temas utilizando TF-IDF y similitud de coseno.\n",
        "        Se asume que mayor similitud entre temas indica mayor consistencia.\n",
        "        \"\"\"\n",
        "        texts = [\" \".join(topic_content.values()) for topic_content in generated_content.values()]\n",
        "        if len(texts) < 2:\n",
        "            return 1.0\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf = vectorizer.fit_transform(texts)\n",
        "        similarity_matrix = cosine_similarity(tfidf)\n",
        "        n = similarity_matrix.shape[0]\n",
        "        # Excluir la diagonal (autosemejanza)\n",
        "        sum_sim = np.sum(similarity_matrix) - n\n",
        "        num_elements = n * (n - 1)\n",
        "        avg_similarity = sum_sim / num_elements if num_elements else 1.0\n",
        "        return avg_similarity\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Ejemplo de evaluación (se asume que 'config', 'contenido' y 'syllabus_data' están definidos)\n",
        "# =============================================================================\n",
        "\n",
        "evaluator = Evaluator(config)\n",
        "evaluacion = evaluator.evaluate_content(contenido, syllabus_data)\n",
        "print(evaluacion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9-mylnNS_LpV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from typing import Dict\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def save_materials_as_text_files(materials: Dict[str, Dict[str, str]], output_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Guarda los materiales generados en archivos de texto.\n",
        "\n",
        "    Cada tema se guarda en una subcarpeta y, para cada tipo de contenido, se crea un archivo.\n",
        "\n",
        "    Args:\n",
        "        materials (Dict[str, Dict[str, str]]): Diccionario con los materiales generados.\n",
        "        output_dir (str): Directorio base donde se guardarán los archivos.\n",
        "    \"\"\"\n",
        "    # Crear el directorio base (y sus subdirectorios) de forma recursiva\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for topic_id, content in materials.items():\n",
        "        topic_dir = os.path.join(output_dir, f\"tema_{topic_id}\")\n",
        "        os.makedirs(topic_dir, exist_ok=True)\n",
        "\n",
        "        for content_type, text in content.items():\n",
        "            filename = os.path.join(topic_dir, f\"{content_type}.txt\")\n",
        "            try:\n",
        "                with open(filename, 'w', encoding='utf-8') as f:\n",
        "                    f.write(text)\n",
        "                logger.info(f\"Archivo guardado: {filename}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error al escribir el archivo {filename}: {e}\", exc_info=True)\n",
        "\n",
        "    logger.info(f\"Materiales guardados en: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-W-0zN04HB2u",
        "outputId": "6017bbcf-01c4-4e5c-9370-9a0aad1eca82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "texlive-fonts-recommended is already the newest version (2021.20220204-1).\n",
            "texlive-plain-generic is already the newest version (2021.20220204-1).\n",
            "texlive-xetex is already the newest version (2021.20220204-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-freefont-ttf\n",
            "0 upgraded, 1 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 2,388 kB of archives.\n",
            "After this operation, 6,653 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-freefont-ttf all 20120503-10build1 [2,388 kB]\n",
            "Fetched 2,388 kB in 2s (1,042 kB/s)\n",
            "Selecting previously unselected package fonts-freefont-ttf.\n",
            "(Reading database ... 162001 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-freefont-ttf_20120503-10build1_all.deb ...\n",
            "Unpacking fonts-freefont-ttf (20120503-10build1) ...\n",
            "Setting up fonts-freefont-ttf (20120503-10build1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.11/dist-packages (1.15)\n"
          ]
        }
      ],
      "source": [
        "# Instalar dependencias en Google Colab\n",
        "!apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic\n",
        "!apt-get install -y fonts-freefont-ttf  # Fuente alternativa FreeSerif (parecida a Times New Roman)\n",
        "!pip install pypandoc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "id": "VZy9R21c_Ybd",
        "outputId": "e779c199-4f17-49b5-e09b-c4b0561758f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📜 Vista previa del contenido Markdown:\n",
            " # Tema 1\n",
            "\n",
            "## Lecture Notes\n",
            "\n",
            "¡Excelente! Vamos a construir unas notas de clase detalladas y rigurosas sobre el tema especificado, asegurándonos de cubrir los subtemas con profundidad, ejemplos y aplicaciones prácticas.  Por favor, proporciona el tema principal (el número 5) y la lista de subtemas que deseas que cubra.  Una vez que me proporciones esa información, generaré las notas de clase.\n",
            "\n",
            "**Ejemplo de cómo proporcionarme la información:**\n",
            "\n",
            "*   **Tema:** 5. Variables Aleatorias Continuas\n",
            "*   *\n",
            "⏳ Convirtiendo Markdown a PDF...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[WARNING] Missing character: There is no ∂ (U+2202) (U+2202) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ∂ (U+2202) (U+2202) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ∂ (U+2202) (U+2202) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ∂ (U+2202) (U+2202) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ∂ (U+2202) (U+2202) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ∂ (U+2202) (U+2202) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ∂ (U+2202) (U+2202) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ∂ (U+2202) (U+2202) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ≈ (U+2248) (U+2248) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ≈ (U+2248) (U+2248) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ≈ (U+2248) (U+2248) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ≈ (U+2248) (U+2248) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ≈ (U+2248) (U+2248) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ≈ (U+2248) (U+2248) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ≈ (U+2248) (U+2248) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ≈ (U+2248) (U+2248) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ≈ (U+2248) (U+2248) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ≈ (U+2248) (U+2248) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ≈ (U+2248) (U+2248) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ≈ (U+2248) (U+2248) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ≤ (U+2264) (U+2264) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ≤ (U+2264) (U+2264) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ≤ (U+2264) (U+2264) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ≤ (U+2264) (U+2264) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ≤ (U+2264) (U+2264) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ≤ (U+2264) (U+2264) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ≥ (U+2265) (U+2265) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ≥ (U+2265) (U+2265) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ≤ (U+2264) (U+2264) in font [lmmono10-regular]:!\n",
            "WARNING:pypandoc:Missing character: There is no ≤ (U+2264) (U+2264) in font [lmmono10-regular]:!\n",
            "[WARNING] Missing character: There is no ≤ (U+2264) (U+2264) in font [lmmono10-regular]:!\n",
            "\n",
            "WARNING:pypandoc:Missing character: There is no ≤ (U+2264) (U+2264) in font [lmmono10-regular]:!\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ PDF generado con éxito: materiales_curso.pdf\n",
            "📥 Descargando PDF...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_8d19fd68-e96a-4ee1-b668-feb401c1ab8f\", \"materiales_curso.pdf\", 166794)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import os\n",
        "import pypandoc\n",
        "\n",
        "# En Google Colab, ejecutar estas celdas para instalar dependencias:\n",
        "# !apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic\n",
        "# !apt-get install -y fonts-freefont-ttf  # Fuente alternativa FreeSerif (similar a Times New Roman)\n",
        "# !pip install pypandoc\n",
        "\n",
        "def dict_to_pdf(materials: dict, output_pdf: str = \"materiales_curso.pdf\") -> bool:\n",
        "    \"\"\"\n",
        "    Convierte un diccionario de materiales a un archivo PDF.\n",
        "\n",
        "    El diccionario se transforma a Markdown y luego se convierte a PDF usando pypandoc.\n",
        "\n",
        "    Args:\n",
        "        materials (dict): Diccionario con la estructura de temas y secciones.\n",
        "        output_pdf (str): Nombre del archivo PDF de salida.\n",
        "\n",
        "    Returns:\n",
        "        bool: True si el PDF se generó correctamente; False en caso de error.\n",
        "    \"\"\"\n",
        "    # Construir el contenido Markdown de manera eficiente\n",
        "    md_lines = []\n",
        "    for topic_id, content in materials.items():\n",
        "        md_lines.append(f\"# Tema {topic_id}\\n\")\n",
        "        for section, text in content.items():\n",
        "            section_title = section.replace('_', ' ').title()\n",
        "            md_lines.append(f\"## {section_title}\\n\")\n",
        "            md_lines.append(f\"{text}\\n\")\n",
        "        md_lines.append(\"\\n---\\n\")\n",
        "    md_content = \"\\n\".join(md_lines)\n",
        "\n",
        "    # Vista previa para depuración\n",
        "    print(\"📜 Vista previa del contenido Markdown:\\n\", md_content[:500])\n",
        "\n",
        "    if not md_content.strip():\n",
        "        raise ValueError(\"⚠️ Error: El contenido del PDF está vacío.\")\n",
        "\n",
        "    # Usar \"FreeSerif\" como fuente segura en LaTeX\n",
        "    extra_args = [\n",
        "        '--pdf-engine=xelatex',\n",
        "        '-V', 'mainfont=\"FreeSerif\"'\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        print(\"⏳ Convirtiendo Markdown a PDF...\")\n",
        "        pypandoc.convert_text(md_content, to='pdf', format='md', outputfile=output_pdf, extra_args=extra_args)\n",
        "\n",
        "        if os.path.exists(output_pdf):\n",
        "            print(f\"✅ PDF generado con éxito: {output_pdf}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"❌ Error: La conversión a PDF no generó un archivo.\")\n",
        "    except RuntimeError as e:\n",
        "        print(f\"❌ Error durante la conversión a PDF: {e}\")\n",
        "        return False\n",
        "\n",
        "    return True\n",
        "\n",
        "# Llamar a la función para generar el PDF (se asume que 'contenido' está definido)\n",
        "pdf_created = dict_to_pdf(contenido, \"materiales_curso.pdf\")\n",
        "\n",
        "# Descargar el PDF solo si se generó correctamente (en Colab)\n",
        "if pdf_created and os.path.exists(\"materiales_curso.pdf\"):\n",
        "    from google.colab import files\n",
        "    print(\"📥 Descargando PDF...\")\n",
        "    files.download(\"materiales_curso.pdf\")\n",
        "else:\n",
        "    print(\"⚠️ No se pudo generar el archivo PDF. Revisa los errores anteriores.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mteqXhlxHqjD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def format_text_with_markdown(text):\n",
        "    \"\"\"\n",
        "    Convierte el texto con formato Markdown (negritas y listas) a HTML.\n",
        "    - **texto** -> <strong>texto</strong>\n",
        "    - * item -> <ul><li>item</li></ul>\n",
        "    \"\"\"\n",
        "    # Convertir negritas (**texto**)\n",
        "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'<strong>\\1</strong>', text)\n",
        "\n",
        "    # Convertir listas con viñetas (* item)\n",
        "    lines = text.split('\\n')\n",
        "    in_list = False\n",
        "    formatted_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if line.strip().startswith('* '):\n",
        "            if not in_list:\n",
        "                formatted_lines.append('<ul>')\n",
        "                in_list = True\n",
        "            formatted_lines.append(f'<li>{line.strip()[2:]}</li>')\n",
        "        else:\n",
        "            if in_list:\n",
        "                formatted_lines.append('</ul>')\n",
        "                in_list = False\n",
        "            formatted_lines.append(line)\n",
        "\n",
        "    if in_list:\n",
        "        formatted_lines.append('</ul>')\n",
        "\n",
        "    return '\\n'.join(formatted_lines)\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "def get_colab_font():\n",
        "    # Esta función ya no es necesaria para HTML, pero la mantenemos por compatibilidad\n",
        "    return None\n",
        "\n",
        "def wrap_text_justified(text, max_width, font):\n",
        "    # Esta función ya no es necesaria para HTML, pero la mantenemos por compatibilidad\n",
        "    return text\n",
        "\n",
        "def wrap_text_by_words(text, words_per_line=12):\n",
        "    \"\"\"\n",
        "    Recibe un texto y lo divide en líneas de 'words_per_line' palabras.\n",
        "    Retorna el texto con saltos de línea (\\n) insertados.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    lines = []\n",
        "    for i in range(0, len(words), words_per_line):\n",
        "        lines.append(\" \".join(words[i:i+words_per_line]))\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def create_gradient_background():\n",
        "    \"\"\"\n",
        "    Crea un fondo degradado vertical usando CSS.\n",
        "    \"\"\"\n",
        "    return \"\"\"\n",
        "    background: linear-gradient(to bottom, #dce6ff, #ffffff);\n",
        "    \"\"\"\n",
        "\n",
        "def draw_text_with_shadow():\n",
        "    \"\"\"\n",
        "    Aplica un efecto de sombra al texto usando CSS.\n",
        "    \"\"\"\n",
        "    return \"\"\"\n",
        "    text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.2);\n",
        "    \"\"\"\n",
        "\n",
        "def parse_discussion_questions(file_path):\n",
        "    \"\"\"\n",
        "    Lee el archivo `discussion_questions.txt` y extrae cada pregunta y su nota para el instructor.\n",
        "    Retorna una lista de diccionarios con las claves:\n",
        "    [\n",
        "      {\n",
        "        'pregunta': 'Texto de la pregunta...',\n",
        "        'nota_instructor': 'Texto de la nota para el instructor...'\n",
        "      },\n",
        "      ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        return []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = [line.strip() for line in f]\n",
        "\n",
        "    questions_data = []\n",
        "    current_question = None\n",
        "    current_note = None\n",
        "    capturing_question = False\n",
        "    capturing_note = False\n",
        "\n",
        "    for line in lines:\n",
        "        if \"**Pregunta:**\" in line:\n",
        "            if current_question and current_note:\n",
        "                questions_data.append({\n",
        "                    'pregunta': current_question.strip(),\n",
        "                    'nota_instructor': current_note.strip()\n",
        "                })\n",
        "            current_question = line.split(\"**Pregunta:**\")[-1].strip()\n",
        "            current_note = \"\"\n",
        "            capturing_question = True\n",
        "            capturing_note = False\n",
        "            continue\n",
        "\n",
        "        if \"**Nota para el Instructor:**\" in line:\n",
        "            capturing_question = False\n",
        "            capturing_note = True\n",
        "            current_note += line.split(\"**Nota para el Instructor:**\")[-1].strip() + \" \"\n",
        "            continue\n",
        "\n",
        "        if capturing_question:\n",
        "            current_question += \" \" + line\n",
        "\n",
        "        if capturing_note:\n",
        "            current_note += \" \" + line\n",
        "\n",
        "    if current_question and current_note:\n",
        "        questions_data.append({\n",
        "            'pregunta': current_question.strip(),\n",
        "            'nota_instructor': current_note.strip()\n",
        "        })\n",
        "\n",
        "    return questions_data\n",
        "\n",
        "def parse_practice_problems_universal(file_path):\n",
        "    \"\"\"\n",
        "    Parser \"universal\" para practice_problems.txt, que maneja distintos estilos.\n",
        "    Retorna una lista de diccionarios con:\n",
        "    [\n",
        "      {\n",
        "        'titulo_problema': '...',\n",
        "        'enunciado': '...',\n",
        "        'solucion': '...'\n",
        "      },\n",
        "      ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        return []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = [line.strip() for line in f]\n",
        "\n",
        "    problems_data = []\n",
        "    current_title = \"\"\n",
        "    current_enunciado = \"\"\n",
        "    current_solucion = \"\"\n",
        "    capturing_enunciado = False\n",
        "    capturing_solucion = False\n",
        "\n",
        "    def guardar_problema_si_existe():\n",
        "        if current_title.strip() or current_enunciado.strip() or current_solucion.strip():\n",
        "            problems_data.append({\n",
        "                'titulo_problema': current_title.strip(),\n",
        "                'enunciado': current_enunciado.strip(),\n",
        "                'solucion': current_solucion.strip()\n",
        "            })\n",
        "\n",
        "    for line in lines:\n",
        "        if (line.startswith(\"**\")\n",
        "            and (\"Enunciado:\" not in line)\n",
        "            and (\"Solución:\" not in line)\n",
        "            and (\"Problema:\" not in line)):\n",
        "            if current_title or current_enunciado or current_solucion:\n",
        "                guardar_problema_si_existe()\n",
        "            current_title = line.replace(\"**\", \"\").strip()\n",
        "            current_enunciado = \"\"\n",
        "            current_solucion = \"\"\n",
        "            capturing_enunciado = False\n",
        "            capturing_solucion = False\n",
        "            continue\n",
        "\n",
        "        if re.search(r\"\\*\\*Enunciado:\\*\\*|\\*\\*Problema:\\*\\*\", line):\n",
        "            capturing_enunciado = True\n",
        "            capturing_solucion = False\n",
        "            if \"**Enunciado:**\" in line:\n",
        "                current_enunciado += \" \" + line.split(\"**Enunciado:**\")[-1].strip()\n",
        "            elif \"**Problema:**\" in line:\n",
        "                current_enunciado += \" \" + line.split(\"**Problema:**\")[-1].strip()\n",
        "            continue\n",
        "\n",
        "        if \"**Solución:**\" in line:\n",
        "            capturing_enunciado = False\n",
        "            capturing_solucion = True\n",
        "            current_solucion += \" \" + line.split(\"**Solución:**\")[-1].strip()\n",
        "            continue\n",
        "\n",
        "        if capturing_enunciado:\n",
        "            current_enunciado += \" \" + line\n",
        "\n",
        "        if capturing_solucion:\n",
        "            current_solucion += \" \" + line\n",
        "\n",
        "    if current_title or current_enunciado or current_solucion:\n",
        "        guardar_problema_si_existe()\n",
        "\n",
        "    return problems_data\n",
        "\n",
        "\n",
        "def generar_flashcard_html(titulo, text, output_path):\n",
        "    # Convertir texto en negrita\n",
        "    contenido = re.sub(r'\\*\\*(.*?)\\*\\*', r'<strong>\\1</strong>', text)\n",
        "    \n",
        "    # Convertir asteriscos en elementos de lista\n",
        "    lineas = contenido.split('\\n')\n",
        "    contenido_procesado = []\n",
        "\n",
        "    for linea in lineas:\n",
        "        if '*' in linea:\n",
        "            # Dividir la línea por asteriscos\n",
        "            partes = linea.split('*')\n",
        "            # Eliminar espacios en blanco al inicio y final de cada parte\n",
        "            partes = [parte.strip() for parte in partes if parte.strip()]\n",
        "            # Si hay partes válidas, crear una lista\n",
        "            if partes:\n",
        "                contenido_procesado.append('<ul>')\n",
        "                for parte in partes:\n",
        "                    contenido_procesado.append(f'<li>{parte}</li>')\n",
        "                contenido_procesado.append('</ul>')\n",
        "        else:\n",
        "            contenido_procesado.append(linea)\n",
        "\n",
        "    contenido = '\\n'.join(contenido_procesado)\n",
        "\n",
        "    \"\"\"\n",
        "    Genera una flashcard en formato HTML.\n",
        "    La flashcard ocupará un máximo del 80% del ancho y alto de la pantalla.\n",
        "    \"\"\"\n",
        "    html_content = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html lang=\"es\">\n",
        "    <head>\n",
        "        <meta charset=\"UTF-8\">\n",
        "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "        <title>{titulo}</title>\n",
        "        <style>\n",
        "            body {{\n",
        "                font-family: Arial, sans-serif;\n",
        "                margin: 0;\n",
        "                padding: 0;\n",
        "                display: flex;\n",
        "                justify-content: center;\n",
        "                align-items: center;\n",
        "                height: 100vh;\n",
        "                {create_gradient_background()}\n",
        "            }}\n",
        "            .flashcard {{\n",
        "                width: 80vw; /* 80% del ancho de la pantalla */\n",
        "                max-width: 900px; /* Máximo ancho para no estirar demasiado en pantallas grandes */\n",
        "                min-height: 60vh; /* 60% del alto de la pantalla */\n",
        "                max-height: 80vh; /* Máximo 80% del alto de la pantalla */\n",
        "                background: white;\n",
        "                border-radius: 20px;\n",
        "                box-shadow: 0 8px 16px rgba(0, 0, 0, 0.2);\n",
        "                padding: 20px;\n",
        "                box-sizing: border-box;\n",
        "                border: 3px solid #3c78d8;\n",
        "                overflow-y: auto; /* Scroll si el contenido es muy largo */\n",
        "            }}\n",
        "            .titulo {{\n",
        "                font-size: 28px;\n",
        "                font-weight: bold;\n",
        "                color: #143c6e;\n",
        "                text-align: center;\n",
        "                margin-bottom: 20px;\n",
        "                {draw_text_with_shadow()}\n",
        "            }}\n",
        "            .contenido {{\n",
        "                font-size: 22px;\n",
        "                color: #000;\n",
        "                line-height: 1.6;\n",
        "                padding: 10px;\n",
        "                text-align: justify;\n",
        "            }}\n",
        "            .contenido ul {{\n",
        "                padding-left: 20px; /* Sangría para listas */\n",
        "            }}\n",
        "            .contenido li {{\n",
        "                margin-bottom: 10px; /* Espacio entre elementos de la lista */\n",
        "            }}\n",
        "            .subtitulo {{\n",
        "                font-size: 24px;\n",
        "                color: #4646b4;\n",
        "                margin-top: 20px;\n",
        "                margin-bottom: 10px;\n",
        "                {draw_text_with_shadow()}\n",
        "            }}\n",
        "            @media (max-width: 768px) {{\n",
        "                .flashcard {{\n",
        "                    width: 95vw; /* Ocupa más espacio en pantallas pequeñas */\n",
        "                    min-height: 70vh; /* Más alto en móviles */\n",
        "                    padding: 15px;\n",
        "                }}\n",
        "                .titulo {{\n",
        "                    font-size: 24px;\n",
        "                }}\n",
        "                .contenido {{\n",
        "                    font-size: 18px;\n",
        "                }}\n",
        "                .subtitulo {{\n",
        "                    font-size: 20px;\n",
        "                }}\n",
        "            }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"flashcard\">\n",
        "            <div class=\"titulo\">{titulo}</div>\n",
        "            <div class=\"contenido\">\n",
        "    \"\"\"\n",
        "\n",
        "    # Procesar contenido para separar las secciones\n",
        "    secciones = []\n",
        "    lineas = contenido.split('\\n')\n",
        "    texto_actual = \"\"\n",
        "    titulo_actual = None\n",
        "\n",
        "    for linea in lineas:\n",
        "        if linea.strip().endswith(\":\"):  # Detecta si la línea es un título\n",
        "            if texto_actual:\n",
        "                secciones.append((titulo_actual, texto_actual.strip()))\n",
        "            titulo_actual = linea.strip()\n",
        "            texto_actual = \"\"\n",
        "        else:\n",
        "            texto_actual += \" \" + linea\n",
        "\n",
        "    if texto_actual:\n",
        "        secciones.append((titulo_actual, texto_actual.strip()))\n",
        "\n",
        "    if not secciones:\n",
        "        html_content += f\"<p>{contenido}</p>\"\n",
        "    else:\n",
        "        for subtitulo, texto in secciones:\n",
        "            if subtitulo:\n",
        "                html_content += f\"<div class='subtitulo'>{subtitulo}</div>\"\n",
        "            html_content += f\"<p>{texto}</p>\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "            </div>\n",
        "        </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(html_content)\n",
        "\n",
        "    print(f\"Flashcard generada en: {output_path}\")\n",
        "\n",
        "def generar_flashcards_solo_tema_1(base_dir=\"materiales_generados\"):\n",
        "    \"\"\"\n",
        "    Procesa únicamente la carpeta \"tema_1\" dentro de base_dir.\n",
        "    Busca discussion_questions.txt y practice_problems.txt,\n",
        "    y genera las flashcards en formato HTML (si existen esos archivos).\n",
        "    \"\"\"\n",
        "    tema_path = os.path.join(base_dir, \"tema_1\")\n",
        "    if not os.path.isdir(tema_path):\n",
        "        print(f\"No existe la carpeta: {tema_path}. Saliendo...\")\n",
        "        return\n",
        "\n",
        "    print(f\"Procesando carpeta: {tema_path}\")\n",
        "\n",
        "    discussion_file = os.path.join(tema_path, \"discussion_questions.txt\")\n",
        "    practice_file = os.path.join(tema_path, \"practice_problems.txt\")\n",
        "\n",
        "    # 1) Discussion Questions\n",
        "    dq_data = parse_discussion_questions(discussion_file)\n",
        "    if dq_data:\n",
        "        dq_flashcards_dir = os.path.join(tema_path, \"flashcards_discussion\")\n",
        "        os.makedirs(dq_flashcards_dir, exist_ok=True)\n",
        "        for i, item in enumerate(dq_data, start=1):\n",
        "            pregunta = item['pregunta']\n",
        "            nota_instructor = item['nota_instructor']\n",
        "            titulo = \"Pregunta\"\n",
        "            contenido = f\"{pregunta}\\n\\nNota para el Instructor:\\n{nota_instructor}\"\n",
        "            output_file = os.path.join(dq_flashcards_dir, f\"discussion_flashcard_{i}.html\")\n",
        "            generar_flashcard_html(titulo, contenido, output_file)\n",
        "    else:\n",
        "        print(\"No se encontraron Discussion Questions en:\", tema_path)\n",
        "\n",
        "    # 2) Practice Problems\n",
        "    pp_data = parse_practice_problems_universal(practice_file)\n",
        "    if pp_data:\n",
        "        pp_flashcards_dir = os.path.join(tema_path, \"flashcards_practice\")\n",
        "        os.makedirs(pp_flashcards_dir, exist_ok=True)\n",
        "        for i, item in enumerate(pp_data, start=1):\n",
        "            titulo_problema = item['titulo_problema']\n",
        "            enunciado = item['enunciado']\n",
        "            solucion = item['solucion']\n",
        "            titulo = titulo_problema if titulo_problema else \"Problema\"\n",
        "            contenido = f\"{enunciado}\\n\\nSolución:\\n{solucion}\"\n",
        "            output_file = os.path.join(pp_flashcards_dir, f\"practice_flashcard_{i}.html\")\n",
        "            generar_flashcard_html(titulo, contenido, output_file)\n",
        "    else:\n",
        "        print(\"No se encontraron Practice Problems en:\", tema_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generar_flashcards_solo_tema_1(\"materiales_generados\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
