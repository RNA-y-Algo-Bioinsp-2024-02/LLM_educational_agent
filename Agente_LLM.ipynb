{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ODO3dZ8pZ7cH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Esta es la clave de API que previamente se\n",
        "# debió guardar en Colab en Secrets:\n",
        "os.environ['GOOGLE_API_KEY'] = 'AIzaSyD8Vp2fT5rKPgTJV40ekUgShJd7vkHWqHA'\n",
        "\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WxoKihr_Z-E3"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-2.0-flash-001')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfyttUuubIT8",
        "outputId": "212601ec-0336-45bc-f2cd-9acad12a3fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting backoff\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: backoff\n",
            "Successfully installed backoff-2.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install backoff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwYzrgWwdw7r",
        "outputId": "a34b79f1-8f10-498c-ce52-b27b6f1fedca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1nw-pDXmuHr",
        "outputId": "bc47aa5f-5f0a-4fb8-c829-039e3d2a3b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kshy_kDNmuHr",
        "outputId": "7162f3b6-368f-43e7-d90f-95c2d4fa44b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTRr20lzmuHs",
        "outputId": "5fbbd3bb-c924-4f28-a6ca-0f6e2e36436b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.11/dist-packages (3.0.0)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py) (0.1.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install markdown-it-py python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y pandoc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xmzFsn3wAyh",
        "outputId": "38a09acc-4513-43db-eb39-e05911ba895f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc-data\n",
            "Suggested packages:\n",
            "  texlive-latex-recommended texlive-xetex texlive-luatex pandoc-citeproc texlive-latex-extra\n",
            "  context wkhtmltopdf librsvg2-bin groff ghc nodejs php python ruby libjs-mathjax libjs-katex\n",
            "  citation-style-language-styles\n",
            "The following NEW packages will be installed:\n",
            "  libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3 pandoc pandoc-data\n",
            "0 upgraded, 4 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 20.6 MB of archives.\n",
            "After this operation, 156 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n",
            "Fetched 20.6 MB in 1s (18.9 MB/s)\n",
            "Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n",
            "(Reading database ... 124947 files and directories currently installed.)\n",
            "Preparing to unpack .../libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package pandoc-data.\n",
            "Preparing to unpack .../pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n",
            "Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package pandoc.\n",
            "Preparing to unpack .../pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Setting up pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypandoc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r-IJPNevxhs",
        "outputId": "225d8e7b-964b-4c61-e913-862b3c924bd6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.15-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.15-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pandoc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZ_jDINHwFhu",
        "outputId": "6a925829-30cd-44c1-f842-8f0111ee19d3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pandoc 2.9.2.1\n",
            "Compiled with pandoc-types 1.20, texmath 0.12.0.2, skylighting 0.8.5\n",
            "Default user data directory: /root/.local/share/pandoc or /root/.pandoc\n",
            "Copyright (C) 2006-2020 John MacFarlane\n",
            "Web:  https://pandoc.org\n",
            "This is free software; see the source for copying conditions.\n",
            "There is no warranty, not even for merchantability or fitness\n",
            "for a particular purpose.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzX6TiqFwM74",
        "outputId": "29caf158-0872-4998-ae0c-fe7ec4bf11d0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n",
            "  fonts-urw-base35 libapache-pom-java libcommons-logging-java libcommons-parent-java\n",
            "  libfontbox-java libfontenc1 libgs9 libgs9-common libidn12 libijs-0.35 libjbig2dec0 libkpathsea6\n",
            "  libpdfbox-java libptexenc1 libruby3.0 libsynctex2 libteckit0 libtexlua53 libtexluajit2 libwoff1\n",
            "  libzzip-0-13 lmodern poppler-data preview-latex-style rake ruby ruby-net-telnet ruby-rubygems\n",
            "  ruby-webrick ruby-xmlrpc ruby3.0 rubygems-integration t1utils teckit tex-common tex-gyre\n",
            "  texlive-base texlive-binaries texlive-latex-base texlive-latex-extra texlive-latex-recommended\n",
            "  texlive-pictures tipa xfonts-encodings xfonts-utils\n",
            "Suggested packages:\n",
            "  fonts-noto fonts-freefont-otf | fonts-freefont-ttf libavalon-framework-java\n",
            "  libcommons-logging-java-doc libexcalibur-logkit-java liblog4j1.2-java poppler-utils ghostscript\n",
            "  fonts-japanese-mincho | fonts-ipafont-mincho fonts-japanese-gothic | fonts-ipafont-gothic\n",
            "  fonts-arphic-ukai fonts-arphic-uming fonts-nanum ri ruby-dev bundler debhelper gv\n",
            "  | postscript-viewer perl-tk xpdf | pdf-viewer xzdec texlive-fonts-recommended-doc\n",
            "  texlive-latex-base-doc python3-pygments icc-profiles libfile-which-perl\n",
            "  libspreadsheet-parseexcel-perl texlive-latex-extra-doc texlive-latex-recommended-doc\n",
            "  texlive-luatex texlive-pstricks dot2tex prerex texlive-pictures-doc vprerex default-jre-headless\n",
            "  tipa-doc\n",
            "The following NEW packages will be installed:\n",
            "  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n",
            "  fonts-urw-base35 libapache-pom-java libcommons-logging-java libcommons-parent-java\n",
            "  libfontbox-java libfontenc1 libgs9 libgs9-common libidn12 libijs-0.35 libjbig2dec0 libkpathsea6\n",
            "  libpdfbox-java libptexenc1 libruby3.0 libsynctex2 libteckit0 libtexlua53 libtexluajit2 libwoff1\n",
            "  libzzip-0-13 lmodern poppler-data preview-latex-style rake ruby ruby-net-telnet ruby-rubygems\n",
            "  ruby-webrick ruby-xmlrpc ruby3.0 rubygems-integration t1utils teckit tex-common tex-gyre\n",
            "  texlive-base texlive-binaries texlive-fonts-recommended texlive-latex-base texlive-latex-extra\n",
            "  texlive-latex-recommended texlive-pictures texlive-plain-generic texlive-xetex tipa\n",
            "  xfonts-encodings xfonts-utils\n",
            "0 upgraded, 54 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 182 MB of archives.\n",
            "After this operation, 571 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lato all 2.0-2.1 [2,696 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-common all 6.17 [33.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.10 [752 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.10 [5,031 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libkpathsea6 amd64 2021.20210626.59705-1ubuntu0.2 [60.4 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dvisvgm amd64 2.13.1-1 [1,221 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-lmodern all 2.004.5-6.1 [4,532 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-texgyre all 20180621-3.1 [10.2 MB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libapache-pom-java all 18-1 [4,720 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-parent-java all 43-1 [10.8 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-logging-java all 1.2-2 [60.3 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libptexenc1 amd64 2021.20210626.59705-1ubuntu0.2 [39.1 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 rubygems-integration all 1.18 [5,336 B]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby3.0 amd64 3.0.2-7ubuntu2.8 [50.1 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-rubygems all 3.3.5-2 [228 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby amd64 1:3.0~exp1 [5,100 B]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 rake all 13.0.6-2 [61.7 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-webrick all 1.7.0-3ubuntu0.1 [52.1 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-xmlrpc all 0.3.2-1ubuntu0.1 [24.9 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libruby3.0 amd64 3.0.2-7ubuntu2.8 [5,113 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsynctex2 amd64 2021.20210626.59705-1ubuntu0.2 [55.6 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libteckit0 amd64 2.5.11+ds1-1 [421 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexlua53 amd64 2021.20210626.59705-1ubuntu0.2 [120 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexluajit2 amd64 2021.20210626.59705-1ubuntu0.2 [267 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzzip-0-13 amd64 0.13.72+dfsg.1-1.1 [27.0 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lmodern all 2.004.5-6.1 [9,471 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/universe amd64 preview-latex-style all 12.2-1ubuntu1 [185 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/main amd64 t1utils amd64 1.41-4build2 [61.3 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 teckit amd64 2.5.11+ds1-1 [699 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-gyre all 20180621-3.1 [6,209 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 texlive-binaries amd64 2021.20210626.59705-1ubuntu0.2 [9,860 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-base all 2021.20220204-1 [21.0 MB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-fonts-recommended all 2021.20220204-1 [4,972 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-base all 2021.20220204-1 [1,128 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfontbox-java all 1:1.8.16-2 [207 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpdfbox-java all 1:1.8.16-2 [5,199 kB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-recommended all 2021.20220204-1 [14.4 MB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-pictures all 2021.20220204-1 [8,720 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-extra all 2021.20220204-1 [13.9 MB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-plain-generic all 2021.20220204-1 [27.5 MB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tipa all 2:1.3-21 [2,967 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-xetex all 2021.20220204-1 [12.4 MB]\n",
            "Fetched 182 MB in 5s (33.9 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 125172 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2.1_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2.1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.17_all.deb ...\n",
            "Unpacking tex-common (6.17) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../04-fonts-urw-base35_20200910-1_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../05-libgs9-common_9.55.0~dfsg1-0ubuntu5.10_all.deb ...\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.10) ...\n",
            "Selecting previously unselected package libidn12:amd64.\n",
            "Preparing to unpack .../06-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../07-libijs-0.35_0.35-15build2_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../08-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../09-libgs9_9.55.0~dfsg1-0ubuntu5.10_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.10) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../10-libkpathsea6_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libwoff1:amd64.\n",
            "Preparing to unpack .../11-libwoff1_1.0.2-1build4_amd64.deb ...\n",
            "Unpacking libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Selecting previously unselected package dvisvgm.\n",
            "Preparing to unpack .../12-dvisvgm_2.13.1-1_amd64.deb ...\n",
            "Unpacking dvisvgm (2.13.1-1) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../13-fonts-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../14-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../15-fonts-texgyre_20180621-3.1_all.deb ...\n",
            "Unpacking fonts-texgyre (20180621-3.1) ...\n",
            "Selecting previously unselected package libapache-pom-java.\n",
            "Preparing to unpack .../16-libapache-pom-java_18-1_all.deb ...\n",
            "Unpacking libapache-pom-java (18-1) ...\n",
            "Selecting previously unselected package libcommons-parent-java.\n",
            "Preparing to unpack .../17-libcommons-parent-java_43-1_all.deb ...\n",
            "Unpacking libcommons-parent-java (43-1) ...\n",
            "Selecting previously unselected package libcommons-logging-java.\n",
            "Preparing to unpack .../18-libcommons-logging-java_1.2-2_all.deb ...\n",
            "Unpacking libcommons-logging-java (1.2-2) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../19-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../20-libptexenc1_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../21-rubygems-integration_1.18_all.deb ...\n",
            "Unpacking rubygems-integration (1.18) ...\n",
            "Selecting previously unselected package ruby3.0.\n",
            "Preparing to unpack .../22-ruby3.0_3.0.2-7ubuntu2.8_amd64.deb ...\n",
            "Unpacking ruby3.0 (3.0.2-7ubuntu2.8) ...\n",
            "Selecting previously unselected package ruby-rubygems.\n",
            "Preparing to unpack .../23-ruby-rubygems_3.3.5-2_all.deb ...\n",
            "Unpacking ruby-rubygems (3.3.5-2) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../24-ruby_1%3a3.0~exp1_amd64.deb ...\n",
            "Unpacking ruby (1:3.0~exp1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../25-rake_13.0.6-2_all.deb ...\n",
            "Unpacking rake (13.0.6-2) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../26-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-webrick.\n",
            "Preparing to unpack .../27-ruby-webrick_1.7.0-3ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-xmlrpc.\n",
            "Preparing to unpack .../28-ruby-xmlrpc_0.3.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libruby3.0:amd64.\n",
            "Preparing to unpack .../29-libruby3.0_3.0.2-7ubuntu2.8_amd64.deb ...\n",
            "Unpacking libruby3.0:amd64 (3.0.2-7ubuntu2.8) ...\n",
            "Selecting previously unselected package libsynctex2:amd64.\n",
            "Preparing to unpack .../30-libsynctex2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libteckit0:amd64.\n",
            "Preparing to unpack .../31-libteckit0_2.5.11+ds1-1_amd64.deb ...\n",
            "Unpacking libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Selecting previously unselected package libtexlua53:amd64.\n",
            "Preparing to unpack .../32-libtexlua53_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../33-libtexluajit2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../34-libzzip-0-13_0.13.72+dfsg.1-1.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../35-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../36-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../37-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../38-preview-latex-style_12.2-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (12.2-1ubuntu1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../39-t1utils_1.41-4build2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-4build2) ...\n",
            "Selecting previously unselected package teckit.\n",
            "Preparing to unpack .../40-teckit_2.5.11+ds1-1_amd64.deb ...\n",
            "Unpacking teckit (2.5.11+ds1-1) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../41-tex-gyre_20180621-3.1_all.deb ...\n",
            "Unpacking tex-gyre (20180621-3.1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../42-texlive-binaries_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../43-texlive-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../44-texlive-fonts-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../45-texlive-latex-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package libfontbox-java.\n",
            "Preparing to unpack .../46-libfontbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libfontbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package libpdfbox-java.\n",
            "Preparing to unpack .../47-libpdfbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libpdfbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../48-texlive-latex-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../49-texlive-pictures_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-pictures (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../50-texlive-latex-extra_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-extra (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../51-texlive-plain-generic_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-plain-generic (2021.20220204-1) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../52-tipa_2%3a1.3-21_all.deb ...\n",
            "Unpacking tipa (2:1.3-21) ...\n",
            "Selecting previously unselected package texlive-xetex.\n",
            "Preparing to unpack .../53-texlive-xetex_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-xetex (2021.20220204-1) ...\n",
            "Setting up fonts-lato (2.0-2.1) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Setting up libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Setting up libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libfontbox-java (1:1.8.16-2) ...\n",
            "Setting up rubygems-integration (1.18) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up tex-common (6.17) ...\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Setting up libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Setting up libapache-pom-java (18-1) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up t1utils (1.41-4build2) ...\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Setting up fonts-texgyre (20180621-3.1) ...\n",
            "Setting up libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Setting up fonts-lmodern (2.004.5-6.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Setting up ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Setting up libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.10) ...\n",
            "Setting up teckit (2.5.11+ds1-1) ...\n",
            "Setting up libpdfbox-java (1:1.8.16-2) ...\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.10) ...\n",
            "Setting up preview-latex-style (12.2-1ubuntu1) ...\n",
            "Setting up libcommons-parent-java (43-1) ...\n",
            "Setting up dvisvgm (2.13.1-1) ...\n",
            "Setting up libcommons-logging-java (1.2-2) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up lmodern (2.004.5-6.1) ...\n",
            "Setting up texlive-base (2021.20220204-1) ...\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/tex-ini-files/pdftexconfig.tex\n",
            "Setting up tex-gyre (20180621-3.1) ...\n",
            "Setting up texlive-plain-generic (2021.20220204-1) ...\n",
            "Setting up texlive-latex-base (2021.20220204-1) ...\n",
            "Setting up texlive-latex-recommended (2021.20220204-1) ...\n",
            "Setting up texlive-pictures (2021.20220204-1) ...\n",
            "Setting up texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Setting up tipa (2:1.3-21) ...\n",
            "Setting up texlive-latex-extra (2021.20220204-1) ...\n",
            "Setting up texlive-xetex (2021.20220204-1) ...\n",
            "Setting up rake (13.0.6-2) ...\n",
            "Setting up libruby3.0:amd64 (3.0.2-7ubuntu2.8) ...\n",
            "Setting up ruby3.0 (3.0.2-7ubuntu2.8) ...\n",
            "Setting up ruby (1:3.0~exp1) ...\n",
            "Setting up ruby-rubygems (3.3.5-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for tex-common (6.17) ...\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "glB8e4sVlSvi"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmAZQluLBehk",
        "outputId": "81db9033-89c2-4184-e102-e8c2de29edc9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "pJ5dyViRmuHq"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(\n",
        "        self,\n",
        "        gemini_model='gemini-2.0-flash-001',\n",
        "        llm_tokens_per_minute=50000,\n",
        "        llm_max_tokens_per_request=4000,\n",
        "        prompt_templates_path='prompts.json'\n",
        "    ):\n",
        "        self.gemini_model = gemini_model\n",
        "        self.llm_tokens_per_minute = llm_tokens_per_minute\n",
        "        self.llm_max_tokens_per_request = llm_max_tokens_per_request\n",
        "        self.prompt_templates_path = prompt_templates_path\n",
        "\n",
        "        # Este lo asignas directamente, o lo puedes volver otro argumento\n",
        "        # si quisieras.\n",
        "\n",
        "        # Parámetro adicional que muestras\n",
        "        self.readability_threshold = 60.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo extenso"
      ],
      "metadata": {
        "id": "_uHyus6HnXmM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hbT4ldvj08Er"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "llm_engine.py\n",
        "-------------\n",
        "Módulo para gestionar la interacción con la API de Google Gemini,\n",
        "optimizado para un agente educativo inteligente.\n",
        "\"\"\"\n",
        "\n",
        "import time\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict\n",
        "import google.generativeai as genai\n",
        "import backoff\n",
        "\n",
        "# Configuración básica de logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# =============================================================================\n",
        "# Configuración del LLM\n",
        "# =============================================================================\n",
        "@dataclass\n",
        "class Config:\n",
        "    gemini_model: str\n",
        "    llm_tokens_per_minute: int\n",
        "    llm_max_tokens_per_request: int\n",
        "    prompt_templates_path: str\n",
        "\n",
        "# =============================================================================\n",
        "# Limitador de Tasa (RateLimiter)\n",
        "# =============================================================================\n",
        "class RateLimiter:\n",
        "    \"\"\"\n",
        "    Controla la tasa de llamadas a la API para evitar exceder el límite\n",
        "    de tokens permitidos por petición.\n",
        "    \"\"\"\n",
        "    def __init__(self, tokens_per_minute: int, max_tokens_per_request: int):\n",
        "        self.tokens_per_minute = tokens_per_minute\n",
        "        self.max_tokens_per_request = max_tokens_per_request\n",
        "        self.tokens_used_in_minute = 0\n",
        "        self.last_reset = time.time()\n",
        "\n",
        "    def wait_if_needed(self, tokens: int) -> None:\n",
        "        \"\"\"\n",
        "        Espera si la suma de tokens usados y los solicitados excede el límite por minuto.\n",
        "\n",
        "        Args:\n",
        "            tokens: Número de tokens que se desean consumir en la petición.\n",
        "        \"\"\"\n",
        "        # Validar que no se soliciten más tokens de los permitidos en una petición\n",
        "        if tokens > self.max_tokens_per_request:\n",
        "            raise ValueError(\n",
        "                f\"El número de tokens solicitados ({tokens}) excede el máximo permitido por petición ({self.max_tokens_per_request}).\"\n",
        "            )\n",
        "\n",
        "        now = time.time()\n",
        "        # Reiniciar el contador si ha pasado más de un minuto\n",
        "        if now - self.last_reset >= 60:\n",
        "            self.tokens_used_in_minute = 0\n",
        "            self.last_reset = now\n",
        "\n",
        "        # Si se excede el límite, se espera hasta el reinicio del contador\n",
        "        if self.tokens_used_in_minute + tokens > self.tokens_per_minute:\n",
        "            time_to_wait = 60 - (now - self.last_reset)\n",
        "            if time_to_wait > 0:\n",
        "                logging.info(f\"Limitador de tasa: esperando {time_to_wait:.2f} segundos para cumplir con el límite de tokens.\")\n",
        "                time.sleep(time_to_wait)\n",
        "                # Reiniciar el contador tras la espera\n",
        "                self.tokens_used_in_minute = tokens\n",
        "                self.last_reset = time.time()\n",
        "        else:\n",
        "            self.tokens_used_in_minute += tokens\n",
        "\n",
        "# =============================================================================\n",
        "# LLMEngine: Interfaz para la API de Google Gemini\n",
        "# =============================================================================\n",
        "class LLMEngine:\n",
        "    \"\"\"\n",
        "    Interfaz para interactuar con la API de Google Gemini, generando contenido educativo\n",
        "    de alta calidad.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        \"\"\"\n",
        "        Inicializa el motor LLM con la configuración proporcionada.\n",
        "\n",
        "        Args:\n",
        "            config: Instancia de Config con los parámetros necesarios.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(\"educational_agent.llm_engine\")\n",
        "\n",
        "        # Verificar la existencia de la variable de entorno con la clave API\n",
        "        api_key = os.environ.get('GOOGLE_API_KEY')\n",
        "        if not api_key:\n",
        "            self.logger.error(\"La variable de entorno 'GOOGLE_API_KEY' no está definida.\")\n",
        "            raise EnvironmentError(\"La variable de entorno 'GOOGLE_API_KEY' es requerida.\")\n",
        "\n",
        "        # Configurar la API de Google Gemini\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel(self.config.gemini_model)\n",
        "\n",
        "        # Inicializar el limitador de tasa\n",
        "        self.rate_limiter = RateLimiter(\n",
        "            tokens_per_minute=self.config.llm_tokens_per_minute,\n",
        "            max_tokens_per_request=self.config.llm_max_tokens_per_request\n",
        "        )\n",
        "\n",
        "        # Cargar las plantillas de prompts\n",
        "        self.prompt_templates = self._load_prompt_templates()\n",
        "\n",
        "        # Instrucción base del sistema, aquí la hemos mejorado\n",
        "        self.system_instruction = (\n",
        "            \"Eres un asistente educativo experto en la creación de materiales didácticos de alta calidad para cursos universitarios. \"\n",
        "            \"Tu tarea es generar contenido claro, preciso y bien estructurado, con rigor académico y ejemplos pertinentes, \"\n",
        "            \"adaptado al nivel de profundidad que se solicita. Responde siempre de forma organizada y autoexplicativa, \"\n",
        "            \"para que sea fácil de entender y aplicar.\"\n",
        "        )\n",
        "\n",
        "    def _load_prompt_templates(self) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Carga las plantillas de prompts desde un archivo JSON.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con las plantillas de prompts.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(self.config.prompt_templates_path, 'r', encoding='utf-8') as f:\n",
        "                templates = json.load(f)\n",
        "                self.logger.info(\"Plantillas de prompts cargadas correctamente.\")\n",
        "                return templates\n",
        "        except Exception as e:\n",
        "            self.logger.warning(\n",
        "                f\"No se pudieron cargar las plantillas desde {self.config.prompt_templates_path}: {str(e)}. Se usarán plantillas por defecto.\"\n",
        "            )\n",
        "            return {\n",
        "                \"lecture_notes\": (\n",
        "                    \"Genera notas de clase detalladas para el tema: {topic_title}. \"\n",
        "                    \"Incluye los siguientes subtemas: {subtopics}. \"\n",
        "                    \"Las notas deben incluir definiciones, explicaciones claras, ejemplos y casos de aplicación.\"\n",
        "                ),\n",
        "                \"practice_problems\": (\n",
        "                    \"Crea problemas de práctica con soluciones paso a paso para el tema: {topic_title}. \"\n",
        "                    \"Los problemas deben cubrir: {subtopics}. \"\n",
        "                    \"Incluye problemas de distintos niveles de dificultad.\"\n",
        "                ),\n",
        "                \"discussion_questions\": (\n",
        "                    \"Genera preguntas para discusión sobre el tema: {topic_title}, considerando: {subtopics}. \"\n",
        "                    \"Las preguntas deben promover el pensamiento crítico y el análisis profundo.\"\n",
        "                ),\n",
        "                \"learning_objectives\": (\n",
        "                    \"Crea objetivos de aprendizaje específicos y medibles para el tema: {topic_title}. \"\n",
        "                    \"Considera los siguientes subtemas: {subtopics}. \"\n",
        "                    \"Usa verbos de la taxonomía de Bloom apropiados.\"\n",
        "                ),\n",
        "                \"suggested_resources\": (\n",
        "                    \"Sugiere recursos de aprendizaje adicionales para el tema: {topic_title}. \"\n",
        "                    \"Incluye libros, artículos, videos, cursos en línea y otros materiales relevantes para: {subtopics}.\"\n",
        "                )\n",
        "            }\n",
        "\n",
        "    @backoff.on_exception(\n",
        "        backoff.expo,\n",
        "        Exception,\n",
        "        max_tries=3,\n",
        "        jitter=backoff.full_jitter,\n",
        "        logger=logging.getLogger(\"educational_agent.llm_engine\")\n",
        "    )\n",
        "    def generate_content(self, prompt: str, max_tokens: int = 1000) -> str:\n",
        "        \"\"\"\n",
        "        Genera contenido educativo llamando a la API de Google Gemini.\n",
        "\n",
        "        Args:\n",
        "            prompt: Instrucción detallada para el modelo.\n",
        "            max_tokens: Número máximo de tokens para la respuesta.\n",
        "\n",
        "        Returns:\n",
        "            Texto generado por el modelo.\n",
        "        \"\"\"\n",
        "        self.logger.debug(f\"Generando contenido con prompt: {prompt[:100]}...\")\n",
        "\n",
        "        # Estimar tokens del prompt (aproximación)\n",
        "        prompt_tokens = len(prompt.split())\n",
        "\n",
        "        # Calcular el total de tokens solicitado (prompt + respuesta)\n",
        "        total_tokens = prompt_tokens + max_tokens\n",
        "\n",
        "        # Ajuste dinámico: si total_tokens supera el límite, reducir max_tokens\n",
        "        if total_tokens > self.rate_limiter.max_tokens_per_request:\n",
        "            available_tokens = self.rate_limiter.max_tokens_per_request - prompt_tokens\n",
        "            if available_tokens <= 0:\n",
        "                raise ValueError(\"El prompt es demasiado largo y no deja espacio para la respuesta.\")\n",
        "            self.logger.info(\n",
        "                f\"Ajustando max_tokens de {max_tokens} a {available_tokens} para cumplir el límite.\"\n",
        "            )\n",
        "            max_tokens = available_tokens\n",
        "            total_tokens = prompt_tokens + max_tokens\n",
        "\n",
        "        # Esperar si es necesario para cumplir con los límites de tokens\n",
        "        self.rate_limiter.wait_if_needed(total_tokens)\n",
        "\n",
        "        try:\n",
        "            # Combinar la instrucción del sistema con el prompt del usuario\n",
        "            full_prompt = f\"{self.system_instruction}\\n\\n{prompt}\"\n",
        "\n",
        "            # Configuración para la generación de contenido\n",
        "            generation_config = {\n",
        "                \"max_output_tokens\": max_tokens,\n",
        "                \"temperature\": 0.7,\n",
        "                \"top_p\": 0.9,\n",
        "                \"top_k\": 40\n",
        "            }\n",
        "\n",
        "            self.logger.debug(\"Llamando a la API de Gemini con la configuración definida.\")\n",
        "            response = self.model.generate_content(\n",
        "                contents=[{\"role\": \"user\", \"parts\": [{\"text\": full_prompt}]}],\n",
        "                generation_config=generation_config\n",
        "            )\n",
        "\n",
        "            # Extraer y retornar el texto generado\n",
        "            generated_text = response.text.strip() if response.text else \"\"\n",
        "            self.logger.debug(\"Contenido generado exitosamente.\")\n",
        "            return generated_text\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error al generar contenido con Gemini: {str(e)}\", exc_info=True)\n",
        "            raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nKplizrd1IQo"
      },
      "outputs": [],
      "source": [
        "# content_generator.py\n",
        "# Módulo para generar los diferentes tipos de contenido educativo\n",
        "\n",
        "import logging\n",
        "import textwrap\n",
        "from typing import Dict, Any\n",
        "\n",
        "class ContentGenerator:\n",
        "    \"\"\"Genera contenido educativo utilizando un motor LLM.\"\"\"\n",
        "\n",
        "    def __init__(self, llm_engine, config):\n",
        "        \"\"\"\n",
        "        Inicializa el generador de contenido.\n",
        "\n",
        "        Args:\n",
        "            llm_engine: Instancia del motor LLM para generación de contenido.\n",
        "            config: Configuración general del sistema.\n",
        "        \"\"\"\n",
        "        self.llm_engine = llm_engine\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(\"educational_agent.content_generator\")\n",
        "\n",
        "    def generate_all_materials(self, syllabus_data: Dict[str, Any]) -> Dict[str, Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Genera todos los materiales didácticos para cada tema del programa.\n",
        "\n",
        "        Args:\n",
        "            syllabus_data: Datos estructurados del programa del curso.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario que asocia cada tema (por ID) a un sub-diccionario con los diferentes tipos de contenido.\n",
        "        \"\"\"\n",
        "        all_content = {}\n",
        "\n",
        "        try:\n",
        "            course_context = {\n",
        "                \"course_title\": syllabus_data[\"course_title\"],\n",
        "                \"course_code\": syllabus_data[\"course_code\"],\n",
        "                \"course_description\": syllabus_data[\"description\"],\n",
        "                \"course_objectives\": syllabus_data[\"objectives\"]\n",
        "            }\n",
        "        except KeyError as e:\n",
        "            self.logger.error(f\"Clave faltante en syllabus_data: {e}\")\n",
        "            raise\n",
        "\n",
        "        for topic in syllabus_data.get(\"topics\", []):\n",
        "            topic_id = topic.get(\"id\")\n",
        "            topic_title = topic.get(\"title\", \"Tema sin título\")\n",
        "            self.logger.info(f\"Generando contenido para el tema {topic_id}: {topic_title}\")\n",
        "\n",
        "            topic_content = {\n",
        "                \"lecture_notes\": self._generate_content_with_context(\n",
        "                    topic, course_context, \"lecture_notes\", additional_context=self._lecture_notes_context(course_context)\n",
        "                ),\n",
        "                \"practice_problems\": self._generate_content_with_context(\n",
        "                    topic, course_context, \"practice_problems\", additional_context=self._practice_problems_context(course_context)\n",
        "                ),\n",
        "                \"discussion_questions\": self._generate_content_with_context(\n",
        "                    topic, course_context, \"discussion_questions\", additional_context=self._discussion_questions_context(course_context)\n",
        "                ),\n",
        "                \"learning_objectives\": self._generate_content_with_context(\n",
        "                    topic, course_context, \"learning_objectives\", additional_context=self._learning_objectives_context(course_context)\n",
        "                ),\n",
        "                \"suggested_resources\": self._generate_content_with_context(\n",
        "                    topic, course_context, \"suggested_resources\", additional_context=self._suggested_resources_context(course_context)\n",
        "                )\n",
        "            }\n",
        "            all_content[topic_id] = topic_content\n",
        "\n",
        "        return all_content\n",
        "\n",
        "    def _generate_content_with_context(\n",
        "        self,\n",
        "        topic: Dict[str, Any],\n",
        "        course_context: Dict[str, Any],\n",
        "        template_key: str,\n",
        "        additional_context: str,\n",
        "        default_max_tokens: int = 2000\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Construye y genera contenido a partir de un template y un contexto adicional.\n",
        "\n",
        "        Args:\n",
        "            topic: Datos del tema.\n",
        "            course_context: Datos generales del curso.\n",
        "            template_key: Clave para obtener la plantilla desde llm_engine.prompt_templates.\n",
        "            additional_context: Texto adicional que se agregará al prompt.\n",
        "            default_max_tokens: Límite de tokens en caso de que no se defina uno específico.\n",
        "\n",
        "        Returns:\n",
        "            Contenido generado como cadena de texto.\n",
        "        \"\"\"\n",
        "        prompt_template = self.llm_engine.prompt_templates.get(template_key, \"\")\n",
        "        if not prompt_template:\n",
        "            self.logger.warning(f\"No se encontró plantilla para {template_key}.\")\n",
        "\n",
        "        subtopics = topic.get(\"subtopics\", [])\n",
        "        subtopics_text = \", \".join(subtopics)\n",
        "        prompt = prompt_template.format(\n",
        "            topic_title=topic.get(\"title\", \"\"),\n",
        "            subtopics=subtopics_text\n",
        "        )\n",
        "\n",
        "        # Unimos el contexto adicional + la plantilla con placeholders ya completados\n",
        "        final_prompt = textwrap.dedent(f\"\"\"\n",
        "            {additional_context.strip()}\n",
        "\n",
        "            {prompt.strip()}\n",
        "        \"\"\")\n",
        "        self.logger.debug(f\"Prompt para {template_key}: {final_prompt[:150]}...\")\n",
        "\n",
        "        # Definir límites específicos de tokens por tipo de contenido\n",
        "        tokens_limit = {\n",
        "            \"lecture_notes\": 4000,\n",
        "            \"practice_problems\": 3000,\n",
        "            \"discussion_questions\": 2000,\n",
        "            \"learning_objectives\": 1500,\n",
        "            \"suggested_resources\": 2000\n",
        "        }\n",
        "        max_tokens = tokens_limit.get(template_key, default_max_tokens)\n",
        "\n",
        "        return self.llm_engine.generate_content(final_prompt, max_tokens=max_tokens)\n",
        "\n",
        "    def _lecture_notes_context(self, course_context: Dict[str, Any]) -> str:\n",
        "        \"\"\"Retorna el contexto adicional para generar notas de clase.\"\"\"\n",
        "        return textwrap.dedent(f\"\"\"\n",
        "            Información del curso:\n",
        "            - Título: {course_context.get('course_title', '')}\n",
        "            - Código: {course_context.get('course_code', '')}\n",
        "            - Descripción: {course_context.get('course_description', '')}\n",
        "\n",
        "            Genera notas de clase completas y detalladas que cubran el tema a profundidad.\n",
        "            Emplea rigor académico, definiciones precisas y ejemplos concretos para cada concepto.\n",
        "            Asegúrate de explicar la relevancia práctica y posibles aplicaciones.\n",
        "            Organiza la información de manera clara y estructurada, de modo que sea fácilmente entendible.\n",
        "        \"\"\")\n",
        "\n",
        "    def _practice_problems_context(self, course_context: Dict[str, Any]) -> str:\n",
        "        \"\"\"Retorna el contexto adicional para generar problemas de práctica.\"\"\"\n",
        "        return textwrap.dedent(f\"\"\"\n",
        "            Para el curso: {course_context.get('course_title', '')} ({course_context.get('course_code', '')})\n",
        "\n",
        "            Genera un conjunto de problemas de práctica que cubran todos los aspectos importantes del tema.\n",
        "            Asegúrate de incluir:\n",
        "            1. Al menos 5 problemas con distintos niveles de dificultad (básico, intermedio, avanzado).\n",
        "            2. Cada problema con una solución paso a paso detallada y justificada.\n",
        "            3. Preguntas conceptuales y aplicadas, integrando el contexto del curso.\n",
        "            4. Conexión a casos de la vida real o de la industria cuando sea pertinente.\n",
        "        \"\"\")\n",
        "\n",
        "    def _discussion_questions_context(self, course_context: Dict[str, Any]) -> str:\n",
        "        \"\"\"Retorna el contexto adicional para generar preguntas de discusión.\"\"\"\n",
        "        return textwrap.dedent(f\"\"\"\n",
        "            Para el curso: {course_context.get('course_title', '')}\n",
        "\n",
        "            Genera un conjunto de preguntas para discusión que:\n",
        "            1. Promuevan el pensamiento crítico y reflexivo.\n",
        "            2. Estimulen el debate y el intercambio de ideas entre los estudiantes.\n",
        "            3. Conecten el tema con problemas actuales o aplicaciones reales.\n",
        "            4. Exploren implicaciones éticas, sociales o económicas cuando sea oportuno.\n",
        "            5. Fomenten la conexión entre este tema y otros contenidos del curso.\n",
        "\n",
        "            Para cada pregunta, incluye una breve nota para el instructor sobre los puntos clave que podrían surgir en el debate.\n",
        "        \"\"\")\n",
        "\n",
        "    def _learning_objectives_context(self, course_context: Dict[str, Any]) -> str:\n",
        "        \"\"\"Retorna el contexto adicional para generar objetivos de aprendizaje.\"\"\"\n",
        "        course_objectives = course_context.get(\"course_objectives\", [])\n",
        "        objectives_str = \", \".join(course_objectives)\n",
        "        return textwrap.dedent(f\"\"\"\n",
        "            Para el curso: {course_context.get('course_title', '')}\n",
        "\n",
        "            Genera objetivos de aprendizaje que:\n",
        "            1. Sean específicos, medibles, alcanzables, relevantes y con un tiempo definido (SMART).\n",
        "            2. Utilicen verbos de acción de la taxonomía de Bloom apropiados para el nivel universitario.\n",
        "            3. Cubran diferentes niveles cognitivos (recordar, comprender, aplicar, analizar, evaluar, crear).\n",
        "            4. Se alineen con los objetivos generales del curso.\n",
        "            5. Sean claras y comprensibles para los estudiantes.\n",
        "\n",
        "            Los objetivos generales del curso incluyen: {objectives_str}.\n",
        "        \"\"\")\n",
        "\n",
        "    def _suggested_resources_context(self, course_context: Dict[str, Any]) -> str:\n",
        "        \"\"\"Retorna el contexto adicional para generar recursos sugeridos.\"\"\"\n",
        "        return textwrap.dedent(f\"\"\"\n",
        "            Para el curso: {course_context.get('course_title', '')}\n",
        "\n",
        "            Genera una lista de recursos de aprendizaje que incluya:\n",
        "            1. Libros de texto principales y complementarios (proporciona autores y años de publicación).\n",
        "            2. Artículos académicos relevantes y actualizados, si es posible de acceso abierto.\n",
        "            3. Recursos en línea de calidad (MOOCs, tutoriales, videos).\n",
        "            4. Herramientas o software relevantes cuando sea aplicable.\n",
        "            5. Recursos para diferentes niveles de conocimiento previo y enfoques de aprendizaje.\n",
        "\n",
        "            Para cada recurso, describe su relevancia y utilidad en relación al tema y al curso.\n",
        "        \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ZR4IyHGk1TRm"
      },
      "outputs": [],
      "source": [
        "# document_processor.py\n",
        "# Módulo para procesar y extraer información de documentos (PDF, DOCX, TXT)\n",
        "\n",
        "import os\n",
        "import re\n",
        "import logging\n",
        "from typing import Dict, List, Any\n",
        "import PyPDF2\n",
        "import docx\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Procesa y extrae información estructurada de documentos que contienen programas de curso.\"\"\"\n",
        "\n",
        "    def __init__(self, config: \"Config\") -> None:\n",
        "        \"\"\"\n",
        "        Inicializa el procesador de documentos y descarga recursos NLTK si es necesario.\n",
        "\n",
        "        Args:\n",
        "            config: Objeto de configuración con parámetros del sistema.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(\"educational_agent.document_processor\")\n",
        "\n",
        "        # Asegurarse de que el tokenizador de NLTK esté disponible\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "        except LookupError:\n",
        "            self.logger.info(\"Descargando recursos NLTK necesarios...\")\n",
        "            nltk.download('punkt')\n",
        "\n",
        "    def process_file(self, file_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Procesa un archivo de programa de curso y extrae su estructura.\n",
        "\n",
        "        Args:\n",
        "            file_path: Ruta al archivo del programa.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con la información estructurada del curso.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"No se encontró el archivo: {file_path}\")\n",
        "\n",
        "        file_ext = os.path.splitext(file_path)[1].lower()\n",
        "        if file_ext == '.pdf':\n",
        "            text = self._extract_text_from_pdf(file_path)\n",
        "        elif file_ext == '.docx':\n",
        "            text = self._extract_text_from_docx(file_path)\n",
        "        elif file_ext == '.txt':\n",
        "            text = self._extract_text_from_txt(file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Formato de archivo no soportado: {file_ext}\")\n",
        "\n",
        "        syllabus_data = self._parse_syllabus(text)\n",
        "        return syllabus_data\n",
        "\n",
        "    def _extract_text_from_pdf(self, file_path: str) -> str:\n",
        "        \"\"\"Extrae y retorna el texto completo de un archivo PDF.\"\"\"\n",
        "        self.logger.info(f\"Extrayendo texto de PDF: {file_path}\")\n",
        "        texts = []\n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                reader = PyPDF2.PdfReader(file)\n",
        "                for page in reader.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        texts.append(page_text)\n",
        "            return \"\\n\".join(texts)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error al extraer texto de PDF: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _extract_text_from_docx(self, file_path: str) -> str:\n",
        "        \"\"\"Extrae y retorna el texto completo de un archivo DOCX.\"\"\"\n",
        "        self.logger.info(f\"Extrayendo texto de DOCX: {file_path}\")\n",
        "        try:\n",
        "            doc = docx.Document(file_path)\n",
        "            texts = [paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()]\n",
        "            return \"\\n\".join(texts)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error al extraer texto de DOCX: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _extract_text_from_txt(self, file_path: str) -> str:\n",
        "        \"\"\"Extrae y retorna el texto completo de un archivo TXT.\"\"\"\n",
        "        self.logger.info(f\"Extrayendo texto de TXT: {file_path}\")\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                return file.read()\n",
        "        except UnicodeDecodeError:\n",
        "            with open(file_path, 'r', encoding='latin-1') as file:\n",
        "                return file.read()\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error al extraer texto de TXT: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def _parse_syllabus(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analiza el texto completo del programa y extrae su estructura.\n",
        "\n",
        "        Args:\n",
        "            text: Texto completo del documento.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con información estructurada del curso.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Analizando la estructura del programa de curso\")\n",
        "        syllabus_data: Dict[str, Any] = {\n",
        "            \"course_title\": \"\",\n",
        "            \"course_code\": \"\",\n",
        "            \"instructor\": \"\",\n",
        "            \"description\": \"\",\n",
        "            \"objectives\": [],\n",
        "            \"topics\": [],\n",
        "            \"evaluation_methods\": [],\n",
        "            \"bibliography\": []\n",
        "        }\n",
        "\n",
        "        sections = self._split_into_sections(text)\n",
        "\n",
        "        syllabus_data[\"course_title\"] = self._extract_course_title(sections)\n",
        "        syllabus_data[\"course_code\"] = self._extract_course_code(sections)\n",
        "        syllabus_data[\"instructor\"] = self._extract_instructor(sections)\n",
        "        syllabus_data[\"description\"] = self._extract_description(sections)\n",
        "        syllabus_data[\"objectives\"] = self._extract_objectives(sections)\n",
        "        syllabus_data[\"topics\"] = self._extract_topics(sections)\n",
        "        syllabus_data[\"evaluation_methods\"] = self._extract_evaluation_methods(sections)\n",
        "        syllabus_data[\"bibliography\"] = self._extract_bibliography(sections)\n",
        "\n",
        "        return syllabus_data\n",
        "\n",
        "    def _split_into_sections(self, text: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Divide el texto en secciones basadas en encabezados comunes.\n",
        "\n",
        "        Args:\n",
        "            text: Texto completo del documento.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario donde las claves son los encabezados y los valores el contenido asociado.\n",
        "        \"\"\"\n",
        "        section_headers = [\n",
        "            r\"(?:TÍTULO|NOMBRE)\\s+DEL\\s+CURSO\",\n",
        "            r\"(?:CÓDIGO|CLAVE)\",\n",
        "            r\"(?:PROFESOR|INSTRUCTOR|DOCENTE)\",\n",
        "            r\"(?:DESCRIPCIÓN|DESCRIPCION)\",\n",
        "            r\"(?:OBJETIVOS|METAS)\",\n",
        "            r\"(?:TEMARIO|CONTENIDO|PROGRAMA|UNIDADES)\",\n",
        "            r\"(?:EVALUACIÓN|EVALUACION|CALIFICACIÓN)\",\n",
        "            r\"(?:BIBLIOGRAFÍA|BIBLIOGRAFIA|REFERENCIAS)\"\n",
        "        ]\n",
        "\n",
        "        sections: Dict[str, str] = {\"preamble\": \"\"}\n",
        "        current_section = \"preamble\"\n",
        "        for line in text.splitlines():\n",
        "            stripped_line = line.strip()\n",
        "            is_header = False\n",
        "            for pattern in section_headers:\n",
        "                if re.search(pattern, stripped_line, re.IGNORECASE):\n",
        "                    current_section = stripped_line\n",
        "                    sections[current_section] = \"\"\n",
        "                    is_header = True\n",
        "                    break\n",
        "            if not is_header:\n",
        "                sections[current_section] += stripped_line + \"\\n\"\n",
        "        return sections\n",
        "\n",
        "    def _extract_course_title(self, sections: Dict[str, str]) -> str:\n",
        "        \"\"\"Extrae el título del curso buscando encabezados o líneas en el preámbulo.\"\"\"\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:TÍTULO|NOMBRE)\\s+DEL\\s+CURSO\", header, re.IGNORECASE):\n",
        "                return content.strip()\n",
        "        # Buscar en el preámbulo las primeras líneas no vacías\n",
        "        preamble_lines = sections.get(\"preamble\", \"\").splitlines()\n",
        "        for line in preamble_lines[:5]:\n",
        "            if line.strip():\n",
        "                return line.strip()\n",
        "        return \"No se pudo determinar el título del curso\"\n",
        "\n",
        "    def _extract_course_code(self, sections: Dict[str, str]) -> str:\n",
        "        \"\"\"Extrae el código del curso mediante encabezados o patrones en el texto.\"\"\"\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:CÓDIGO|CLAVE)\", header, re.IGNORECASE):\n",
        "                return content.strip()\n",
        "        for content in sections.values():\n",
        "            code_match = re.search(r\"\\b[A-Z]{2,4}\\s*\\d{3,4}\\b\", content)\n",
        "            if code_match:\n",
        "                return code_match.group(0).strip()\n",
        "        return \"No se pudo determinar el código del curso\"\n",
        "\n",
        "    def _extract_instructor(self, sections: Dict[str, str]) -> str:\n",
        "        \"\"\"Extrae el nombre del instructor del curso.\"\"\"\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:PROFESOR|INSTRUCTOR|DOCENTE)\", header, re.IGNORECASE):\n",
        "                return content.strip()\n",
        "        return \"No se pudo determinar el instructor del curso\"\n",
        "\n",
        "    def _extract_description(self, sections: Dict[str, str]) -> str:\n",
        "        \"\"\"Extrae la descripción del curso.\"\"\"\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:DESCRIPCIÓN|DESCRIPCION)\", header, re.IGNORECASE):\n",
        "                return content.strip()\n",
        "        return \"No se encontró descripción del curso\"\n",
        "\n",
        "    def _extract_objectives(self, sections: Dict[str, str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extrae los objetivos del curso identificando listas o frases relevantes.\n",
        "\n",
        "        Returns:\n",
        "            Lista de objetivos.\n",
        "        \"\"\"\n",
        "        objectives: List[str] = []\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:OBJETIVOS|METAS)\", header, re.IGNORECASE):\n",
        "                for line in content.splitlines():\n",
        "                    line_clean = line.strip()\n",
        "                    if line_clean:\n",
        "                        # Detectar listas con guiones, viñetas o numeración\n",
        "                        if line_clean[0] in \"-•\" or re.match(r\"^\\d+\\.\", line_clean):\n",
        "                            objectives.append(line_clean.lstrip(\"-•0123456789. \").strip())\n",
        "                        elif len(line_clean) > 20:\n",
        "                            # Dividir en oraciones y agregar las suficientemente largas\n",
        "                            for sentence in sent_tokenize(line_clean):\n",
        "                                if len(sentence.strip()) > 20:\n",
        "                                    objectives.append(sentence.strip())\n",
        "        return objectives\n",
        "\n",
        "    def _extract_topics(self, sections: Dict[str, str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Extrae el temario o unidades del curso mediante patrones complejos y métodos alternativos.\n",
        "\n",
        "        Returns:\n",
        "            Lista de diccionarios, cada uno representando un tema o unidad.\n",
        "        \"\"\"\n",
        "        topics: List[Dict[str, Any]] = []\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:TEMARIO|CONTENIDO|PROGRAMA|UNIDADES)\", header, re.IGNORECASE):\n",
        "                # Intento por patrones formales: Unidad, Tema, Módulo o Capítulo\n",
        "                unit_pattern = re.compile(\n",
        "                    r\"(?:Unidad|Tema|Módulo|Capítulo)\\s+(\\d+|[IVXLCDM]+)[\\s:.]+(.+?)(?=(?:Unidad|Tema|Módulo|Capítulo)\\s+\\d+|$)\",\n",
        "                    re.IGNORECASE | re.DOTALL\n",
        "                )\n",
        "                for match in unit_pattern.finditer(content):\n",
        "                    unit_num = match.group(1)\n",
        "                    unit_text = match.group(2).strip()\n",
        "                    # Intentar extraer subtemas mediante listas o dividiendo en oraciones\n",
        "                    subtopics: List[str] = []\n",
        "                    lines = [l.strip() for l in unit_text.splitlines() if l.strip()]\n",
        "                    if lines:\n",
        "                        title = re.sub(r\"^\\d+\\.\\s*\", \"\", lines[0])\n",
        "                        for line in lines[1:]:\n",
        "                            if line[0] in \"-•\" or re.match(r\"^\\d+\\.\\d+\", line):\n",
        "                                subtopics.append(line.lstrip(\"-•0123456789. \").strip())\n",
        "                    else:\n",
        "                        title = unit_text.splitlines()[0] if unit_text else \"Sin título\"\n",
        "                        subtopics = [s.strip() for s in sent_tokenize(unit_text)[1:] if len(s.strip()) > 10]\n",
        "\n",
        "                    topics.append({\n",
        "                        \"id\": unit_num,\n",
        "                        \"title\": title,\n",
        "                        \"subtopics\": subtopics\n",
        "                    })\n",
        "                # Si no se detectaron temas con el patrón formal, intentar otro método\n",
        "                if not topics:\n",
        "                    numbered_lines = re.finditer(r\"^\\s*(\\d+)\\.\\s*(.+)$\", content, re.MULTILINE)\n",
        "                    current_topic = None\n",
        "                    for match in numbered_lines:\n",
        "                        num, text_line = match.group(1), match.group(2).strip()\n",
        "                        if len(num) == 1:\n",
        "                            current_topic = {\"id\": num, \"title\": text_line, \"subtopics\": []}\n",
        "                            topics.append(current_topic)\n",
        "                        elif current_topic is not None:\n",
        "                            current_topic[\"subtopics\"].append(text_line)\n",
        "                # Fallback: cada línea no vacía se considera un tema\n",
        "                if not topics:\n",
        "                    for idx, line in enumerate(content.splitlines(), start=1):\n",
        "                        line_clean = line.strip()\n",
        "                        if line_clean and len(line_clean) > 5:\n",
        "                            topics.append({\n",
        "                                \"id\": str(idx),\n",
        "                                \"title\": line_clean,\n",
        "                                \"subtopics\": []\n",
        "                            })\n",
        "        return topics\n",
        "\n",
        "    def _extract_evaluation_methods(self, sections: Dict[str, str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Extrae los métodos de evaluación y sus porcentajes.\n",
        "\n",
        "        Returns:\n",
        "            Lista de diccionarios con el método y su porcentaje.\n",
        "        \"\"\"\n",
        "        evaluation_methods: List[Dict[str, Any]] = []\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:EVALUACIÓN|EVALUACION|CALIFICACIÓN)\", header, re.IGNORECASE):\n",
        "                # Buscar patrones del tipo \"Examen: 30%\"\n",
        "                for match in re.finditer(r\"([^:]+):\\s*(\\d+)%\", content):\n",
        "                    method = match.group(1).strip()\n",
        "                    percentage = int(match.group(2))\n",
        "                    evaluation_methods.append({\"method\": method, \"percentage\": percentage})\n",
        "                # Alternativa: buscar líneas que contengan '%'\n",
        "                if not evaluation_methods:\n",
        "                    for line in content.splitlines():\n",
        "                        if '%' in line:\n",
        "                            parts = line.split('%')\n",
        "                            percentage_match = re.search(r\"(\\d+)\\s*$\", parts[0])\n",
        "                            if percentage_match:\n",
        "                                percentage = int(percentage_match.group(1))\n",
        "                                method = re.sub(r\"\\d+\\s*$\", \"\", parts[0]).strip()\n",
        "                                evaluation_methods.append({\"method\": method, \"percentage\": percentage})\n",
        "        return evaluation_methods\n",
        "\n",
        "    def _extract_bibliography(self, sections: Dict[str, str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extrae la bibliografía o referencias del curso.\n",
        "\n",
        "        Returns:\n",
        "            Lista de entradas bibliográficas.\n",
        "        \"\"\"\n",
        "        bibliography: List[str] = []\n",
        "        for header, content in sections.items():\n",
        "            if re.search(r\"(?:BIBLIOGRAFÍA|BIBLIOGRAFIA|REFERENCIAS)\", header, re.IGNORECASE):\n",
        "                entries = []\n",
        "                current_entry = \"\"\n",
        "                for line in content.splitlines():\n",
        "                    line = line.strip()\n",
        "                    if not line:\n",
        "                        if current_entry:\n",
        "                            entries.append(current_entry)\n",
        "                            current_entry = \"\"\n",
        "                    else:\n",
        "                        if not current_entry and (line[0] in \"-•\" or re.match(r\"^\\d+\\.\", line)):\n",
        "                            current_entry = line.lstrip(\"-•0123456789. \").strip()\n",
        "                        elif not current_entry:\n",
        "                            current_entry = line\n",
        "                        else:\n",
        "                            current_entry += \" \" + line\n",
        "                if current_entry:\n",
        "                    entries.append(current_entry)\n",
        "                # Si no se hallaron entradas con el formato anterior, usar líneas no vacías\n",
        "                bibliography = entries if entries else [l.strip() for l in content.splitlines() if l.strip() and len(l.strip()) > 10]\n",
        "        return bibliography"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files"
      ],
      "metadata": {
        "id": "EwggaLOiyx1V"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HpZY4XZE5c0g",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "754f0e07-171b-4d19-dd7e-afc3a48613f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "WARNING:educational_agent.llm_engine:No se pudieron cargar las plantillas desde prompts.json: [Errno 2] No such file or directory: 'prompts.json'. Se usarán plantillas por defecto.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Por favor, sube el archivo del programa de curso (ej. PROGRAMA_DE_CURSO.pdf)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-95de7c8c-a81a-454a-812b-d4bb3ba03568\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-95de7c8c-a81a-454a-812b-d4bb3ba03568\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving PROGRAMA_DE_CURSO.pdf to PROGRAMA_DE_CURSO.pdf\n",
            "Datos extraídos del syllabus:\n",
            "{'course_title': '', 'course_code': '', 'instructor': 'DURACIÓN: 16 semanas\\nCRÉDITOS: 4', 'description': 'Este curso introductorio cubre los conceptos básicos de la Inteligencia Artificial, incluyendo\\nalgoritmos de búsqueda, aprendizaje automático y procesamiento del lenguaje natural. Los\\nestudiantes desarrollarán habilidades prácticas mediante proyectos aplic ados.', 'objectives': ['Comprender los fundamentos teóricos de la IA', 'Aplicar algoritmos de búsqueda y optimización', 'Desarrollar modelos básicos de machine learning', 'Analizar casos de estudio de aplicaciones reales', 'Implementar soluciones simples de PLN'], 'topics': [{'id': '1', 'title': 'Introducción a la IA', 'subtopics': ['Historia de la IA', 'Agentes inteligentes', 'Entornos y tareas', 'Ética en IA']}, {'id': '2', 'title': 'Algoritmos de Búsqueda', 'subtopics': ['Búsqueda no informada', 'Búsqueda heurística', 'Algoritmos A*', 'Optimización con algoritmos genéticos']}, {'id': '3', 'title': 'Machine Learning Básico', 'subtopics': ['Regresión lineal', 'Árboles de decisión', 'Clustering']}], 'evaluation_methods': [{'method': '- Examen parcial', 'percentage': 30}, {'method': '- Proyecto práctico', 'percentage': 40}, {'method': '- Participación en clase', 'percentage': 10}, {'method': '- Tareas semanales', 'percentage': 20}], 'bibliography': ['Russell, S. & Norvig, P. - \"Artificial Intelligence: A Modern Approach\" (4ª ed) 2. Géron, A. - \"Hands-On Machine Learning with Scikit -Learn, Keras, and TensorFlow\" 3. Artículo: \"Attention Is All You Need\" (Vaswani et al., 2017) 4. Recursos en línea: Coursera - \"AI For Everyone\" (Andrew Ng)']}\n"
          ]
        }
      ],
      "source": [
        "# Definir los valores necesarios para la configuración\n",
        "gemini_model = 'gemini-2.0-flash-001'\n",
        "llm_tokens_per_minute = 50000\n",
        "llm_max_tokens_per_request = 4000\n",
        "prompt_templates_path = 'prompts.json'\n",
        "\n",
        "# Crear la instancia de Config con los argumentos requeridos\n",
        "config = Config(\n",
        "    gemini_model=gemini_model,\n",
        "    llm_tokens_per_minute=llm_tokens_per_minute,\n",
        "    llm_max_tokens_per_request=llm_max_tokens_per_request,\n",
        "    prompt_templates_path=prompt_templates_path\n",
        ")\n",
        "processor = DocumentProcessor(config)\n",
        "\n",
        "# Inicializar el motor LLM utilizando la configuración\n",
        "llm_engine = LLMEngine(config)\n",
        "\n",
        "# Crear una instancia de ContentGenerator, pasando llm_engine y config\n",
        "generador = ContentGenerator(llm_engine, config)\n",
        "\n",
        "try:\n",
        "    print(\"Por favor, sube el archivo del programa de curso (ej. PROGRAMA_DE_CURSO.pdf)\")\n",
        "    uploaded = files.upload()  # Abre el selector de archivos en Colab\n",
        "\n",
        "    # Verificar que se haya subido al menos un archivo\n",
        "    if not uploaded:\n",
        "        print(\"No se ha subido ningún archivo. Saliendo...\")\n",
        "    else:\n",
        "        # Tomar el primer archivo subido\n",
        "        file_path = list(uploaded.keys())[0]\n",
        "\n",
        "        # Procesar el archivo y extraer la información del syllabus\n",
        "        syllabus_data = processor.process_file(file_path)\n",
        "\n",
        "        # Mostrar los datos extraídos\n",
        "        print(\"Datos extraídos del syllabus:\")\n",
        "        print(syllabus_data)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error al procesar el archivo: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "contenido = generador.generate_all_materials(syllabus_data)"
      ],
      "metadata": {
        "id": "jwkmCoBRXQf-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "6hJWB9CcmuHs"
      },
      "outputs": [],
      "source": [
        "import pypandoc\n",
        "import markdown\n",
        "import logging\n",
        "\n",
        "# Configurar logging\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def dict_to_pdf(materials: dict, output_pdf: str = \"materiales_curso.pdf\") -> None:\n",
        "    \"\"\"\n",
        "    Convierte un diccionario de materiales a un archivo PDF.\n",
        "\n",
        "    El diccionario se transforma a un string en formato Markdown, el cual se convierte a HTML y finalmente a PDF\n",
        "    usando pypandoc.\n",
        "\n",
        "    Args:\n",
        "        materials (dict): Diccionario con la estructura de temas y secciones.\n",
        "        output_pdf (str): Nombre del archivo PDF de salida (por defecto \"materiales_curso.pdf\").\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Construir el contenido en Markdown usando una lista para mayor eficiencia\n",
        "        md_lines = []\n",
        "        for topic_id, content in materials.items():\n",
        "            md_lines.append(f\"# Tema {topic_id}\\n\")\n",
        "            for section, text in content.items():\n",
        "                section_title = section.replace('_', ' ').title()\n",
        "                md_lines.append(f\"## {section_title}\\n\")\n",
        "                md_lines.append(f\"{text}\\n\")\n",
        "            md_lines.append(\"\\n---\\n\")  # Separador entre temas\n",
        "        md_content = \"\\n\".join(md_lines)\n",
        "        logger.info(\"Contenido en Markdown construido correctamente.\")\n",
        "\n",
        "        # Convertir el Markdown a HTML (opcional, ya que pypandoc puede trabajar directamente con Markdown)\n",
        "        html = markdown.markdown(md_content)\n",
        "        logger.info(\"Conversión de Markdown a HTML completada.\")\n",
        "\n",
        "        # Opciones adicionales para pypandoc: usar XeLaTeX y una fuente compatible con Unicode\n",
        "        extra_args = [\n",
        "            '--pdf-engine=xelatex',\n",
        "            '-V', 'mainfont=Times New Roman'\n",
        "        ]\n",
        "\n",
        "        # Convertir HTML a PDF y guardar el archivo\n",
        "        pypandoc.convert_text(html, 'pdf', format='html', outputfile=output_pdf, extra_args=extra_args)\n",
        "        logger.info(f\"Archivo PDF guardado: {output_pdf}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error durante la conversión a PDF: {e}\", exc_info=True)\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "dQxoLkO7muHs"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from typing import Dict\n",
        "\n",
        "# Configurar logging a nivel de módulo\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def save_materials_as_text_files(materials: Dict[str, Dict[str, str]], output_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Guarda los materiales generados en archivos de texto.\n",
        "\n",
        "    Cada tema se guarda en una subcarpeta, y para cada tipo de contenido se crea un archivo.\n",
        "\n",
        "    Args:\n",
        "        materials (Dict[str, Dict[str, str]]): Diccionario con la estructura de temas y secciones.\n",
        "        output_dir (str): Directorio base donde se guardarán los archivos.\n",
        "    \"\"\"\n",
        "    # Crear el directorio base (y sus subdirectorios) si no existen\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for topic_id, content in materials.items():\n",
        "        # Crear una subcarpeta para cada tema\n",
        "        topic_dir = os.path.join(output_dir, f\"tema_{topic_id}\")\n",
        "        os.makedirs(topic_dir, exist_ok=True)\n",
        "\n",
        "        # Guardar cada tipo de material en un archivo distinto\n",
        "        for content_type, text in content.items():\n",
        "            filename = os.path.join(topic_dir, f\"{content_type}.txt\")\n",
        "            try:\n",
        "                with open(filename, 'w', encoding='utf-8') as f:\n",
        "                    f.write(text)\n",
        "                logger.info(f\"Archivo guardado: {filename}\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error al escribir el archivo {filename}: {e}\", exc_info=True)\n",
        "\n",
        "    logger.info(f\"Materiales guardados en: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_materials_as_text_files(contenido, \"materiales_generados\")"
      ],
      "metadata": {
        "id": "IedYTagMXSs4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk-sGAwBmuHs",
        "outputId": "f4b7591b-3fff-4e63-998f-81298774680f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'topic_scores': {'1': {'lecture_notes': 0.8931531531531531, 'practice_problems': 0.8999999999999999, 'discussion_questions': 0.7916666666666666, 'learning_objectives': 0.6904761904761904, 'suggested_resources': 0.9333333333333332, 'average': 0.8417258687258686}, '2': {'lecture_notes': 0.8764572460169472, 'practice_problems': 0.8999999999999999, 'discussion_questions': 0.6666666666666666, 'learning_objectives': 0.6904761904761904, 'suggested_resources': 0.9333333333333332, 'average': 0.8133866872986275}, '3': {'lecture_notes': 0.8817948717948717, 'practice_problems': 0.8999999999999999, 'discussion_questions': 0.5277777777777778, 'learning_objectives': 0.7904761904761904, 'suggested_resources': 0.9333333333333332, 'average': 0.8066764346764346}}, 'content_type_scores': {'lecture_notes': 0.883801756988324, 'practice_problems': 0.8999999999999999, 'discussion_questions': 0.6620370370370371, 'learning_objectives': 0.7238095238095238, 'suggested_resources': 0.9333333333333332}, 'overall_metrics': {'relevance_score': 0.3333333333333333, 'consistency_score': 0.7060118979455794, 'readability_score': 0.3159782260646814, 'domain_terminology_score': 0.25098874353513845}, 'average_score': 0.6110871902266635}\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Asegurarse de que NLTK cuente con los recursos necesarios\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Configurar logging a nivel de módulo\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# =============================================================================\n",
        "# Clase Evaluator: Evaluación del contenido educativo generado\n",
        "# =============================================================================\n",
        "class Evaluator:\n",
        "    \"\"\"Evalúa la calidad del contenido educativo generado utilizando diversas métricas.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Any) -> None:\n",
        "        \"\"\"\n",
        "        Inicializa el evaluador con la configuración dada.\n",
        "\n",
        "        Args:\n",
        "            config: Configuración general (se puede ampliar según las necesidades).\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(\"educational_agent.evaluator\")\n",
        "\n",
        "    def evaluate_content(\n",
        "        self,\n",
        "        generated_content: Dict[str, Dict[str, str]],\n",
        "        syllabus_data: Dict[str, Any]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evalúa el contenido generado mediante varias métricas:\n",
        "          - Relevancia (cobertura de subtemas)\n",
        "          - Consistencia (similitud de coseno entre secciones)\n",
        "          - Legibilidad (longitud promedio de oraciones)\n",
        "          - Uso de terminología específica (basada en TF-IDF)\n",
        "\n",
        "        Args:\n",
        "            generated_content: Diccionario con materiales generados, organizados por tema y tipo.\n",
        "            syllabus_data: Datos estructurados del syllabus del curso.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con los resultados de la evaluación.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Iniciando evaluación del contenido generado\")\n",
        "\n",
        "        evaluation_results: Dict[str, Any] = {\n",
        "            \"topic_scores\": {},\n",
        "            \"content_type_scores\": {\n",
        "                \"lecture_notes\": 0,\n",
        "                \"practice_problems\": 0,\n",
        "                \"discussion_questions\": 0,\n",
        "                \"learning_objectives\": 0,\n",
        "                \"suggested_resources\": 0\n",
        "            },\n",
        "            \"overall_metrics\": {\n",
        "                \"relevance_score\": 0,\n",
        "                \"consistency_score\": 0,\n",
        "                \"readability_score\": 0,\n",
        "                \"domain_terminology_score\": 0\n",
        "            },\n",
        "            \"average_score\": 0\n",
        "        }\n",
        "\n",
        "        # Extraer terminología clave del syllabus para evaluar su uso en el contenido\n",
        "        course_terminology = self._extract_domain_terminology(syllabus_data)\n",
        "\n",
        "        # Evaluar cada tema individualmente\n",
        "        for topic_id, topic_content in generated_content.items():\n",
        "            # Buscar información del tema en el syllabus (por ID)\n",
        "            topic_info = next((t for t in syllabus_data[\"topics\"] if t[\"id\"] == topic_id), None)\n",
        "            if not topic_info:\n",
        "                self.logger.warning(f\"No se encontró información para el tema {topic_id} en el syllabus\")\n",
        "                continue\n",
        "\n",
        "            # Extraer textos por tipo de contenido\n",
        "            lecture = topic_content.get(\"lecture_notes\", \"\")\n",
        "            practice = topic_content.get(\"practice_problems\", \"\")\n",
        "            discussion = topic_content.get(\"discussion_questions\", \"\")\n",
        "            objectives = topic_content.get(\"learning_objectives\", \"\")\n",
        "            resources = topic_content.get(\"suggested_resources\", \"\")\n",
        "\n",
        "            # Calcular puntuaciones para cada tipo de contenido\n",
        "            topic_scores: Dict[str, float] = {\n",
        "                \"lecture_notes\": self._evaluate_lecture_notes(lecture, topic_info, course_terminology),\n",
        "                \"practice_problems\": self._evaluate_practice_problems(practice, topic_info),\n",
        "                \"discussion_questions\": self._evaluate_discussion_questions(discussion, topic_info),\n",
        "                \"learning_objectives\": self._evaluate_learning_objectives(objectives, topic_info),\n",
        "                \"suggested_resources\": self._evaluate_suggested_resources(resources, topic_info)\n",
        "            }\n",
        "\n",
        "            # Calcular la puntuación promedio para el tema\n",
        "            topic_avg = sum(topic_scores.values()) / len(topic_scores)\n",
        "            topic_scores[\"average\"] = topic_avg\n",
        "            evaluation_results[\"topic_scores\"][topic_id] = topic_scores\n",
        "\n",
        "            # Acumular las puntuaciones para cada tipo de contenido\n",
        "            for key, score in topic_scores.items():\n",
        "                if key != \"average\" and key in evaluation_results[\"content_type_scores\"]:\n",
        "                    evaluation_results[\"content_type_scores\"][key] += score\n",
        "\n",
        "        # Promediar las puntuaciones por tipo de contenido\n",
        "        num_topics = len(generated_content)\n",
        "        if num_topics > 0:\n",
        "            for key in evaluation_results[\"content_type_scores\"]:\n",
        "                evaluation_results[\"content_type_scores\"][key] /= num_topics\n",
        "\n",
        "        # Calcular métricas globales basadas en todo el contenido generado\n",
        "        all_texts = self._join_all_texts(generated_content)\n",
        "        overall_consistency = self._evaluate_consistency(generated_content)\n",
        "        overall_readability = self._calculate_readability(all_texts)\n",
        "        overall_terminology = self._calculate_terminology_usage(all_texts, course_terminology)\n",
        "\n",
        "        evaluation_results[\"overall_metrics\"][\"consistency_score\"] = overall_consistency\n",
        "        evaluation_results[\"overall_metrics\"][\"readability_score\"] = overall_readability\n",
        "        evaluation_results[\"overall_metrics\"][\"domain_terminology_score\"] = overall_terminology\n",
        "\n",
        "        # Relevancia: promedio de cobertura de subtemas en las notas de clase\n",
        "        overall_relevance = np.mean([\n",
        "            self._calculate_subtopic_coverage(topic_content.get(\"lecture_notes\", \"\"), topic_info[\"subtopics\"])\n",
        "            for topic_id, topic_content in generated_content.items()\n",
        "            if topic_id in [t[\"id\"] for t in syllabus_data[\"topics\"]]\n",
        "        ])\n",
        "        evaluation_results[\"overall_metrics\"][\"relevance_score\"] = overall_relevance\n",
        "\n",
        "        # Calcular la puntuación global promedio\n",
        "        content_type_avg = np.mean(list(evaluation_results[\"content_type_scores\"].values()))\n",
        "        overall_metrics_avg = np.mean(list(evaluation_results[\"overall_metrics\"].values()))\n",
        "        evaluation_results[\"average_score\"] = (content_type_avg + overall_metrics_avg) / 2\n",
        "\n",
        "        self.logger.info(f\"Evaluación completada. Puntuación promedio: {evaluation_results['average_score']:.2f}\")\n",
        "        return evaluation_results\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Métodos auxiliares para análisis global del contenido\n",
        "    # -------------------------------------------------------------------------\n",
        "    def _join_all_texts(self, generated_content: Dict[str, Dict[str, str]]) -> str:\n",
        "        \"\"\"Une todo el texto de todos los temas y secciones para análisis global.\"\"\"\n",
        "        texts: List[str] = []\n",
        "        for topic_content in generated_content.values():\n",
        "            texts.extend(topic_content.values())\n",
        "        return \"\\n\".join(texts)\n",
        "\n",
        "    def _calculate_readability(self, text: str) -> float:\n",
        "        \"\"\"\n",
        "        Calcula la legibilidad basada en la longitud promedio de las oraciones.\n",
        "        Se asume que oraciones más cortas facilitan la lectura.\n",
        "        \"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        words = word_tokenize(text)\n",
        "        if not sentences:\n",
        "            return 0.0\n",
        "        avg_sentence_length = len(words) / len(sentences)\n",
        "        if avg_sentence_length <= 15:\n",
        "            return 1.0\n",
        "        elif avg_sentence_length >= 30:\n",
        "            return 0.0\n",
        "        else:\n",
        "            return (30 - avg_sentence_length) / 15\n",
        "\n",
        "    def _extract_domain_terminology(self, syllabus_data: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extrae términos clave del syllabus combinando la descripción, objetivos, temario y bibliografía.\n",
        "\n",
        "        Returns:\n",
        "            Lista de términos extraídos mediante TF-IDF.\n",
        "        \"\"\"\n",
        "        combined_text = syllabus_data.get(\"description\", \"\")\n",
        "        for obj in syllabus_data.get(\"objectives\", []):\n",
        "            combined_text += \" \" + obj\n",
        "        for topic in syllabus_data.get(\"topics\", []):\n",
        "            combined_text += \" \" + topic.get(\"title\", \"\")\n",
        "            for subtopic in topic.get(\"subtopics\", []):\n",
        "                combined_text += \" \" + subtopic\n",
        "        for bib in syllabus_data.get(\"bibliography\", []):\n",
        "            combined_text += \" \" + bib\n",
        "\n",
        "        vectorizer = TfidfVectorizer(max_features=50, stop_words='english', ngram_range=(1, 2))\n",
        "        try:\n",
        "            tfidf_matrix = vectorizer.fit_transform([combined_text])\n",
        "            feature_names = vectorizer.get_feature_names_out()\n",
        "            return list(feature_names)\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"Error extrayendo terminología: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _calculate_terminology_usage(self, text: str, terminology: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calcula la densidad de uso de la terminología específica en el contenido.\n",
        "\n",
        "        Returns:\n",
        "            Valor entre 0 y 1 indicando la densidad.\n",
        "        \"\"\"\n",
        "        content_lower = text.lower()\n",
        "        term_count = sum(1 for term in terminology if term.lower() in content_lower)\n",
        "        word_count = len(word_tokenize(text))\n",
        "        term_density = (term_count * 1000) / word_count if word_count > 0 else 0\n",
        "        # Se asume que 5 términos por cada 1000 palabras es ideal\n",
        "        return min(1.0, term_density / 5)\n",
        "\n",
        "    def _calculate_subtopic_coverage(self, content: str, subtopics: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calcula la proporción de subtemas cubiertos en el contenido.\n",
        "\n",
        "        Returns:\n",
        "            Valor entre 0 y 1 indicando la cobertura.\n",
        "        \"\"\"\n",
        "        content_lower = content.lower()\n",
        "        covered = 0\n",
        "        for sub in subtopics:\n",
        "            sub_terms = [w.lower() for w in word_tokenize(sub) if len(w) > 3]\n",
        "            if sub_terms and (sum(1 for term in sub_terms if term in content_lower) / len(sub_terms)) >= 0.5:\n",
        "                covered += 1\n",
        "        return covered / len(subtopics) if subtopics else 0\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Métodos de evaluación específicos para cada tipo de contenido\n",
        "    # -------------------------------------------------------------------------\n",
        "    def _evaluate_lecture_notes(self, lecture_notes: str, topic_info: Dict[str, Any], course_terminology: List[str]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(lecture_notes, topic_info.get(\"subtopics\", [])),\n",
        "            self._calculate_terminology_usage(lecture_notes, course_terminology),\n",
        "            self._calculate_readability(lecture_notes),\n",
        "            self._evaluate_content_structure(lecture_notes),\n",
        "            self._evaluate_examples_presence(lecture_notes)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    def _evaluate_practice_problems(self, practice_problems: str, topic_info: Dict[str, Any]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(practice_problems, topic_info.get(\"subtopics\", [])),\n",
        "            self._evaluate_solutions_presence(practice_problems),\n",
        "            self._evaluate_difficulty_variety(practice_problems),\n",
        "            self._evaluate_problem_clarity(practice_problems)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    def _evaluate_discussion_questions(self, discussion_questions: str, topic_info: Dict[str, Any]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(discussion_questions, topic_info.get(\"subtopics\", [])),\n",
        "            self._evaluate_critical_thinking(discussion_questions),\n",
        "            self._evaluate_open_ended_questions(discussion_questions)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    def _evaluate_learning_objectives(self, learning_objectives: str, topic_info: Dict[str, Any]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(learning_objectives, topic_info.get(\"subtopics\", [])),\n",
        "            self._evaluate_bloom_taxonomy_usage(learning_objectives),\n",
        "            self._evaluate_measurable_objectives(learning_objectives)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    def _evaluate_suggested_resources(self, suggested_resources: str, topic_info: Dict[str, Any]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(suggested_resources, topic_info.get(\"subtopics\", [])),\n",
        "            self._evaluate_resource_variety(suggested_resources),\n",
        "            self._evaluate_resource_detail(suggested_resources)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Métodos \"stub\" o simples para evaluación adicional\n",
        "    # -------------------------------------------------------------------------\n",
        "    def _evaluate_content_structure(self, text: str) -> float:\n",
        "        paragraphs = [p for p in text.split(\"\\n\\n\") if p.strip()]\n",
        "        num_paragraphs = len(paragraphs)\n",
        "        return 1.0 if num_paragraphs >= 5 else (num_paragraphs / 5.0 if num_paragraphs else 0.0)\n",
        "\n",
        "    def _evaluate_examples_presence(self, text: str) -> float:\n",
        "        return 1.0 if \"ejemplo\" in text.lower() else 0.0\n",
        "\n",
        "    def _evaluate_solutions_presence(self, text: str) -> float:\n",
        "        return 1.0 if (\"solución\" in text.lower() or \"solución:\" in text.lower()) else 0.0\n",
        "\n",
        "    def _evaluate_difficulty_variety(self, text: str) -> float:\n",
        "        # Valor fijo de ejemplo; puede ser mejorado con un análisis más detallado.\n",
        "        return 0.8\n",
        "\n",
        "    def _evaluate_problem_clarity(self, text: str) -> float:\n",
        "        # Ejemplo simple: se asume claridad si el texto tiene cierta puntuación.\n",
        "        return 0.8\n",
        "\n",
        "    def _evaluate_critical_thinking(self, text: str) -> float:\n",
        "        keywords = [\"analiza\", \"discute\", \"reflexiona\", \"argumenta\"]\n",
        "        count = sum(1 for kw in keywords if kw in text.lower())\n",
        "        return min(1.0, count / len(keywords))\n",
        "\n",
        "    def _evaluate_open_ended_questions(self, text: str) -> float:\n",
        "        questions = [line for line in text.splitlines() if \"?\" in line]\n",
        "        if not questions:\n",
        "            return 0.0\n",
        "        open_count = sum(1 for q in questions if not re.search(r'\\b(?:sí|no)\\b', q.lower()))\n",
        "        return open_count / len(questions)\n",
        "\n",
        "    def _evaluate_bloom_taxonomy_usage(self, text: str) -> float:\n",
        "        bloom_verbs = [\"analiza\", \"aplica\", \"compara\", \"evalúa\", \"crea\", \"sintetiza\", \"interpreta\"]\n",
        "        count = sum(1 for verb in bloom_verbs if verb in text.lower())\n",
        "        return min(1.0, count / len(bloom_verbs))\n",
        "\n",
        "    def _evaluate_measurable_objectives(self, text: str) -> float:\n",
        "        return 0.8 if any(kw in text.lower() for kw in [\"porcentaje\", \"número\", \"cuantifica\"]) else 0.5\n",
        "\n",
        "    def _evaluate_resource_variety(self, text: str) -> float:\n",
        "        lines = [l for l in text.splitlines() if l.strip()]\n",
        "        return min(1.0, len(lines) / 5.0)\n",
        "\n",
        "    def _evaluate_resource_detail(self, text: str) -> float:\n",
        "        return 0.8\n",
        "\n",
        "    def _evaluate_consistency(self, generated_content: Dict[str, Dict[str, str]]) -> float:\n",
        "        \"\"\"\n",
        "        Evalúa la consistencia semántica entre temas usando TF-IDF y similitud de coseno.\n",
        "        Se asume que mayor similitud entre temas indica mayor consistencia.\n",
        "        \"\"\"\n",
        "        texts: List[str] = []\n",
        "        for topic_content in generated_content.values():\n",
        "            combined = \" \".join(topic_content.values())\n",
        "            texts.append(combined)\n",
        "        if len(texts) < 2:\n",
        "            return 1.0\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf = vectorizer.fit_transform(texts)\n",
        "        similarity_matrix = cosine_similarity(tfidf)\n",
        "        n = similarity_matrix.shape[0]\n",
        "        sum_sim = np.sum(similarity_matrix) - n  # Excluir la diagonal\n",
        "        num_elements = n * (n - 1)\n",
        "        avg_similarity = sum_sim / num_elements if num_elements > 0 else 1.0\n",
        "        return avg_similarity\n",
        "\n",
        "# =============================================================================\n",
        "# Ejemplo de evaluación (se asume que 'config', 'contenido' y 'syllabus_data' están definidos)\n",
        "# =============================================================================\n",
        "# Crear una instancia del evaluador (la variable 'config' debe estar definida en otro módulo)\n",
        "evaluator = Evaluator(config)\n",
        "evaluacion = evaluator.evaluate_content(contenido, syllabus_data)\n",
        "print(evaluacion)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_materials_as_text_files(materials: dict, output_dir: str):\n",
        "    \"\"\"\n",
        "    Guarda los materiales generados en archivos de texto.\n",
        "\n",
        "    Cada tema se guarda en una subcarpeta, y para cada tipo de contenido se crea un archivo.\n",
        "\n",
        "    Args:\n",
        "        materials: Diccionario con los materiales generados.\n",
        "        output_dir: Directorio base donde se guardarán los archivos.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for topic_id, content in materials.items():\n",
        "        # Crear una subcarpeta para cada tema, usando el ID o el título (si se prefiere)\n",
        "        topic_dir = os.path.join(output_dir, f\"tema_{topic_id}\")\n",
        "        if not os.path.exists(topic_dir):\n",
        "            os.makedirs(topic_dir)\n",
        "\n",
        "        # Guardar cada tipo de material en un archivo distinto\n",
        "        for content_type, text in content.items():\n",
        "            filename = os.path.join(topic_dir, f\"{content_type}.txt\")\n",
        "            with open(filename, 'w', encoding='utf-8') as f:\n",
        "                f.write(text)\n",
        "    print(f\"Materiales guardados en: {output_dir}\")"
      ],
      "metadata": {
        "id": "9dFqTtl1XDZw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pypandoc\n",
        "import os\n",
        "\n",
        "# ✅ Instalar dependencias en Google Colab\n",
        "!apt-get install -y texlive-xetex texlive-fonts-recommended texlive-plain-generic\n",
        "!apt-get install -y fonts-freefont-ttf  # Fuente alternativa FreeSerif (parecida a Times New Roman)\n",
        "!pip install pypandoc\n",
        "\n",
        "# ✅ Función para convertir un diccionario en un PDF\n",
        "def dict_to_pdf(materials: dict, output_pdf: str = \"materiales_curso.pdf\"):\n",
        "    \"\"\"\n",
        "    Convierte un diccionario de materiales a un PDF.\n",
        "\n",
        "    Args:\n",
        "        materials: Diccionario con la estructura de temas y secciones.\n",
        "        output_pdf: Nombre del archivo PDF de salida.\n",
        "    \"\"\"\n",
        "    # 🔹 Convertir el diccionario a Markdown\n",
        "    md_content = \"\"\n",
        "    for topic_id, content in materials.items():\n",
        "        md_content += f\"# Tema {topic_id}\\n\\n\"\n",
        "        for section, text in content.items():\n",
        "            section_title = section.replace('_', ' ').title()\n",
        "            md_content += f\"## {section_title}\\n\\n\"\n",
        "            md_content += text + \"\\n\\n\"\n",
        "        md_content += \"\\n---\\n\\n\"  # Separador entre temas\n",
        "\n",
        "    # 🔹 Mostrar una vista previa para depuración\n",
        "    print(\"📜 Vista previa del contenido Markdown:\\n\", md_content[:500])\n",
        "\n",
        "    # 🔹 Verificar que el contenido no está vacío\n",
        "    if not md_content.strip():\n",
        "        raise ValueError(\"⚠️ Error: El contenido del PDF está vacío.\")\n",
        "\n",
        "    # 🔹 Usar \"FreeSerif\", una fuente segura en LaTeX que funciona en Google Colab\n",
        "    extra_args = [\n",
        "        '--pdf-engine=xelatex',\n",
        "        '-V', 'mainfont=\"FreeSerif\"',\n",
        "    ]\n",
        "\n",
        "    # 🔹 Convertir Markdown a PDF\n",
        "    try:\n",
        "        print(\"⏳ Convirtiendo Markdown a PDF...\")\n",
        "        pypandoc.convert_text(md_content, 'pdf', format='md', outputfile=output_pdf, extra_args=extra_args)\n",
        "\n",
        "        # 🔹 Verificar si el PDF fue creado correctamente\n",
        "        if os.path.exists(output_pdf):\n",
        "            print(f\"✅ PDF generado con éxito: {output_pdf}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(\"❌ Error: La conversión a PDF no generó un archivo.\")\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(f\"❌ Error durante la conversión a PDF: {e}\")\n",
        "        return False  # Indica fallo\n",
        "\n",
        "    return True  # Indica éxito\n",
        "\n",
        "# ✅ Llamar a la función para generar el PDF\n",
        "pdf_created = dict_to_pdf(contenido, \"materiales_curso.pdf\")\n",
        "\n",
        "# ✅ Solo descargar si el PDF se generó correctamente\n",
        "if pdf_created and os.path.exists(\"materiales_curso.pdf\"):\n",
        "    from google.colab import files\n",
        "    print(\"📥 Descargando PDF...\")\n",
        "    files.download(\"materiales_curso.pdf\")\n",
        "else:\n",
        "    print(\"⚠️ No se pudo generar el archivo PDF. Revisa los errores anteriores.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "zcdafh1X1LrY",
        "outputId": "67dd018d-18b5-4c0a-8023-0a851ceb6603"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "texlive-fonts-recommended is already the newest version (2021.20220204-1).\n",
            "texlive-plain-generic is already the newest version (2021.20220204-1).\n",
            "texlive-xetex is already the newest version (2021.20220204-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-freefont-ttf\n",
            "0 upgraded, 1 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 2,388 kB of archives.\n",
            "After this operation, 6,653 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-freefont-ttf all 20120503-10build1 [2,388 kB]\n",
            "Fetched 2,388 kB in 0s (5,666 kB/s)\n",
            "Selecting previously unselected package fonts-freefont-ttf.\n",
            "(Reading database ... 162001 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-freefont-ttf_20120503-10build1_all.deb ...\n",
            "Unpacking fonts-freefont-ttf (20120503-10build1) ...\n",
            "Setting up fonts-freefont-ttf (20120503-10build1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Requirement already satisfied: pypandoc in /usr/local/lib/python3.11/dist-packages (1.15)\n",
            "📜 Vista previa del contenido Markdown:\n",
            " # Tema 1\n",
            "\n",
            "## Lecture Notes\n",
            "\n",
            "¡Absolutamente! Aquí tienes unas notas de clase detalladas sobre la Introducción a la Inteligencia Artificial, diseñadas para un curso universitario introductorio.\n",
            "\n",
            "**Notas de Clase: Introducción a la Inteligencia Artificial**\n",
            "\n",
            "**Curso:** [Nombre del Curso]\n",
            "**Código:** [Código del Curso]\n",
            "\n",
            "**I. ¿Qué es la Inteligencia Artificial?**\n",
            "\n",
            "*   **Definición Formal:** La Inteligencia Artificial (IA) es la rama de la ciencia de la computación que se ocupa del diseño y desarrollo\n",
            "⏳ Convirtiendo Markdown a PDF...\n",
            "✅ PDF generado con éxito: materiales_curso.pdf\n",
            "📥 Descargando PDF...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5557d7ad-eeb4-49c8-9be3-e6a3d82ad5c8\", \"materiales_curso.pdf\", 216784)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_colab_font():\n",
        "    try:\n",
        "        # Instalar fuente DejaVu si no está disponible\n",
        "        !apt-get update -qq > /dev/null\n",
        "        !apt-get install -qq fonts-dejavu > /dev/null\n",
        "        return \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\"\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "l-0pMtuAMSie"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para ajustar el texto y justificarlo\n",
        "def wrap_text_justified(text, max_width, font):\n",
        "    words = text.split()\n",
        "    lines = []\n",
        "    current_line = []\n",
        "    current_width = 0\n",
        "\n",
        "    effective_width = max_width - 2 * LEFT_MARGIN\n",
        "\n",
        "    for word in words:\n",
        "        word_width = font.getlength(word + \" \")\n",
        "        if current_width + word_width <= effective_width:\n",
        "            current_line.append(word)\n",
        "            current_width += word_width\n",
        "        else:\n",
        "            lines.append(current_line)\n",
        "            current_line = [word]\n",
        "            current_width = word_width\n",
        "\n",
        "    if current_line:  # Añadir la última línea\n",
        "        lines.append(current_line)\n",
        "\n",
        "    justified_lines = []\n",
        "    for i, line in enumerate(lines):\n",
        "        if i == len(lines) - 1 or len(line) <= 1:\n",
        "            justified_lines.append(\" \".join(line))\n",
        "        else:\n",
        "            total_spaces = max_width - sum(font.getlength(word) for word in line)\n",
        "            spaces_needed = len(line) - 1\n",
        "            if spaces_needed > 0:\n",
        "                space_width = total_spaces / spaces_needed\n",
        "                result = line[0]\n",
        "                for word in line[1:]:\n",
        "                    result += \" \" * int(space_width) + word\n",
        "                justified_lines.append(result)\n",
        "            else:\n",
        "                justified_lines.append(\" \".join(line))\n",
        "\n",
        "    return \"\\n\".join(justified_lines)"
      ],
      "metadata": {
        "id": "WU4p5gLgMUXL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from PIL import Image, ImageDraw, ImageFont, ImageFilter\n",
        "\n",
        "# -----------------------------------------------\n",
        "# CONFIGURACIÓN: Número de palabras por línea\n",
        "WORDS_PER_LINE = 12\n",
        "LEFT_MARGIN = 40\n",
        "# -----------------------------------------------\n",
        "\n",
        "def wrap_text_by_words(text, words_per_line=WORDS_PER_LINE):\n",
        "    \"\"\"\n",
        "    Recibe un texto y lo divide en líneas de 'words_per_line' palabras.\n",
        "    Retorna el texto con saltos de línea (\\n) insertados.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    lines = []\n",
        "    for i in range(0, len(words), words_per_line):\n",
        "        lines.append(\" \".join(words[i:i+words_per_line]))\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def create_gradient_background(width, height, top_color, bottom_color):\n",
        "    \"\"\"\n",
        "    Crea un fondo degradado vertical de top_color a bottom_color.\n",
        "    \"\"\"\n",
        "    base = Image.new('RGB', (width, height), top_color)\n",
        "    top = Image.new('RGB', (width, height), bottom_color)\n",
        "    mask = Image.new('L', (width, height))\n",
        "    mask_data = []\n",
        "    for y in range(height):\n",
        "        mask_data.extend([int(255 * (y / height))] * width)\n",
        "    mask.putdata(mask_data)\n",
        "    base.paste(top, (0, 0), mask)\n",
        "    return base\n",
        "\n",
        "def draw_text_with_shadow(draw_obj, position, text, font, text_color, spacing=8, align=\"left\"):\n",
        "    \"\"\"\n",
        "    Dibuja el texto con una sombra para darle efecto de relieve.\n",
        "    \"\"\"\n",
        "    x, y = position\n",
        "    draw_obj.multiline_text(\n",
        "        (x, y),\n",
        "        text,\n",
        "        font=font,\n",
        "        fill=text_color,\n",
        "        spacing=spacing,\n",
        "        align=align\n",
        "    )\n",
        "\n",
        "def parse_discussion_questions(file_path):\n",
        "    \"\"\"\n",
        "    Lee el archivo `discussion_questions.txt` y extrae cada pregunta y su nota para el instructor.\n",
        "    Retorna una lista de diccionarios con las claves:\n",
        "    [\n",
        "      {\n",
        "        'pregunta': 'Texto de la pregunta...',\n",
        "        'nota_instructor': 'Texto de la nota para el instructor...'\n",
        "      },\n",
        "      ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        return []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = [line.strip() for line in f]\n",
        "\n",
        "    questions_data = []\n",
        "    current_question = None\n",
        "    current_note = None\n",
        "    capturing_question = False\n",
        "    capturing_note = False\n",
        "\n",
        "    for line in lines:\n",
        "        if \"**Pregunta:**\" in line:\n",
        "            # Guardar la anterior (si existe)\n",
        "            if current_question and current_note:\n",
        "                questions_data.append({\n",
        "                    'pregunta': current_question.strip(),\n",
        "                    'nota_instructor': current_note.strip()\n",
        "                })\n",
        "            # Nueva pregunta\n",
        "            current_question = line.split(\"**Pregunta:**\")[-1].strip()\n",
        "            current_note = \"\"\n",
        "            capturing_question = True\n",
        "            capturing_note = False\n",
        "            continue\n",
        "\n",
        "        if \"**Nota para el Instructor:**\" in line:\n",
        "            capturing_question = False\n",
        "            capturing_note = True\n",
        "            current_note += line.split(\"**Nota para el Instructor:**\")[-1].strip() + \" \"\n",
        "            continue\n",
        "\n",
        "        if capturing_question:\n",
        "            current_question += \" \" + line\n",
        "\n",
        "        if capturing_note:\n",
        "            current_note += \" \" + line\n",
        "\n",
        "    # Guardar la última si quedó pendiente\n",
        "    if current_question and current_note:\n",
        "        questions_data.append({\n",
        "            'pregunta': current_question.strip(),\n",
        "            'nota_instructor': current_note.strip()\n",
        "        })\n",
        "\n",
        "    return questions_data\n",
        "\n",
        "def parse_practice_problems_universal(file_path):\n",
        "    \"\"\"\n",
        "    Parser \"universal\" para practice_problems.txt, que maneja distintos estilos.\n",
        "\n",
        "    Retorna una lista de diccionarios con:\n",
        "    [\n",
        "      {\n",
        "        'titulo_problema': '...',\n",
        "        'enunciado': '...',\n",
        "        'solucion': '...'\n",
        "      },\n",
        "      ...\n",
        "    ]\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        return []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = [line.strip() for line in f]\n",
        "\n",
        "    problems_data = []\n",
        "    current_title = \"\"\n",
        "    current_enunciado = \"\"\n",
        "    current_solucion = \"\"\n",
        "    capturing_enunciado = False\n",
        "    capturing_solucion = False\n",
        "\n",
        "    def guardar_problema_si_existe():\n",
        "        if current_title.strip() or current_enunciado.strip() or current_solucion.strip():\n",
        "            problems_data.append({\n",
        "                'titulo_problema': current_title.strip(),\n",
        "                'enunciado': current_enunciado.strip(),\n",
        "                'solucion': current_solucion.strip()\n",
        "            })\n",
        "\n",
        "    for line in lines:\n",
        "        # Detectar línea de inicio de problema\n",
        "        if (line.startswith(\"**\")\n",
        "            and (\"Enunciado:\" not in line)\n",
        "            and (\"Solución:\" not in line)\n",
        "            and (\"Problema:\" not in line)):\n",
        "            if current_title or current_enunciado or current_solucion:\n",
        "                guardar_problema_si_existe()\n",
        "            current_title = line.replace(\"**\", \"\").strip()\n",
        "            current_enunciado = \"\"\n",
        "            current_solucion = \"\"\n",
        "            capturing_enunciado = False\n",
        "            capturing_solucion = False\n",
        "            continue\n",
        "\n",
        "        # Detectar enunciado\n",
        "        if re.search(r\"\\*\\*Enunciado:\\*\\*|\\*\\*Problema:\\*\\*\", line):\n",
        "            capturing_enunciado = True\n",
        "            capturing_solucion = False\n",
        "            if \"**Enunciado:**\" in line:\n",
        "                current_enunciado += \" \" + line.split(\"**Enunciado:**\")[-1].strip()\n",
        "            elif \"**Problema:**\" in line:\n",
        "                current_enunciado += \" \" + line.split(\"**Problema:**\")[-1].strip()\n",
        "            continue\n",
        "\n",
        "        # Detectar solución\n",
        "        if \"**Solución:**\" in line:\n",
        "            capturing_enunciado = False\n",
        "            capturing_solucion = True\n",
        "            current_solucion += \" \" + line.split(\"**Solución:**\")[-1].strip()\n",
        "            continue\n",
        "\n",
        "        # Seguir capturando enunciado\n",
        "        if capturing_enunciado:\n",
        "            current_enunciado += \" \" + line\n",
        "\n",
        "        # Seguir capturando solución\n",
        "        if capturing_solucion:\n",
        "            current_solucion += \" \" + line\n",
        "\n",
        "    # Guardar lo que haya quedado\n",
        "    if current_title or current_enunciado or current_solucion:\n",
        "        guardar_problema_si_existe()\n",
        "\n",
        "    return problems_data\n",
        "\n",
        "def generar_flashcard_imagen(titulo, contenido, output_path):\n",
        "    ancho, alto = 900, 650\n",
        "    fondo_top = (220, 230, 255)\n",
        "    fondo_bottom = (255, 255, 255)\n",
        "    imagen = create_gradient_background(ancho, alto, fondo_top, fondo_bottom)\n",
        "    imagen = imagen.convert(\"RGBA\")\n",
        "    dibujo = ImageDraw.Draw(imagen)\n",
        "\n",
        "    card_margin = 50\n",
        "    card_width = ancho - 2 * card_margin\n",
        "    card_height = alto - 2 * card_margin\n",
        "    card_radius = 20\n",
        "    card_color = (255, 255, 255, 255)\n",
        "    card_outline = (60, 120, 220, 255)\n",
        "    shadow_offset = 8\n",
        "\n",
        "    shadow = Image.new('RGBA', (ancho, alto), (0, 0, 0, 0))\n",
        "    shadow_draw = ImageDraw.Draw(shadow)\n",
        "    shadow_box = (\n",
        "        card_margin + shadow_offset,\n",
        "        card_margin + shadow_offset,\n",
        "        card_margin + card_width + shadow_offset,\n",
        "        card_margin + card_height + shadow_offset\n",
        "    )\n",
        "    shadow_draw.rounded_rectangle(shadow_box, radius=card_radius, fill=(0, 0, 0, 80))\n",
        "    shadow = shadow.filter(ImageFilter.GaussianBlur(10))\n",
        "    imagen = Image.alpha_composite(imagen, shadow)\n",
        "    dibujo = ImageDraw.Draw(imagen)\n",
        "\n",
        "    card_box = (card_margin, card_margin, card_margin + card_width, card_margin + card_height)\n",
        "    dibujo.rounded_rectangle(card_box, radius=card_radius, fill=card_color, outline=card_outline, width=3)\n",
        "\n",
        "    colab_font = get_colab_font()\n",
        "    try:\n",
        "        titulo_font = ImageFont.truetype(colab_font, 28)\n",
        "        texto_font = ImageFont.truetype(colab_font, 22)\n",
        "        subtitulo_font = ImageFont.truetype(colab_font, 24)\n",
        "    except Exception:\n",
        "        titulo_font = ImageFont.load_default()\n",
        "        texto_font = ImageFont.load_default()\n",
        "        subtitulo_font = ImageFont.load_default()\n",
        "\n",
        "    # Calcular posiciones para layout mejorado\n",
        "    margin_left = card_margin + LEFT_MARGIN\n",
        "    y_position = card_margin + 30\n",
        "\n",
        "    # Dibujar título centrado\n",
        "    draw_text_with_shadow(\n",
        "        dibujo,\n",
        "        ((card_width - dibujo.textlength(titulo, font=titulo_font)) // 2 + card_margin, y_position),\n",
        "        titulo,\n",
        "        titulo_font,\n",
        "        (20, 60, 120),  # Azul oscuro para título\n",
        "        spacing=8,\n",
        "        align=\"center\"\n",
        "    )\n",
        "\n",
        "    y_position += dibujo.textbbox((0, 0), titulo, font=titulo_font)[3] + 30\n",
        "\n",
        "    # Procesar contenido para separar las secciones\n",
        "    secciones = []\n",
        "    lineas = contenido.split('\\n')\n",
        "    texto_actual = \"\"\n",
        "    titulo_actual = None\n",
        "\n",
        "    for linea in lineas:\n",
        "        if \"Solución:\" in linea or \"Nota para el Instructor:\" in linea:\n",
        "            if texto_actual:\n",
        "                secciones.append((titulo_actual, texto_actual.strip()))\n",
        "            titulo_actual = linea.split(\":\")[0] + \":\"\n",
        "            texto_actual = linea.split(\":\", 1)[1] if \":\" in linea else \"\"\n",
        "        else:\n",
        "            texto_actual += \" \" + linea\n",
        "\n",
        "    # Añadir la última sección\n",
        "    if texto_actual:\n",
        "        secciones.append((titulo_actual, texto_actual.strip()))\n",
        "\n",
        "    # Si no hay secciones, usar el contenido completo\n",
        "    if not secciones:\n",
        "        available_width = card_width - 2 * LEFT_MARGIN\n",
        "        wrapped_text = wrap_text_justified(contenido, available_width, texto_font)\n",
        "        draw_text_with_shadow(\n",
        "            dibujo,\n",
        "            (margin_left, y_position),\n",
        "            wrapped_text,\n",
        "            texto_font,\n",
        "            (0, 0, 0),\n",
        "            spacing=8,\n",
        "            align=\"left\"\n",
        "        )\n",
        "        # Calcular altura del texto para posicionar el siguiente\n",
        "        text_bbox = dibujo.multiline_textbbox((0, 0), wrapped_text, font=texto_font, spacing=8)\n",
        "        y_position += text_bbox[3] - text_bbox[1] + 25\n",
        "    else:\n",
        "        # Dibujar cada sección con su subtítulo\n",
        "        for i, (subtitulo, texto) in enumerate(secciones):\n",
        "            if i == 0 and not subtitulo:  # Primera sección sin subtítulo (enunciado)\n",
        "                available_width = card_width - 2 * LEFT_MARGIN\n",
        "                wrapped_text = wrap_text_justified(texto, available_width, texto_font)\n",
        "                draw_text_with_shadow(\n",
        "                    dibujo,\n",
        "                    (margin_left, y_position),\n",
        "                    wrapped_text,\n",
        "                    texto_font,\n",
        "                    (0, 0, 0),\n",
        "                    spacing=8,\n",
        "                    align=\"left\"\n",
        "                )\n",
        "                # Calcular altura del texto para posicionar el siguiente\n",
        "                text_bbox = dibujo.multiline_textbbox((0, 0), wrapped_text, font=texto_font, spacing=8)\n",
        "                y_position += text_bbox[3] - text_bbox[1] + 25\n",
        "            else:\n",
        "                # Dibujar subtítulo\n",
        "                draw_text_with_shadow(\n",
        "                    dibujo,\n",
        "                    (margin_left, y_position),\n",
        "                    subtitulo,\n",
        "                    subtitulo_font,\n",
        "                    (70, 70, 140),  # Color para subtítulos\n",
        "                    spacing=8,\n",
        "                    align=\"left\"\n",
        "                )\n",
        "                y_position += dibujo.textbbox((0, 0), subtitulo, font=subtitulo_font)[3] + 15\n",
        "\n",
        "                # Dibujar texto de la sección\n",
        "                wrapped_text = wrap_text_justified(texto, available_width, texto_font)\n",
        "                draw_text_with_shadow(\n",
        "                    dibujo,\n",
        "                    (margin_left + 10, y_position),  # Indentación adicional\n",
        "                    wrapped_text,\n",
        "                    texto_font,\n",
        "                    (0, 0, 0),\n",
        "                    spacing=8,\n",
        "                    align=\"left\"\n",
        "                )\n",
        "                # Calcular altura del texto para posicionar el siguiente\n",
        "                text_bbox = dibujo.multiline_textbbox((0, 0), wrapped_text, font=texto_font, spacing=8)\n",
        "                y_position += text_bbox[3] - text_bbox[1] + 25\n",
        "\n",
        "    imagen.convert(\"RGB\").save(output_path)\n",
        "    print(f\"Flashcard generada en: {output_path}\")\n",
        "\n",
        "def generar_flashcards_solo_tema_1(base_dir=\"materiales_generados\"):\n",
        "    \"\"\"\n",
        "    Procesa únicamente la carpeta \"tema_1\" dentro de base_dir.\n",
        "    Busca discussion_questions.txt y practice_problems.txt,\n",
        "    y genera las flashcards (si existen esos archivos).\n",
        "    \"\"\"\n",
        "    tema_path = os.path.join(base_dir, \"tema_1\")\n",
        "    if not os.path.isdir(tema_path):\n",
        "        print(f\"No existe la carpeta: {tema_path}. Saliendo...\")\n",
        "        return\n",
        "\n",
        "    print(f\"Procesando carpeta: {tema_path}\")\n",
        "\n",
        "    discussion_file = os.path.join(tema_path, \"discussion_questions.txt\")\n",
        "    practice_file = os.path.join(tema_path, \"practice_problems.txt\")\n",
        "\n",
        "    # 1) Discussion Questions\n",
        "    dq_data = parse_discussion_questions(discussion_file)\n",
        "    if dq_data:\n",
        "        dq_flashcards_dir = os.path.join(tema_path, \"flashcards_discussion\")\n",
        "        os.makedirs(dq_flashcards_dir, exist_ok=True)\n",
        "        for i, item in enumerate(dq_data, start=1):\n",
        "            pregunta = item['pregunta']\n",
        "            nota_instructor = item['nota_instructor']\n",
        "            titulo = \"Pregunta\"\n",
        "            contenido = f\"{pregunta}\\n\\nNota para el Instructor:\\n{nota_instructor}\"\n",
        "            output_file = os.path.join(dq_flashcards_dir, f\"discussion_flashcard_{i}.png\")\n",
        "            generar_flashcard_imagen(titulo, contenido, output_file)\n",
        "    else:\n",
        "        print(\"No se encontraron Discussion Questions en:\", tema_path)\n",
        "\n",
        "    # 2) Practice Problems\n",
        "    pp_data = parse_practice_problems_universal(practice_file)\n",
        "    if pp_data:\n",
        "        pp_flashcards_dir = os.path.join(tema_path, \"flashcards_practice\")\n",
        "        os.makedirs(pp_flashcards_dir, exist_ok=True)\n",
        "        for i, item in enumerate(pp_data, start=1):\n",
        "            titulo_problema = item['titulo_problema']\n",
        "            enunciado = item['enunciado']\n",
        "            solucion = item['solucion']\n",
        "            titulo = titulo_problema if titulo_problema else \"Problema\"\n",
        "            contenido = f\"{enunciado}\\n\\nSolución:\\n{solucion}\"\n",
        "            output_file = os.path.join(pp_flashcards_dir, f\"practice_flashcard_{i}.png\")\n",
        "            generar_flashcard_imagen(titulo, contenido, output_file)\n",
        "    else:\n",
        "        print(\"No se encontraron Practice Problems en:\", tema_path)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generar_flashcards_solo_tema_1(\"materiales_generados\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zujpNm_ee37O",
        "outputId": "80fac527-7125-4efc-e582-845ed62d5dbd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesando carpeta: materiales_generados/tema_1\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_discussion/discussion_flashcard_1.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_discussion/discussion_flashcard_2.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_discussion/discussion_flashcard_3.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_discussion/discussion_flashcard_4.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_discussion/discussion_flashcard_5.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_practice/practice_flashcard_1.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_practice/practice_flashcard_2.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_practice/practice_flashcard_3.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_practice/practice_flashcard_4.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_practice/practice_flashcard_5.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_practice/practice_flashcard_6.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_practice/practice_flashcard_7.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_practice/practice_flashcard_8.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_practice/practice_flashcard_9.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_practice/practice_flashcard_10.png\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Flashcard generada en: materiales_generados/tema_1/flashcards_practice/practice_flashcard_11.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo sintetizado"
      ],
      "metadata": {
        "id": "PXuvkafNnSYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Any, Union\n",
        "import google.generativeai as genai\n",
        "import backoff\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential\n",
        "\n",
        "# Configuración mejorada de logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# Configuración del LLM\n",
        "# =============================================================================\n",
        "@dataclass\n",
        "class Config:\n",
        "    gemini_model: str = \"gemini-1.5-pro\"  # Modelo por defecto\n",
        "    llm_tokens_per_minute: int = 90000    # Límite por defecto para Gemini Pro\n",
        "    llm_max_tokens_per_request: int = 32000  # Límite por defecto\n",
        "    prompt_templates_path: str = \"templates/prompts.json\"\n",
        "    temperature_by_content: Dict[str, float] = None  # Temperaturas específicas por tipo de contenido\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Valores por defecto para temperaturas por tipo de contenido\n",
        "        if self.temperature_by_content is None:\n",
        "            self.temperature_by_content = {\n",
        "                \"lecture_notes\": 0.3,      # Más preciso y estructurado\n",
        "                \"practice_problems\": 0.5,  # Balanceado\n",
        "                \"discussion_questions\": 0.7,  # Más creativo\n",
        "                \"learning_objectives\": 0.2,   # Muy preciso\n",
        "                \"suggested_resources\": 0.4    # Moderadamente creativo\n",
        "            }\n",
        "\n",
        "# =============================================================================\n",
        "# Limitador de Tasa (RateLimiter)\n",
        "# =============================================================================\n",
        "class RateLimiter:\n",
        "    \"\"\"\n",
        "    Controla la tasa de llamadas a la API para evitar exceder el límite\n",
        "    de tokens permitidos por petición.\n",
        "    \"\"\"\n",
        "    def __init__(self, tokens_per_minute: int, max_tokens_per_request: int):\n",
        "        self.tokens_per_minute = tokens_per_minute\n",
        "        self.max_tokens_per_request = max_tokens_per_request\n",
        "        self.tokens_used_in_minute = 0\n",
        "        self.last_reset = time.time()\n",
        "        self.logger = logging.getLogger(\"educational_agent.rate_limiter\")\n",
        "\n",
        "    def wait_if_needed(self, tokens: int) -> None:\n",
        "        \"\"\"\n",
        "        Espera si la suma de tokens usados y los solicitados excede el límite por minuto.\n",
        "\n",
        "        Args:\n",
        "            tokens: Número de tokens que se desean consumir en la petición.\n",
        "        \"\"\"\n",
        "        # Validar que no se soliciten más tokens de los permitidos en una petición\n",
        "        if tokens > self.max_tokens_per_request:\n",
        "            raise ValueError(\n",
        "                f\"El número de tokens solicitados ({tokens}) excede el máximo permitido por petición ({self.max_tokens_per_request}).\"\n",
        "            )\n",
        "\n",
        "        now = time.time()\n",
        "        # Reiniciar el contador si ha pasado más de un minuto\n",
        "        if now - self.last_reset >= 60:\n",
        "            self.logger.debug(f\"Reinicio del contador de tokens. Usados anteriormente: {self.tokens_used_in_minute}\")\n",
        "            self.tokens_used_in_minute = 0\n",
        "            self.last_reset = now\n",
        "\n",
        "        # Si se excede el límite, se espera hasta el reinicio del contador\n",
        "        if self.tokens_used_in_minute + tokens > self.tokens_per_minute:\n",
        "            time_to_wait = 60 - (now - self.last_reset)\n",
        "            if time_to_wait > 0:\n",
        "                self.logger.info(f\"Limitador de tasa: esperando {time_to_wait:.2f} segundos para cumplir con el límite de tokens.\")\n",
        "                time.sleep(time_to_wait)\n",
        "                # Reiniciar el contador tras la espera\n",
        "                self.tokens_used_in_minute = tokens\n",
        "                self.last_reset = time.time()\n",
        "        else:\n",
        "            self.tokens_used_in_minute += tokens\n",
        "            self.logger.debug(f\"Tokens acumulados en este minuto: {self.tokens_used_in_minute}\")\n",
        "\n",
        "# =============================================================================\n",
        "# Cache de prompts y respuestas\n",
        "# =============================================================================\n",
        "class ResponseCache:\n",
        "    \"\"\"\n",
        "    Clase para cachear respuestas del LLM basadas en el prompt,\n",
        "    para evitar llamadas redundantes a la API.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_size: int = 100):\n",
        "        self.cache: Dict[str, str] = {}\n",
        "        self.max_size = max_size\n",
        "        self.logger = logging.getLogger(\"educational_agent.response_cache\")\n",
        "\n",
        "    def get(self, prompt_key: str) -> Optional[str]:\n",
        "        \"\"\"Obtiene una respuesta cacheada si existe.\"\"\"\n",
        "        return self.cache.get(prompt_key)\n",
        "\n",
        "    def set(self, prompt_key: str, response: str) -> None:\n",
        "        \"\"\"Guarda una respuesta en la caché.\"\"\"\n",
        "        # Si se excede el tamaño máximo, eliminar una entrada aleatoria\n",
        "        if len(self.cache) >= self.max_size:\n",
        "            random_key = next(iter(self.cache))\n",
        "            del self.cache[random_key]\n",
        "            self.logger.debug(f\"Caché llena. Eliminada entrada: {random_key[:20]}...\")\n",
        "\n",
        "        self.cache[prompt_key] = response\n",
        "        self.logger.debug(f\"Respuesta cacheada con clave: {prompt_key[:20]}...\")\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Limpia toda la caché.\"\"\"\n",
        "        self.cache.clear()\n",
        "        self.logger.debug(\"Caché limpiada.\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Retorna el número de entradas en la caché.\"\"\"\n",
        "        return len(self.cache)\n",
        "\n",
        "# =============================================================================\n",
        "# LLMEngine: Interfaz mejorada para la API de Google Gemini\n",
        "# =============================================================================\n",
        "class LLMEngine:\n",
        "    \"\"\"\n",
        "    Interfaz para interactuar con la API de Google Gemini, generando contenido educativo\n",
        "    de alta calidad con características mejoradas como cache, retry y evaluación.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Config):\n",
        "        \"\"\"\n",
        "        Inicializa el motor LLM con la configuración proporcionada.\n",
        "\n",
        "        Args:\n",
        "            config: Instancia de Config con los parámetros necesarios.\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(\"educational_agent.llm_engine\")\n",
        "\n",
        "        # Inicializar caché de respuestas\n",
        "        self.cache = ResponseCache()\n",
        "\n",
        "        # Verificar la existencia de la variable de entorno con la clave API\n",
        "        api_key = os.environ.get('GOOGLE_API_KEY')\n",
        "        if not api_key:\n",
        "            self.logger.error(\"La variable de entorno 'GOOGLE_API_KEY' no está definida.\")\n",
        "            raise EnvironmentError(\"La variable de entorno 'GOOGLE_API_KEY' es requerida.\")\n",
        "\n",
        "        # Configurar la API de Google Gemini\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel(self.config.gemini_model)\n",
        "\n",
        "        # Inicializar el limitador de tasa\n",
        "        self.rate_limiter = RateLimiter(\n",
        "            tokens_per_minute=self.config.llm_tokens_per_minute,\n",
        "            max_tokens_per_request=self.config.llm_max_tokens_per_request\n",
        "        )\n",
        "\n",
        "        # Cargar las plantillas de prompts\n",
        "        self.prompt_templates = self._load_prompt_templates()\n",
        "\n",
        "        # Instrucción base del sistema, mejorada con más detalles\n",
        "        self.system_instruction = self._get_improved_system_prompt()\n",
        "\n",
        "    def _get_improved_system_prompt(self) -> str:\n",
        "        \"\"\"\n",
        "        Proporciona una instrucción del sistema mejorada y más detallada.\n",
        "\n",
        "        Returns:\n",
        "            Instrucción del sistema formateada.\n",
        "        \"\"\"\n",
        "        return \"\"\"\n",
        "        Eres un asistente educativo experto con especialización en la creación de materiales didácticos\n",
        "        de alta calidad para cursos universitarios.\n",
        "\n",
        "        Tus características principales son:\n",
        "\n",
        "        1. Rigor académico: Tus explicaciones son precisas, basadas en información actualizada y\n",
        "           siguen los estándares académicos más altos.\n",
        "\n",
        "        2. Claridad y estructura: Organizas la información de manera lógica, con secciones claramente\n",
        "           definidas y progresión didáctica coherente.\n",
        "\n",
        "        3. Adaptación al nivel educativo: Ajustas la complejidad según el nivel universitario indicado,\n",
        "           sin simplificar en exceso ni complicar innecesariamente.\n",
        "\n",
        "        4. Enfoque práctico: Incluyes ejemplos del mundo real, casos de estudio y aplicaciones\n",
        "           prácticas para conectar la teoría con la práctica profesional.\n",
        "\n",
        "        5. Pensamiento crítico: Promueves el análisis, la evaluación y la síntesis de la información,\n",
        "           evitando la mera memorización.\n",
        "\n",
        "        Cuando generes contenido educativo, asegúrate de:\n",
        "        - Incluir preguntas reflexivas que fomenten el pensamiento profundo\n",
        "        - Proporcionar múltiples perspectivas sobre temas complejos cuando sea apropiado\n",
        "        - Incorporar referencias interdisciplinarias para enriquecer el material\n",
        "        - Usar un lenguaje preciso y terminología específica de cada disciplina\n",
        "        - Estructurar el contenido con encabezados, listas y elementos visuales descriptivos\n",
        "        \"\"\"\n",
        "\n",
        "    def _load_prompt_templates(self) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Carga las plantillas de prompts desde un archivo JSON.\n",
        "        Si no puede cargarlas, utiliza plantillas mejoradas por defecto.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con las plantillas de prompts.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with open(self.config.prompt_templates_path, 'r', encoding='utf-8') as f:\n",
        "                templates = json.load(f)\n",
        "                self.logger.info(\"Plantillas de prompts cargadas correctamente.\")\n",
        "                return templates\n",
        "        except Exception as e:\n",
        "            self.logger.warning(\n",
        "                f\"No se pudieron cargar las plantillas desde {self.config.prompt_templates_path}: {str(e)}. Se usarán plantillas por defecto.\"\n",
        "            )\n",
        "            # Plantillas mejoradas por defecto con instrucciones más específicas\n",
        "            return {\n",
        "                \"lecture_notes\": \"\"\"\n",
        "                Genera notas de clase detalladas y rigurosas para el tema: {topic_title}.\n",
        "\n",
        "                Incluye los siguientes subtemas: {subtopics}.\n",
        "\n",
        "                Estructura las notas siguiendo estos lineamientos:\n",
        "                1. Objetivos específicos de aprendizaje al inicio de cada sección.\n",
        "                2. Introducciones conceptuales que expliquen la relevancia del tema.\n",
        "                3. Desarrollo progresivo de ideas, desde lo fundamental hacia lo complejo.\n",
        "                4. Definiciones precisas resaltadas o en recuadros.\n",
        "                5. Ejemplos prácticos que ilustren los conceptos clave.\n",
        "                6. Diagramas o representaciones visuales (descritos textualmente).\n",
        "                7. Conexiones con otras áreas del curso o disciplinas relacionadas.\n",
        "                8. Preguntas de reflexión intercaladas para promover el pensamiento crítico.\n",
        "                9. Resumen final con los puntos principales.\n",
        "\n",
        "                Enfatiza tanto el \"por qué\" como el \"cómo\" de cada concepto, relacionando constantemente\n",
        "                la teoría con aplicaciones prácticas y casos del mundo real.\n",
        "                \"\"\",\n",
        "\n",
        "                \"practice_problems\": \"\"\"\n",
        "                Crea un conjunto diverso de problemas de práctica con soluciones paso a paso para el tema: {topic_title}.\n",
        "\n",
        "                Los problemas deben cubrir los siguientes subtemas: {subtopics}.\n",
        "\n",
        "                Sigue estas directrices para crear los problemas:\n",
        "                1. Desarrolla un mínimo de 7 problemas organizados en tres niveles de dificultad:\n",
        "                   - Básico (2 problemas): Para reforzar conceptos fundamentales.\n",
        "                   - Intermedio (3 problemas): Para aplicar conceptos en situaciones más complejas.\n",
        "                   - Avanzado (2 problemas): Para integrar múltiples conceptos y fomentar el pensamiento crítico.\n",
        "\n",
        "                2. Para cada problema:\n",
        "                   - Define claramente el objetivo y los datos proporcionados.\n",
        "                   - Proporciona una solución detallada que explique cada paso del razonamiento.\n",
        "                   - Incluye comentarios que identifiquen los conceptos clave utilizados.\n",
        "                   - Añade notas sobre errores comunes o conceptos erróneos a evitar.\n",
        "\n",
        "                3. Incorpora problemas de diferentes tipos:\n",
        "                   - Problemas conceptuales que evalúen la comprensión teórica.\n",
        "                   - Problemas numéricos o cuantitativos cuando sea apropiado.\n",
        "                   - Problemas de análisis que requieran evaluación crítica.\n",
        "                   - Problemas de diseño o aplicación real cuando sea pertinente.\n",
        "\n",
        "                4. Concluye con pistas para problemas adicionales que los estudiantes podrían crear.\n",
        "                \"\"\",\n",
        "\n",
        "                \"discussion_questions\": \"\"\"\n",
        "                Genera preguntas para discusión sobre el tema: {topic_title}, considerando: {subtopics}.\n",
        "\n",
        "                Desarrolla preguntas que promuevan el pensamiento crítico, el debate constructivo y la exploración profunda\n",
        "                del tema. Las preguntas deben:\n",
        "\n",
        "                1. Estimular diferentes niveles de pensamiento:\n",
        "                   - Análisis: Examinar componentes, relaciones y principios organizativos.\n",
        "                   - Evaluación: Valorar ideas basándose en criterios específicos y evidencia.\n",
        "                   - Síntesis: Conectar ideas de diferentes fuentes o campos.\n",
        "                   - Aplicación: Transferir conocimientos a nuevos contextos o situaciones.\n",
        "\n",
        "                2. Abordar múltiples dimensiones:\n",
        "                   - Implicaciones éticas y sociales del tema.\n",
        "                   - Perspectivas históricas y evolución del conocimiento.\n",
        "                   - Controversias actuales y debates relevantes.\n",
        "                   - Conexiones interdisciplinarias.\n",
        "                   - Aplicaciones en el mundo real y relevancia profesional.\n",
        "\n",
        "                3. Para cada pregunta, incluye:\n",
        "                   - La pregunta principal formulada de manera clara y abierta.\n",
        "                   - 2-3 subpreguntas de seguimiento para profundizar.\n",
        "                   - Una breve guía para el facilitador con puntos clave a considerar.\n",
        "                   - Posibles direcciones que podría tomar la discusión.\n",
        "\n",
        "                4. Organiza las preguntas en secciones temáticas cohesivas.\n",
        "                \"\"\",\n",
        "\n",
        "                \"learning_objectives\": \"\"\"\n",
        "                Crea objetivos de aprendizaje específicos, medibles, alcanzables,\n",
        "                relevantes y temporales (SMART) para el tema: {topic_title}.\n",
        "\n",
        "                Considera los siguientes subtemas: {subtopics}.\n",
        "\n",
        "                Estructura los objetivos de aprendizaje siguiendo estas pautas:\n",
        "\n",
        "                1. Utiliza taxonomía de Bloom actualizada, cubriendo todos los niveles cognitivos:\n",
        "                   - Recordar:\n",
        "                - Recordar: Verbos como identificar, listar, definir, reconocer.\n",
        "                   - Comprender: Verbos como explicar, interpretar, clasificar, resumir.\n",
        "                   - Aplicar: Verbos como implementar, ejecutar, usar, resolver.\n",
        "                   - Analizar: Verbos como diferenciar, organizar, atribuir, comparar.\n",
        "                   - Evaluar: Verbos como comprobar, criticar, juzgar, defender.\n",
        "                   - Crear: Verbos como diseñar, formular, planear, producir.\n",
        "\n",
        "                2. Para cada objetivo:\n",
        "                   - Inicia con un verbo de acción observable y medible.\n",
        "                   - Especifica el conocimiento o habilidad a adquirir.\n",
        "                   - Define el nivel de competencia esperado.\n",
        "                   - Establece el contexto o condiciones cuando sea relevante.\n",
        "\n",
        "                3. Asegura que los objetivos:\n",
        "                   - Se alineen con el nivel educativo universitario.\n",
        "                   - Cubran tanto conocimientos teóricos como aplicaciones prácticas.\n",
        "                   - Incluyan aspectos de pensamiento crítico y resolución de problemas.\n",
        "                   - Sean claros y comprensibles para los estudiantes.\n",
        "\n",
        "                4. Organiza los objetivos en una progresión lógica, desde los más fundamentales\n",
        "                   hasta los más complejos y avanzados.\n",
        "                \"\"\",\n",
        "\n",
        "                \"suggested_resources\": \"\"\"\n",
        "                Sugiere recursos de aprendizaje diversos y de alta calidad para el tema: {topic_title}.\n",
        "\n",
        "                Considera los siguientes subtemas al seleccionar los recursos: {subtopics}.\n",
        "\n",
        "                Sigue estas directrices para crear un conjunto completo de recursos:\n",
        "\n",
        "                1. Libros y publicaciones académicas:\n",
        "                   - Textos fundamentales del campo (clásicos y contemporáneos).\n",
        "                   - Libros de texto actualizados (publicados en los últimos 5 años cuando sea posible).\n",
        "                   - Artículos académicos de revistas de prestigio.\n",
        "                   - Capítulos específicos que aborden directamente los subtemas.\n",
        "\n",
        "                2. Recursos digitales:\n",
        "                   - Cursos en línea de universidades prestigiosas o plataformas reconocidas.\n",
        "                   - Conferencias o seminarios web de expertos en el campo.\n",
        "                   - Bases de datos especializadas y repositorios de información.\n",
        "                   - Simulaciones interactivas o laboratorios virtuales cuando sea apropiado.\n",
        "\n",
        "                3. Multimedia y recursos alternativos:\n",
        "                   - Documentales o películas relacionadas con el tema.\n",
        "                   - Podcasts académicos o entrevistas con expertos.\n",
        "                   - Visualizaciones de datos o infografías relevantes.\n",
        "                   - Herramientas de software o aplicaciones prácticas.\n",
        "\n",
        "                4. Para cada recurso:\n",
        "                   - Proporciona la información bibliográfica completa.\n",
        "                   - Incluye una breve descripción de su contenido y enfoque.\n",
        "                   - Indica el nivel de dificultad o audiencia objetivo.\n",
        "                   - Especifica qué subtemas cubre particularmente bien.\n",
        "                   - Cuando sea posible, menciona recursos de acceso abierto.\n",
        "\n",
        "                5. Organiza los recursos por tipo y relevancia, señalando cuáles son esenciales\n",
        "                   y cuáles complementarios.\n",
        "                \"\"\"\n",
        "            }\n",
        "\n",
        "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
        "    def generate_content(self, prompt: str, content_type: str = None, max_tokens: int = 1000) -> str:\n",
        "        \"\"\"\n",
        "        Genera contenido educativo llamando a la API de Google Gemini con\n",
        "        sistema de reintentos mejorado.\n",
        "\n",
        "        Args:\n",
        "            prompt: Instrucción detallada para el modelo.\n",
        "            content_type: Tipo de contenido a generar (lecture_notes, practice_problems, etc.)\n",
        "            max_tokens: Número máximo de tokens para la respuesta.\n",
        "\n",
        "        Returns:\n",
        "            Texto generado por el modelo.\n",
        "        \"\"\"\n",
        "        self.logger.debug(f\"Generando contenido para tipo: {content_type or 'no especificado'}\")\n",
        "\n",
        "        # Generar una clave de caché única para este prompt\n",
        "        cache_key = f\"{prompt[:100]}_{max_tokens}\"\n",
        "\n",
        "        # Verificar si ya existe una respuesta cacheada\n",
        "        cached_response = self.cache.get(cache_key)\n",
        "        if cached_response:\n",
        "            self.logger.info(\"Respuesta encontrada en caché, evitando llamada a la API.\")\n",
        "            return cached_response\n",
        "\n",
        "        # Estimar tokens del prompt (aproximación)\n",
        "        prompt_tokens = len(prompt.split())\n",
        "\n",
        "        # Calcular el total de tokens solicitado (prompt + respuesta)\n",
        "        total_tokens = prompt_tokens + max_tokens\n",
        "\n",
        "        # Ajuste dinámico: si total_tokens supera el límite, reducir max_tokens\n",
        "        if total_tokens > self.rate_limiter.max_tokens_per_request:\n",
        "            available_tokens = self.rate_limiter.max_tokens_per_request - prompt_tokens\n",
        "            if available_tokens <= 0:\n",
        "                raise ValueError(\"El prompt es demasiado largo y no deja espacio para la respuesta.\")\n",
        "            self.logger.info(\n",
        "                f\"Ajustando max_tokens de {max_tokens} a {available_tokens} para cumplir el límite.\"\n",
        "            )\n",
        "            max_tokens = available_tokens\n",
        "            total_tokens = prompt_tokens + max_tokens\n",
        "\n",
        "        # Esperar si es necesario para cumplir con los límites de tokens\n",
        "        self.rate_limiter.wait_if_needed(total_tokens)\n",
        "\n",
        "        try:\n",
        "            # Determinar la temperatura según el tipo de contenido\n",
        "            temperature = 0.7  # Valor por defecto\n",
        "            if content_type and content_type in self.config.temperature_by_content:\n",
        "                temperature = self.config.temperature_by_content[content_type]\n",
        "                self.logger.debug(f\"Usando temperatura específica para {content_type}: {temperature}\")\n",
        "\n",
        "            # Combinar la instrucción del sistema con el prompt del usuario\n",
        "            full_prompt = f\"{self.system_instruction}\\n\\n{prompt}\"\n",
        "\n",
        "            # Configuración para la generación de contenido\n",
        "            generation_config = {\n",
        "                \"max_output_tokens\": max_tokens,\n",
        "                \"temperature\": temperature,\n",
        "                \"top_p\": 0.95,\n",
        "                \"top_k\": 40\n",
        "            }\n",
        "\n",
        "            self.logger.debug(\"Llamando a la API de Gemini con la configuración definida.\")\n",
        "            response = self.model.generate_content(\n",
        "                contents=[{\"role\": \"user\", \"parts\": [{\"text\": full_prompt}]}],\n",
        "                generation_config=generation_config\n",
        "            )\n",
        "\n",
        "            # Extraer y retornar el texto generado\n",
        "            generated_text = response.text.strip() if response.text else \"\"\n",
        "\n",
        "            # Evaluar la calidad del contenido generado\n",
        "            quality_score = self._evaluate_content_quality(generated_text, content_type)\n",
        "\n",
        "            # Si la calidad es muy baja, intentar regenerar con ajustes\n",
        "            if quality_score < 0.6:\n",
        "                self.logger.warning(f\"Calidad del contenido generado baja ({quality_score}). Ajustando parámetros y regenerando.\")\n",
        "\n",
        "                # Ajustar la temperatura para mejorar la calidad\n",
        "                adjusted_temperature = min(0.2, temperature) if quality_score < 0.3 else temperature * 0.8\n",
        "\n",
        "                # Añadir indicaciones para mejorar la calidad\n",
        "                enhanced_prompt = f\"\"\"\n",
        "                {full_prompt}\n",
        "\n",
        "                IMPORTANTE: El contenido debe ser de alta calidad académica. Por favor, asegúrate de:\n",
        "                1. Proporcionar explicaciones detalladas y rigurosas.\n",
        "                2. Estructurar el contenido de manera clara y lógica.\n",
        "                3. Incluir ejemplos específicos y relevantes.\n",
        "                4. Usar terminología precisa y apropiada para el nivel universitario.\n",
        "                \"\"\"\n",
        "\n",
        "                generation_config[\"temperature\"] = adjusted_temperature\n",
        "\n",
        "                # Reintentar con el prompt mejorado\n",
        "                response = self.model.generate_content(\n",
        "                    contents=[{\"role\": \"user\", \"parts\": [{\"text\": enhanced_prompt}]}],\n",
        "                    generation_config=generation_config\n",
        "                )\n",
        "\n",
        "                generated_text = response.text.strip() if response.text else \"\"\n",
        "\n",
        "            # Guardar la respuesta en caché\n",
        "            if generated_text:\n",
        "                self.cache.set(cache_key, generated_text)\n",
        "\n",
        "            self.logger.debug(\"Contenido generado exitosamente.\")\n",
        "            return generated_text\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error al generar contenido con Gemini: {str(e)}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def _evaluate_content_quality(self, content: str, content_type: str = None) -> float:\n",
        "        \"\"\"\n",
        "        Evalúa la calidad del contenido generado mediante heurísticas.\n",
        "\n",
        "        Args:\n",
        "            content: Texto generado por el modelo.\n",
        "            content_type: Tipo de contenido (lecture_notes, practice_problems, etc.)\n",
        "\n",
        "        Returns:\n",
        "            Puntuación de calidad entre 0.0 y 1.0\n",
        "        \"\"\"\n",
        "        if not content:\n",
        "            return 0.0\n",
        "\n",
        "        # Puntuación base\n",
        "        score = 0.7\n",
        "\n",
        "        # Criterios de calidad generales\n",
        "        # 1. Longitud adecuada (contenido sustancioso)\n",
        "        words = len(content.split())\n",
        "        if words < 100:\n",
        "            score -= 0.2\n",
        "        elif words > 500:\n",
        "            score += 0.1\n",
        "\n",
        "        # 2. Estructura (encabezados, listas, párrafos)\n",
        "        if \"# \" in content or \"## \" in content:\n",
        "            score += 0.05\n",
        "        if \"- \" in content or \"* \" in content:\n",
        "            score += 0.05\n",
        "        if \"\\n\\n\" in content:\n",
        "            score += 0.05\n",
        "\n",
        "        # 3. Diversidad de vocabulario\n",
        "        unique_words = len(set(content.lower().split()))\n",
        "        vocabulary_ratio = unique_words / (words + 1)  # Evitar división por cero\n",
        "        if vocabulary_ratio > 0.5:\n",
        "            score += 0.1\n",
        "\n",
        "        # Criterios específicos por tipo de contenido\n",
        "        if content_type == \"lecture_notes\":\n",
        "            # Buscar definiciones, ejemplos y explicaciones\n",
        "            if \"definición\" in content.lower() or \":\" in content:\n",
        "                score += 0.05\n",
        "            if \"ejemplo\" in content.lower() or \"por ejemplo\" in content.lower():\n",
        "                score += 0.05\n",
        "            if \"porque\" in content.lower() or \"debido a\" in content.lower():\n",
        "                score += 0.05\n",
        "\n",
        "        elif content_type == \"practice_problems\":\n",
        "            # Buscar problemas numerados, soluciones y diferentes niveles\n",
        "            if \"problema\" in content.lower() and any(f\"{i}.\" in content for i in range(1, 10)):\n",
        "                score += 0.1\n",
        "            if \"solución\" in content.lower() or \"respuesta\" in content.lower():\n",
        "                score += 0.1\n",
        "            if \"básico\" in content.lower() or \"avanzado\" in content.lower() or \"intermedio\" in content.lower():\n",
        "                score += 0.05\n",
        "\n",
        "        elif content_type == \"discussion_questions\":\n",
        "            # Buscar preguntas abiertas y de pensamiento crítico\n",
        "            if \"?\" in content and (\"por qué\" in content.lower() or \"cómo\" in content.lower()):\n",
        "                score += 0.15\n",
        "            if \"analiza\" in content.lower() or \"evalúa\" in content.lower() or \"compara\" in content.lower():\n",
        "                score += 0.1\n",
        "\n",
        "        elif content_type == \"learning_objectives\":\n",
        "            # Buscar verbos de acción de la taxonomía de Bloom\n",
        "            bloom_verbs = [\"identificar\", \"describir\", \"aplicar\", \"analizar\", \"evaluar\", \"crear\",\n",
        "                          \"explicar\", \"comparar\", \"implementar\", \"diferenciar\", \"justificar\", \"diseñar\"]\n",
        "            if any(verb in content.lower() for verb in bloom_verbs):\n",
        "                score += 0.15\n",
        "\n",
        "        elif content_type == \"suggested_resources\":\n",
        "            # Buscar referencias bibliográficas y descripciones de recursos\n",
        "            if \":\" in content and (any(year in content for year in map(str, range(2000, 2025)))):\n",
        "                score += 0.1\n",
        "            if \"libro\" in content.lower() or \"artículo\" in content.lower() or \"recurso\" in content.lower():\n",
        "                score += 0.1\n",
        "\n",
        "        # Limitar la puntuación al rango [0.0, 1.0]\n",
        "        return max(0.0, min(1.0, score))\n",
        "\n",
        "    def batch_generate_content(self, prompts: List[Dict[str, Any]]) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Genera contenido en lote para múltiples prompts.\n",
        "\n",
        "        Args:\n",
        "            prompts: Lista de diccionarios con los campos 'prompt', 'content_type' y 'max_tokens'.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con los resultados usando las claves proporcionadas.\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "        for prompt_data in prompts:\n",
        "            prompt_key = prompt_data.get('key', str(id(prompt_data)))\n",
        "            try:\n",
        "                results[prompt_key] = self.generate_content(\n",
        "                    prompt=prompt_data['prompt'],\n",
        "                    content_type=prompt_data.get('content_type'),\n",
        "                    max_tokens=prompt_data.get('max_tokens', 1000)\n",
        "                )\n",
        "            except Exception as e:\n",
        "                self.logger.error(f\"Error en generación de contenido para '{prompt_key}': {str(e)}\")\n",
        "                results[prompt_key] = f\"Error: {str(e)}\"\n",
        "        return results\n",
        "\n",
        "    def generate_with_critique(self, prompt: str, content_type: str = None, max_tokens: int = 1000) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Genera contenido y lo evalúa con una crítica automática.\n",
        "\n",
        "        Args:\n",
        "            prompt: Instrucción para el modelo.\n",
        "            content_type: Tipo de contenido a generar.\n",
        "            max_tokens: Límite de tokens para la respuesta.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con el contenido generado y su crítica.\n",
        "        \"\"\"\n",
        "        # Generar el contenido inicial\n",
        "        initial_content = self.generate_content(prompt, content_type, max_tokens)\n",
        "\n",
        "        # Prompt para la crítica\n",
        "        critique_prompt = f\"\"\"\n",
        "        Analiza el siguiente contenido educativo y proporciona una crítica constructiva:\n",
        "\n",
        "        {initial_content}\n",
        "\n",
        "        Realiza una evaluación detallada considerando:\n",
        "        1. Precisión académica y rigor\n",
        "        2. Claridad y estructura\n",
        "        3. Profundidad y completitud\n",
        "        4. Relevancia y aplicabilidad\n",
        "        5. Nivel de complejidad adecuado\n",
        "\n",
        "        Proporciona sugerencias específicas para mejorar el contenido.\n",
        "        \"\"\"\n",
        "\n",
        "        # Generar la crítica\n",
        "        critique = self.generate_content(critique_prompt, \"critique\", max_tokens=1000)\n",
        "\n",
        "        # Determinar puntuación basada en la crítica\n",
        "        score = self._extract_score_from_critique(critique)\n",
        "\n",
        "        # Mejorar el contenido si la puntuación es baja\n",
        "        improved_content = initial_content\n",
        "        if score < 7.0:\n",
        "            improvement_prompt = f\"\"\"\n",
        "            Mejora el siguiente contenido educativo según esta crítica:\n",
        "\n",
        "            CONTENIDO ORIGINAL:\n",
        "            {initial_content}\n",
        "\n",
        "            CRÍTICA:\n",
        "            {critique}\n",
        "\n",
        "            DIRECTRICES PARA LA MEJORA:\n",
        "\n",
        "            1. Mantén la estructura original pero mejora los aspectos débiles señalados.\n",
        "            2. Asegúrate de corregir cualquier imprecisión académica.\n",
        "            3. Profundiza en las áreas que se señalan como superficiales.\n",
        "            4. Mejora la claridad y precisión del lenguaje donde sea necesario.\n",
        "            5. Añade ejemplos concretos o aplicaciones prácticas si falta contexto.\n",
        "            \"\"\"\n",
        "\n",
        "            # Generar contenido mejorado\n",
        "            improved_content = self.generate_content(improvement_prompt, content_type, max_tokens)\n",
        "\n",
        "        # Retornar tanto el contenido original como el mejorado\n",
        "        return {\n",
        "            \"original_content\": initial_content,\n",
        "            \"critique\": critique,\n",
        "            \"improved_content\": improved_content,\n",
        "            \"quality_score\": score\n",
        "        }\n",
        "\n",
        "    def _extract_score_from_critique(self, critique: str) -> float:\n",
        "        \"\"\"\n",
        "        Analiza la crítica y estima una puntuación numérica.\n",
        "\n",
        "        Args:\n",
        "            critique: Texto de la crítica generada.\n",
        "\n",
        "        Returns:\n",
        "            Puntuación estimada en escala de 0 a 10.\n",
        "        \"\"\"\n",
        "        # Palabras positivas y negativas para el análisis de sentimiento\n",
        "        positive_words = [\"excelente\", \"bueno\", \"claro\", \"preciso\", \"detallado\", \"efectivo\",\n",
        "                         \"comprehensivo\", \"relevante\", \"apropiado\", \"útil\", \"sólido\", \"valioso\"]\n",
        "        negative_words = [\"confuso\", \"impreciso\", \"superficial\", \"limitado\", \"incompleto\",\n",
        "                         \"inadecuado\", \"simplista\", \"vago\", \"carente\", \"básico\", \"débil\", \"problemático\"]\n",
        "\n",
        "        # Contar ocurrencias\n",
        "        positive_count = sum(1 for word in positive_words if word in critique.lower())\n",
        "        negative_count = sum(1 for word in negative_words if word in critique.lower())\n",
        "\n",
        "        # Calcular una puntuación base\n",
        "        total_count = positive_count + negative_count\n",
        "        if total_count == 0:\n",
        "            return 5.0  # Puntuación neutral si no hay coincidencias\n",
        "\n",
        "        # Calcular puntuación ponderada\n",
        "        score = 5.0 + (positive_count - negative_count) * 2.5 / total_count\n",
        "\n",
        "        # Limitar al rango [0, 10]\n",
        "        return max(0.0, min(10.0, score))\n",
        "\n",
        "    def format_content_for_export(self, content: str, content_type: str, export_format: str = \"markdown\") -> str:\n",
        "        \"\"\"\n",
        "        Formatea el contenido generado para exportación en diferentes formatos.\n",
        "\n",
        "        Args:\n",
        "            content: Contenido generado por el modelo.\n",
        "            content_type: Tipo de contenido (lecture_notes, practice_problems, etc.)\n",
        "            export_format: Formato de exportación (markdown, html, texto)\n",
        "\n",
        "        Returns:\n",
        "            Contenido formateado según el formato especificado.\n",
        "        \"\"\"\n",
        "        if export_format == \"markdown\":\n",
        "            # Ya está en formato markdown, solo agregar encabezado\n",
        "            title = f\"# Contenido Educativo: {content_type.replace('_', ' ').title()}\\n\\n\"\n",
        "            return title + content\n",
        "\n",
        "        elif export_format == \"html\":\n",
        "            # Convertir markdown a HTML (implementación simplificada)\n",
        "            html_content = content\n",
        "            # Convertir encabezados\n",
        "            for i in range(6, 0, -1):\n",
        "                pattern = \"#\" * i + \" (.+)\"\n",
        "                replacement = f\"<h{i}>\\\\1</h{i}>\"\n",
        "                html_content = re.sub(pattern, replacement, html_content)\n",
        "\n",
        "            # Convertir listas\n",
        "            html_content = re.sub(r\"^\\s*-\\s+(.+)$\", r\"<li>\\1</li>\", html_content, flags=re.MULTILINE)\n",
        "            html_content = re.sub(r\"((?:<li>.*</li>\\s*)+)\", r\"<ul>\\1</ul>\", html_content)\n",
        "\n",
        "            # Convertir párrafos\n",
        "            html_content = re.sub(r\"^([^<\\s].+)$\", r\"<p>\\1</p>\", html_content, flags=re.MULTILINE)\n",
        "\n",
        "            # Añadir estructura HTML básica\n",
        "            return f\"\"\"<!DOCTYPE html>\n",
        "<html>\n",
        "<head>\n",
        "    <title>Contenido Educativo: {content_type.replace('_', ' ').title()}</title>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <style>\n",
        "        body {{ font-family: Arial, sans-serif; line-height: 1.6; max-width: 800px; margin: 0 auto; padding: 20px; }}\n",
        "        h1, h2, h3 {{ color: #2c3e50; }}\n",
        "        code {{ background-color: #f7f7f7; padding: 2px 5px; border-radius: 3px; }}\n",
        "    </style>\n",
        "</head>\n",
        "<body>\n",
        "    <h1>Contenido Educativo: {content_type.replace('_', ' ').title()}</h1>\n",
        "    {html_content}\n",
        "</body>\n",
        "</html>\"\"\"\n",
        "\n",
        "        elif export_format == \"text\":\n",
        "            # Convertir a texto plano (eliminar marcadores markdown)\n",
        "            text_content = content\n",
        "            # Eliminar marcadores de encabezados\n",
        "            text_content = re.sub(r\"^#+\\s+\", \"\", text_content, flags=re.MULTILINE)\n",
        "            # Convertir listas\n",
        "            text_content = re.sub(r\"^\\s*-\\s+\", \"* \", text_content, flags=re.MULTILINE)\n",
        "\n",
        "            return f\"CONTENIDO EDUCATIVO: {content_type.replace('_', ' ').upper()}\\n\\n{text_content}\"\n",
        "\n",
        "        else:\n",
        "            self.logger.warning(f\"Formato de exportación no soportado: {export_format}\")\n",
        "            return content\n",
        "\n",
        "# =============================================================================\n",
        "# EducationalContentGenerator: Clase principal para generar contenido educativo\n",
        "# =============================================================================\n",
        "class EducationalContentGenerator:\n",
        "    \"\"\"\n",
        "    Clase principal para generar materiales educativos completos.\n",
        "    \"\"\"\n",
        "    def __init__(self, config: Optional[Config] = None):\n",
        "        \"\"\"\n",
        "        Inicializa el generador de contenido educativo.\n",
        "\n",
        "        Args:\n",
        "            config: Configuración opcional. Si no se proporciona, se utiliza la configuración por defecto.\n",
        "        \"\"\"\n",
        "        self.config = config or Config()\n",
        "        self.logger = logging.getLogger(\"educational_agent.content_generator\")\n",
        "        self.llm_engine = LLMEngine(self.config)\n",
        "\n",
        "    def generate_course_material(self,\n",
        "                                topic: str,\n",
        "                                subtopics: List[str],\n",
        "                                material_types: List[str] = None,\n",
        "                                level: str = \"undergraduate\",\n",
        "                                max_tokens_per_type: int = 4000) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Genera materiales educativos completos para un tema de curso.\n",
        "\n",
        "        Args:\n",
        "            topic: Tema principal del curso.\n",
        "            subtopics: Lista de subtemas a cubrir.\n",
        "            material_types: Tipos de materiales a generar (lecture_notes, practice_problems, etc.).\n",
        "            level: Nivel educativo (undergraduate, graduate, etc.).\n",
        "            max_tokens_per_type: Máximo de tokens por tipo de material.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con los materiales generados por tipo.\n",
        "        \"\"\"\n",
        "        if material_types is None:\n",
        "            material_types = [\"lecture_notes\", \"practice_problems\", \"discussion_questions\",\n",
        "                             \"learning_objectives\", \"suggested_resources\"]\n",
        "\n",
        "        self.logger.info(f\"Generando materiales educativos para el tema: '{topic}' con {len(subtopics)} subtemas.\")\n",
        "\n",
        "        # Formatear la lista de subtemas para los prompts\n",
        "        subtopics_text = \", \".join(subtopics)\n",
        "\n",
        "        # Crear lista de prompts para generación en lote\n",
        "        prompts = []\n",
        "        for material_type in material_types:\n",
        "            if material_type not in self.llm_engine.prompt_templates:\n",
        "                self.logger.warning(f\"Tipo de material '{material_type}' no tiene plantilla. Ignorando.\")\n",
        "                continue\n",
        "\n",
        "            # Obtener la plantilla y formatearla con los datos del tema\n",
        "            template = self.llm_engine.prompt_templates[material_type]\n",
        "            formatted_prompt = template.format(\n",
        "                topic_title=topic,\n",
        "                subtopics=subtopics_text,\n",
        "                level=level\n",
        "            )\n",
        "\n",
        "            prompts.append({\n",
        "                \"key\": material_type,\n",
        "                \"prompt\": formatted_prompt,\n",
        "                \"content_type\": material_type,\n",
        "                \"max_tokens\": max_tokens_per_type\n",
        "            })\n",
        "\n",
        "        # Generar contenido para todos los tipos de materiales\n",
        "        results = self.llm_engine.batch_generate_content(prompts)\n",
        "\n",
        "        self.logger.info(f\"Generación de materiales completa para el tema: '{topic}'\")\n",
        "        return results\n",
        "\n",
        "    def generate_lesson_plan(self,\n",
        "                            topic: str,\n",
        "                            duration_minutes: int = 90,\n",
        "                            level: str = \"undergraduate\") -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Genera un plan de lección completo para un tema específico.\n",
        "\n",
        "        Args:\n",
        "            topic: Tema de la lección.\n",
        "            duration_minutes: Duración de la lección en minutos.\n",
        "            level: Nivel educativo (undergraduate, graduate, etc.).\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con los componentes del plan de lección.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Generando plan de lección para: '{topic}' ({duration_minutes} minutos)\")\n",
        "\n",
        "        # Prompt para el plan de lección\n",
        "        lesson_plan_prompt = f\"\"\"\n",
        "        Crea un plan de lección detallado para una clase de {duration_minutes} minutos sobre el tema:\n",
        "        \"{topic}\" para estudiantes de nivel {level}.\n",
        "\n",
        "        El plan de lección debe incluir:\n",
        "\n",
        "        1. Objetivos de aprendizaje claros y alcanzables.\n",
        "        2. Actividades de introducción/calentamiento (10% del tiempo).\n",
        "        3. Presentación de conceptos clave (30% del tiempo).\n",
        "        4. Actividades de aplicación/práctica (40% del tiempo).\n",
        "        5. Discusión y reflexión (15% del tiempo).\n",
        "        6. Cierre y evaluación (5% del tiempo).\n",
        "\n",
        "        Para cada sección, proporciona:\n",
        "        - Duración específica en minutos\n",
        "        - Descripción detallada de la actividad\n",
        "        - Recursos necesarios\n",
        "        - Preguntas clave para guiar la discusión\n",
        "        - Estrategias para involucrar a diferentes tipos de estudiantes\n",
        "\n",
        "        Asegúrate de incorporar actividades variadas que atiendan diferentes estilos de aprendizaje\n",
        "        y niveles de habilidad.\n",
        "        \"\"\"\n",
        "\n",
        "        # Generar el plan de lección\n",
        "        lesson_plan = self.llm_engine.generate_content(lesson_plan_prompt, \"lesson_plan\", 5000)\n",
        "\n",
        "        # Generar materiales complementarios\n",
        "        activity_prompt = f\"\"\"\n",
        "        Crea materiales complementarios para una lección sobre \"{topic}\" para estudiantes de nivel {level}.\n",
        "\n",
        "        Desarrolla:\n",
        "        1. Una actividad de evaluación formativa para verificar la comprensión durante la clase.\n",
        "        2. Una actividad de aprendizaje activo que promueva la colaboración entre estudiantes.\n",
        "        3. Una rúbrica de evaluación para calificar la actividad principal.\n",
        "\n",
        "        Proporciona instrucciones detalladas, criterios de evaluación y materiales necesarios.\n",
        "        \"\"\"\n",
        "\n",
        "        activities = self.llm_engine.generate_content(activity_prompt, \"activities\", 3000)\n",
        "\n",
        "        # Generar sugerencias para adaptaciones\n",
        "        adaptations_prompt = f\"\"\"\n",
        "        Proporciona sugerencias para adaptar una lección sobre \"{topic}\" para:\n",
        "\n",
        "        1. Estudiantes con diferentes estilos de aprendizaje (visual, auditivo, kinestésico).\n",
        "        2. Estudiantes con diferentes niveles de competencia.\n",
        "        3. Modalidades alternativas de enseñanza (en línea, híbrida, presencial).\n",
        "\n",
        "        Para cada adaptación, proporciona ejemplos concretos y recomendaciones prácticas.\n",
        "        \"\"\"\n",
        "\n",
        "        adaptations = self.llm_engine.generate_content(adaptations_prompt, \"adaptations\", 2000)\n",
        "\n",
        "        return {\n",
        "            \"lesson_plan\": lesson_plan,\n",
        "            \"activities\": activities,\n",
        "            \"adaptations\": adaptations\n",
        "        }\n",
        "\n",
        "    def generate_content_from_pdf(self,\n",
        "                            pdf_path: str,\n",
        "                            content_types: List[str] = None,\n",
        "                            level: str = \"undergraduate\",\n",
        "                            max_tokens_per_type: int = 4000,\n",
        "                            max_retries: int = 3,\n",
        "                            retry_delay: int = 60) -> Dict[str, str]:\n",
        "      \"\"\"\n",
        "      Genera materiales educativos a partir de un archivo PDF.\n",
        "\n",
        "      Args:\n",
        "          pdf_path: Ruta al archivo PDF.\n",
        "          content_types: Tipos de materiales a generar.\n",
        "          level: Nivel educativo.\n",
        "          max_tokens_per_type: Máximo de tokens por tipo de material.\n",
        "          max_retries: Número máximo de reintentos en caso de error de cuota.\n",
        "          retry_delay: Tiempo de espera entre reintentos (en segundos).\n",
        "\n",
        "      Returns:\n",
        "          Diccionario con los materiales generados por tipo.\n",
        "      \"\"\"\n",
        "      import PyPDF2\n",
        "      import time\n",
        "      import json\n",
        "\n",
        "      self.logger.info(f\"Extrayendo contenido del PDF: {pdf_path}\")\n",
        "\n",
        "      if content_types is None:\n",
        "          content_types = [\"lecture_notes\", \"practice_problems\", \"discussion_questions\"]\n",
        "\n",
        "      try:\n",
        "          # Extraer texto del PDF\n",
        "          text = \"\"\n",
        "          with open(pdf_path, 'rb') as file:\n",
        "              pdf_reader = PyPDF2.PdfReader(file)\n",
        "              for page_num in range(len(pdf_reader.pages)):\n",
        "                  page = pdf_reader.pages[page_num]\n",
        "                  text += page.extract_text()\n",
        "\n",
        "          if not text.strip():\n",
        "              self.logger.error(\"No se pudo extraer texto del PDF.\")\n",
        "              return {}\n",
        "\n",
        "          self.logger.info(f\"Texto extraído exitosamente: {len(text)} caracteres\")\n",
        "\n",
        "          # Analizar el texto para identificar tema y subtemas\n",
        "          analysis_prompt = f\"\"\"\n",
        "          Analiza el siguiente texto extraído de un PDF académico y determina:\n",
        "          1. El tema principal del documento\n",
        "          2. Los subtemas principales (enumera hasta 5)\n",
        "          3. El nivel académico aparente (básico, intermedio, avanzado)\n",
        "\n",
        "          Proporciona la información en formato JSON con las claves \"topic\", \"subtopics\" y \"level\".\n",
        "\n",
        "          Texto del PDF:\n",
        "          {text[:8000]}\n",
        "          \"\"\"\n",
        "\n",
        "          # Generar análisis del contenido con reintentos\n",
        "          analysis_result = None\n",
        "          for attempt in range(max_retries):\n",
        "              try:\n",
        "                  analysis_result = self.llm_engine.generate_content(analysis_prompt, \"content_analysis\", 1000)\n",
        "                  break\n",
        "              except Exception as e:\n",
        "                  if \"Resource has been exhausted\" in str(e) and attempt < max_retries - 1:\n",
        "                      wait_time = retry_delay * (2 ** attempt)  # Espera exponencial\n",
        "                      self.logger.warning(f\"Error de cuota. Reintentando en {wait_time} segundos...\")\n",
        "                      time.sleep(wait_time)\n",
        "                  else:\n",
        "                      raise\n",
        "\n",
        "          # Procesar el resultado del análisis\n",
        "          if analysis_result:\n",
        "              try:\n",
        "                  # Intentar extraer el JSON del texto (puede estar rodeado de texto explicativo)\n",
        "                  import re\n",
        "                  json_match = re.search(r'\\{.*\\}', analysis_result, re.DOTALL)\n",
        "                  if json_match:\n",
        "                      json_str = json_match.group(0)\n",
        "                      analysis = json.loads(json_str)\n",
        "                  else:\n",
        "                      analysis = json.loads(analysis_result)\n",
        "\n",
        "                  topic = analysis.get(\"topic\", \"Tema no identificado\")\n",
        "                  subtopics = analysis.get(\"subtopics\", [\"Subtema 1\", \"Subtema 2\", \"Subtema 3\"])\n",
        "                  detected_level = analysis.get(\"level\", level)\n",
        "\n",
        "                  self.logger.info(f\"Tema detectado: '{topic}' con {len(subtopics)} subtemas\")\n",
        "\n",
        "              except json.JSONDecodeError:\n",
        "                  self.logger.warning(\"No se pudo parsear el análisis como JSON. Utilizando valores predeterminados.\")\n",
        "                  # Extraer información básica del texto\n",
        "                  lines = text[:1000].split('\\n')\n",
        "                  topic = lines[0] if lines else \"Tema del PDF\"\n",
        "                  subtopics = [\"Contenido general del PDF\"]\n",
        "                  detected_level = level\n",
        "          else:\n",
        "              # Si no se pudo obtener análisis, usar valores predeterminados\n",
        "              self.logger.warning(\"No se pudo realizar el análisis. Utilizando valores predeterminados.\")\n",
        "              topic = \"Tema del PDF\"\n",
        "              subtopics = [\"Contenido general del PDF\"]\n",
        "              detected_level = level\n",
        "\n",
        "          # Generar materiales con el contenido extraído\n",
        "          results = {}\n",
        "\n",
        "          for content_type in content_types:\n",
        "              if content_type not in self.llm_engine.prompt_templates:\n",
        "                  self.logger.warning(f\"Tipo de material '{content_type}' no tiene plantilla. Usando plantilla genérica.\")\n",
        "                  prompt = f\"\"\"\n",
        "                  Basándote en el siguiente contenido extraído de un PDF sobre el tema \"{topic}\",\n",
        "                  genera material educativo de tipo {content_type} para nivel {detected_level}.\n",
        "\n",
        "                  Los subtemas identificados son: {', '.join(subtopics)}\n",
        "\n",
        "                  Contenido del PDF:\n",
        "                  {text[:15000]}\n",
        "\n",
        "                  Crea un material educativo completo, estructurado y académicamente riguroso.\n",
        "                  \"\"\"\n",
        "              else:\n",
        "                  template = self.llm_engine.prompt_templates[content_type]\n",
        "                  prompt = template.format(\n",
        "                      topic_title=topic,\n",
        "                      subtopics=', '.join(subtopics),\n",
        "                      level=detected_level\n",
        "                  )\n",
        "                  prompt += f\"\\n\\nContexto adicional del PDF original:\\n{text[:10000]}\"\n",
        "\n",
        "              # Generar contenido con reintentos en caso de error de cuota\n",
        "              for attempt in range(max_retries):\n",
        "                  try:\n",
        "                      content = self.llm_engine.generate_content(prompt, content_type, max_tokens_per_type)\n",
        "                      results[content_type] = content\n",
        "                      break\n",
        "                  except Exception as e:\n",
        "                      if \"Resource has been exhausted\" in str(e) and attempt < max_retries - 1:\n",
        "                          wait_time = retry_delay * (2 ** attempt)  # Espera exponencial\n",
        "                          self.logger.warning(f\"Error de cuota al generar {content_type}. Reintentando en {wait_time} segundos...\")\n",
        "                          time.sleep(wait_time)\n",
        "                      else:\n",
        "                          self.logger.error(f\"Error al generar {content_type}: {str(e)}\")\n",
        "                          results[content_type] = f\"Error al generar contenido: {str(e)}\"\n",
        "\n",
        "          self.logger.info(f\"Generación de materiales completa para el PDF: {pdf_path}\")\n",
        "          return results\n",
        "\n",
        "      except Exception as e:\n",
        "          self.logger.error(f\"Error al procesar el PDF: {str(e)}\")\n",
        "          return {\"error\": str(e)}\n",
        "\n",
        "    def generate_assessment(self,\n",
        "                           topic: str,\n",
        "                           subtopics: List[str],\n",
        "                           assessment_type: str = \"quiz\",\n",
        "                           level: str = \"undergraduate\") -> str:\n",
        "        \"\"\"\n",
        "        Genera instrumentos de evaluación para un tema específico.\n",
        "\n",
        "        Args:\n",
        "            topic: Tema principal.\n",
        "            subtopics: Lista de subtemas.\n",
        "            assessment_type: Tipo de evaluación (quiz, exam, project, etc.).\n",
        "            level: Nivel educativo.\n",
        "\n",
        "        Returns:\n",
        "            Instrumento de evaluación generado.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Generando evaluación tipo '{assessment_type}' para el tema: '{topic}'\")\n",
        "\n",
        "        # Formatear la lista de subtemas\n",
        "        subtopics_text = \", \".join(subtopics)\n",
        "\n",
        "        # Prompts específicos según el tipo de evaluación\n",
        "        if assessment_type == \"quiz\":\n",
        "            prompt = f\"\"\"\n",
        "            Crea un cuestionario (quiz) para evaluar el conocimiento sobre el tema \"{topic}\".\n",
        "\n",
        "            El cuestionario debe cubrir los siguientes subtemas: {subtopics_text}\n",
        "\n",
        "            Incluye:\n",
        "            1. 10 preguntas de opción múltiple (4 opciones cada una)\n",
        "            2. 5 preguntas de verdadero/falso\n",
        "            3. 3 preguntas de respuesta corta\n",
        "\n",
        "            Para cada pregunta:\n",
        "            - Indica el subtema al que corresponde\n",
        "            - Proporciona la respuesta correcta\n",
        "            - Incluye una breve explicación de por qué es correcta\n",
        "            - Asigna un nivel de dificultad (básico, intermedio, avanzado)\n",
        "\n",
        "            Las preguntas deben evaluar tanto el conocimiento factual como la comprensión conceptual\n",
        "            y la capacidad de aplicación.\n",
        "            \"\"\"\n",
        "        elif assessment_type == \"exam\":\n",
        "            prompt = f\"\"\"\n",
        "            Diseña un examen completo para evaluar el dominio del tema \"{topic}\" a nivel {level}.\n",
        "\n",
        "            El examen debe cubrir los siguientes subtemas: {subtopics_text}\n",
        "\n",
        "            Estructura del examen:\n",
        "            1. Sección de preguntas de opción múltiple (30% del puntaje total)\n",
        "            2. Sección de preguntas de desarrollo breve (30% del puntaje total)\n",
        "            3. Sección de problemas de aplicación/análisis (40% del puntaje total)\n",
        "\n",
        "            Para cada sección:\n",
        "            - Proporciona instrucciones claras\n",
        "            - Incluye preguntas de diferentes niveles de dificultad\n",
        "            - Especifica el puntaje de cada pregunta\n",
        "            - Proporciona una guía de calificación con respuestas modelo\n",
        "\n",
        "            El examen debe evaluar diferentes niveles de la taxonomía de Bloom,\n",
        "            desde el conocimiento hasta la evaluación y la creación.\n",
        "            \"\"\"\n",
        "        elif assessment_type == \"project\":\n",
        "            prompt = f\"\"\"\n",
        "            Diseña un proyecto de evaluación para el tema \"{topic}\" a nivel {level}.\n",
        "\n",
        "            El proyecto debe permitir a los estudiantes demostrar su comprensión y aplicación del conocimiento sobre los siguientes subtemas: {subtopics_text}\n",
        "\n",
        "            Incluye:\n",
        "            1. Descripción general del proyecto\n",
        "            2. Objetivos de aprendizaje específicos\n",
        "            3. Instrucciones detalladas paso a paso\n",
        "            4. Cronograma sugerido con hitos\n",
        "            5. Criterios de evaluación (rúbrica detallada)\n",
        "            6. Recursos recomendados\n",
        "            7. Sugerencias para la presentación final\n",
        "\n",
        "            El proyecto debe ser:\n",
        "            - Auténtico y conectado con problemas del mundo real\n",
        "            - Adaptable a diferentes estilos de aprendizaje\n",
        "            - Escalable para permitir tanto profundidad como amplitud\n",
        "            - Diseñado para fomentar habilidades de pensamiento crítico y creatividad\n",
        "            \"\"\"\n",
        "        elif assessment_type == \"rubric\":\n",
        "            prompt = f\"\"\"\n",
        "            Desarrolla una rúbrica de evaluación detallada para evaluar el aprendizaje sobre el tema \"{topic}\".\n",
        "\n",
        "            La rúbrica debe cubrir los siguientes subtemas: {subtopics_text}\n",
        "\n",
        "            Estructura de la rúbrica:\n",
        "            1. Al menos 5 criterios de evaluación principales\n",
        "            2. 4 niveles de desempeño para cada criterio (Ejemplar, Competente, En desarrollo, Inicial)\n",
        "            3. Descriptores detallados para cada nivel de desempeño\n",
        "\n",
        "            Para cada criterio:\n",
        "            - Proporciona una descripción clara\n",
        "            - Especifica qué aspectos del aprendizaje se evalúan\n",
        "            - Incluye indicadores observables de desempeño\n",
        "            - Asigna un porcentaje o peso relativo\n",
        "\n",
        "            La rúbrica debe evaluar tanto el contenido (precisión, profundidad, amplitud) como\n",
        "            habilidades (análisis, aplicación, comunicación, pensamiento crítico).\n",
        "            \"\"\"\n",
        "        else:\n",
        "            # Por defecto, generar un cuestionario básico\n",
        "            prompt = f\"\"\"\n",
        "            Crea una evaluación general para el tema \"{topic}\" que incluya diferentes tipos de preguntas.\n",
        "            Asegúrate de cubrir los siguientes subtemas: {subtopics_text}\n",
        "\n",
        "            La evaluación debe ser apropiada para el nivel {level} y debe incluir una variedad de formatos\n",
        "            de preguntas que evalúen diferentes niveles de aprendizaje.\n",
        "            \"\"\"\n",
        "\n",
        "        # Generar la evaluación\n",
        "        assessment = self.llm_engine.generate_content(prompt, f\"assessment_{assessment_type}\", 6000)\n",
        "        return assessment\n",
        "\n",
        "    def generate_differentiated_materials(self,\n",
        "                                         topic: str,\n",
        "                                         material_type: str = \"lecture_notes\",\n",
        "                                         difficulty_levels: List[str] = None) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Genera materiales diferenciados para distintos niveles de habilidad.\n",
        "\n",
        "        Args:\n",
        "            topic: Tema principal.\n",
        "            material_type: Tipo de material a generar.\n",
        "            difficulty_levels: Lista de niveles de dificultad.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con materiales por nivel de dificultad.\n",
        "        \"\"\"\n",
        "        if difficulty_levels is None:\n",
        "            difficulty_levels = [\"basic\", \"intermediate\", \"advanced\"]\n",
        "\n",
        "        self.logger.info(f\"Generando materiales diferenciados para '{topic}' de tipo '{material_type}'\")\n",
        "\n",
        "        results = {}\n",
        "        for level in difficulty_levels:\n",
        "            prompt = f\"\"\"\n",
        "            Crea material educativo sobre \"{topic}\" de tipo {material_type} adaptado específicamente\n",
        "            para estudiantes de nivel {level}.\n",
        "\n",
        "            Si el nivel es \"basic\":\n",
        "            - Usa lenguaje más sencillo y directo\n",
        "            - Proporciona más explicaciones y ejemplos\n",
        "            - Enfócate en los conceptos fundamentales\n",
        "            - Incluye más ayudas visuales (descriptivas)\n",
        "            - Estructura el contenido en pasos más pequeños\n",
        "\n",
        "            Si el nivel es \"intermediate\":\n",
        "            - Profundiza más en los conceptos\n",
        "            - Incluye algunas aplicaciones más complejas\n",
        "            - Establece conexiones entre conceptos\n",
        "            - Introduce más terminología especializada\n",
        "            - Fomenta más análisis y pensamiento crítico\n",
        "\n",
        "            Si el nivel es \"advanced\":\n",
        "            - Incluye contenido más sofisticado y matizado\n",
        "            - Aborda excepciones y casos especiales\n",
        "            - Incorpora referencias a investigaciones actuales\n",
        "            - Fomenta la síntesis y evaluación de ideas\n",
        "            - Plantea preguntas más abstractas y teóricas\n",
        "\n",
        "            Manteniendo siempre el rigor académico apropiado.\n",
        "            \"\"\"\n",
        "\n",
        "            # Generar el material para este nivel\n",
        "            level_content = self.llm_engine.generate_content(prompt, material_type, 4000)\n",
        "            results[level] = level_content\n",
        "\n",
        "        return results\n",
        "\n",
        "    def critique_and_improve_material(self, content: str, content_type: str) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Analiza críticamente y mejora un material educativo existente.\n",
        "\n",
        "        Args:\n",
        "            content: Contenido a mejorar.\n",
        "            content_type: Tipo de contenido.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con crítica y contenido mejorado.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Realizando crítica y mejora de material tipo '{content_type}'\")\n",
        "\n",
        "        # Prompt para la crítica\n",
        "        critique_prompt = f\"\"\"\n",
        "        Analiza críticamente el siguiente material educativo de tipo '{content_type}':\n",
        "\n",
        "        {content}\n",
        "\n",
        "        Proporciona una evaluación detallada considerando:\n",
        "        1. Precisión académica y rigor conceptual\n",
        "        2. Claridad y accesibilidad de las explicaciones\n",
        "        3. Organización y estructura lógica\n",
        "        4. Profundidad y amplitud de la cobertura\n",
        "        5. Relevancia y aplicabilidad de los ejemplos\n",
        "        6. Potencial para promover el pensamiento crítico\n",
        "        7. Adaptabilidad a diferentes estilos de aprendizaje\n",
        "\n",
        "        Identifica tanto fortalezas como áreas de mejora. Proporciona recomendaciones\n",
        "        específicas y accionables para mejorar el material.\n",
        "        \"\"\"\n",
        "\n",
        "        # Generar la crítica\n",
        "        critique = self.llm_engine.generate_content(critique_prompt, \"critique\", 3000)\n",
        "\n",
        "        # Prompt para la mejora\n",
        "        improvement_prompt = f\"\"\"\n",
        "        Mejora el siguiente material educativo basándote en la crítica proporcionada:\n",
        "\n",
        "        MATERIAL ORIGINAL:\n",
        "        {content}\n",
        "\n",
        "        CRÍTICA Y RECOMENDACIONES:\n",
        "        {critique}\n",
        "\n",
        "        Desarrolla una versión mejorada que aborde todas las áreas de mejora identificadas,\n",
        "        manteniendo y potenciando las fortalezas. Asegúrate de que el material mejorado:\n",
        "\n",
        "        1. Corrija cualquier imprecisión o ambigüedad\n",
        "        2. Mejore la claridad de las explicaciones\n",
        "        3. Optimice la estructura y organización\n",
        "        4. Profundice en áreas que necesitan mayor desarrollo\n",
        "        5. Incluya ejemplos más relevantes y aplicables\n",
        "        6. Fomente mejor el pensamiento crítico\n",
        "        7. Sea más adaptable a diferentes estilos de aprendizaje\n",
        "\n",
        "        Proporciona el material completo mejorado, no solo los cambios.\n",
        "        \"\"\"\n",
        "\n",
        "        # Generar el material mejorado\n",
        "        improved_content = self.llm_engine.generate_content(improvement_prompt, content_type, 6000)\n",
        "\n",
        "        return {\n",
        "            \"original\": content,\n",
        "            \"critique\": critique,\n",
        "            \"improved\": improved_content\n",
        "        }\n",
        "\n",
        "    def export_all_materials(self, materials: Dict[str, str], export_format: str = \"markdown\") -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Exporta todos los materiales generados al formato especificado.\n",
        "\n",
        "        Args:\n",
        "            materials: Diccionario con los materiales por tipo.\n",
        "            export_format: Formato de exportación (markdown, html, text).\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con los materiales formateados por tipo.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Exportando materiales a formato '{export_format}'\")\n",
        "\n",
        "        exported = {}\n",
        "        for content_type, content in materials.items():\n",
        "            exported[content_type] = self.llm_engine.format_content_for_export(\n",
        "                content, content_type, export_format\n",
        "            )\n",
        "\n",
        "        return exported\n",
        "\n",
        "    def generate_complete_module(self,\n",
        "                                topic: str,\n",
        "                                subtopics: List[str],\n",
        "                                level: str = \"undergraduate\") -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Genera un módulo educativo completo para un tema.\n",
        "\n",
        "        Args:\n",
        "            topic: Tema principal del módulo.\n",
        "            subtopics: Lista de subtemas.\n",
        "            level: Nivel educativo.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con todos los componentes del módulo.\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Generando módulo completo para el tema '{topic}'\")\n",
        "\n",
        "        # Generar todos los materiales necesarios para un módulo completo\n",
        "        module = {}\n",
        "\n",
        "        # 1. Objetivos de aprendizaje y descripción general\n",
        "        module_intro_prompt = f\"\"\"\n",
        "        Crea una introducción completa para un módulo educativo sobre \"{topic}\" para nivel {level}.\n",
        "\n",
        "        La introducción debe incluir:\n",
        "        1. Descripción general del módulo (4-5 párrafos)\n",
        "        2. Objetivos de aprendizaje específicos y medibles (8-10 objetivos)\n",
        "        3. Prerrequisitos y conocimientos previos recomendados\n",
        "        4. Visión general de los subtemas que se cubrirán: {', '.join(subtopics)}\n",
        "        5. Relevancia y aplicaciones prácticas del tema\n",
        "\n",
        "        Escribe la introducción en un estilo claro, motivador y académicamente riguroso.\n",
        "        \"\"\"\n",
        "\n",
        "        module[\"introduction\"] = self.llm_engine.generate_content(module_intro_prompt, \"introduction\", 3000)\n",
        "\n",
        "        # 2. Materiales principales\n",
        "        material_types = [\"lecture_notes\", \"practice_problems\", \"discussion_questions\"]\n",
        "        module[\"materials\"] = self.generate_course_material(\n",
        "            topic, subtopics, material_types, level, max_tokens_per_type=5000\n",
        "        )\n",
        "\n",
        "        # 3. Plan de lección\n",
        "        module[\"lesson_plan\"] = self.generate_lesson_plan(topic, 120, level)\n",
        "\n",
        "        # 4. Evaluaciones\n",
        "        assessment_types = [\"quiz\", \"rubric\"]\n",
        "        module[\"assessments\"] = {}\n",
        "        for assessment_type in assessment_types:\n",
        "            module[\"assessments\"][assessment_type] = self.generate_assessment(\n",
        "                topic, subtopics, assessment_type, level\n",
        "            )\n",
        "\n",
        "        # 5. Recursos complementarios\n",
        "        resources_prompt = f\"\"\"\n",
        "        Crea una lista exhaustiva de recursos complementarios para apoyar el aprendizaje\n",
        "        sobre el tema \"{topic}\" para estudiantes de nivel {level}.\n",
        "\n",
        "        Incluye:\n",
        "        1. Bibliografía comentada (libros de texto y referencias académicas)\n",
        "        2. Recursos en línea (sitios web, videos, simulaciones)\n",
        "        3. Artículos y publicaciones científicas relevantes\n",
        "        4. Herramientas y aplicaciones útiles\n",
        "        5. Oportunidades de aprendizaje experiencial\n",
        "\n",
        "        Para cada recurso, proporciona:\n",
        "        - Información completa de referencia/acceso\n",
        "        - Breve descripción de su contenido y relevancia\n",
        "        - Indicación de para qué subtemas es más útil: {', '.join(subtopics)}\n",
        "        - Nivel de dificultad o accesibilidad\n",
        "\n",
        "        Organiza los recursos por categoría y relevancia.\n",
        "        \"\"\"\n",
        "\n",
        "        module[\"resources\"] = self.llm_engine.generate_content(resources_prompt, \"resources\", 4000)\n",
        "\n",
        "        # 6. Guía del instructor\n",
        "        instructor_guide_prompt = f\"\"\"\n",
        "        Desarrolla una guía completa para instructores que enseñarán el tema \"{topic}\"\n",
        "        a estudiantes de nivel {level}.\n",
        "\n",
        "        La guía debe incluir:\n",
        "        1. Notas pedagógicas sobre puntos clave y conceptos difíciles\n",
        "        2. Estrategias de enseñanza recomendadas para cada subtema: {', '.join(subtopics)}\n",
        "        3. Posibles conceptos erróneos de los estudiantes y cómo abordarlos\n",
        "        4. Actividades adicionales y extensiones sugeridas\n",
        "        5. Consejos para la gestión del tiempo y priorización del contenido\n",
        "        6. Estrategias de evaluación formativa y sumativa\n",
        "        7. Adaptaciones para diferentes contextos (presencial, en línea, híbrido)\n",
        "\n",
        "        La guía debe ser práctica, específica y basada en principios pedagógicos sólidos.\n",
        "        \"\"\"\n",
        "\n",
        "        module[\"instructor_guide\"] = self.llm_engine.generate_content(\n",
        "            instructor_guide_prompt, \"instructor_guide\", 5000\n",
        "        )\n",
        "\n",
        "        return module\n",
        "\n",
        "\n",
        "    def extract_syllabus(self, pdf_path: str) -> Dict[str, Any]:\n",
        "          \"\"\"\n",
        "          Extrae la estructura del syllabus de un PDF de curso.\n",
        "\n",
        "          Args:\n",
        "              pdf_path: Ruta al archivo PDF del curso\n",
        "\n",
        "          Returns:\n",
        "              Dict con la estructura del syllabus que incluye:\n",
        "              - description: Descripción general del curso\n",
        "              - objectives: Lista de objetivos de aprendizaje\n",
        "              - topics: Lista de temas, cada uno con:\n",
        "                - id: Identificador único\n",
        "                - title: Título del tema\n",
        "                - subtopics: Lista de subtemas\n",
        "              - bibliography: Lista de referencias bibliográficas\n",
        "          \"\"\"\n",
        "          logging.info(f\"Extrayendo syllabus del PDF: {pdf_path}\")\n",
        "\n",
        "          # Extraer el texto completo del PDF\n",
        "          text = self._extract_text_from_pdf(pdf_path)\n",
        "\n",
        "          # Inicializar la estructura del syllabus\n",
        "          syllabus_data = {\n",
        "              \"description\": \"\",\n",
        "              \"objectives\": [],\n",
        "              \"topics\": [],\n",
        "              \"bibliography\": []\n",
        "          }\n",
        "\n",
        "          # Extraer la descripción del curso\n",
        "          syllabus_data[\"description\"] = self._extract_description(text)\n",
        "\n",
        "          # Extraer los objetivos de aprendizaje\n",
        "          syllabus_data[\"objectives\"] = self._extract_objectives(text)\n",
        "\n",
        "          # Extraer los temas y subtemas\n",
        "          syllabus_data[\"topics\"] = self._extract_topics(text)\n",
        "\n",
        "          # Extraer la bibliografía\n",
        "          syllabus_data[\"bibliography\"] = self._extract_bibliography(text)\n",
        "\n",
        "          logging.info(f\"Syllabus extraído exitosamente con {len(syllabus_data['topics'])} temas\")\n",
        "          return syllabus_data\n",
        "\n",
        "    def _extract_text_from_pdf(self, pdf_path: str) -> str:\n",
        "        \"\"\"Extrae el texto completo de un archivo PDF.\"\"\"\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            text = \"\"\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error al extraer texto del PDF {pdf_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def _extract_description(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Extrae la descripción del curso del texto del PDF.\n",
        "        Busca secciones como \"Descripción del curso\", \"Presentación\", etc.\n",
        "        \"\"\"\n",
        "        patterns = [\n",
        "            r\"(?:Descripción|Presentación|Introducción|Descripción general|Sobre el curso)(?:\\s+del curso)?:(.*?)(?=\\n\\n\\s*(?:[A-Z]|Objetivo|Tema|Unidad|Bibliografía))\",\n",
        "            r\"(?:Descripción|Presentación|Introducción|Descripción general|Sobre el curso)(?:\\s+del curso)?(?:\\n+)(.*?)(?=\\n\\n\\s*(?:[A-Z]|Objetivo|Tema|Unidad|Bibliografía))\"\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "            if match:\n",
        "                description = match.group(1).strip()\n",
        "                # Limitar a los primeros 5 párrafos si es muy largo\n",
        "                paragraphs = [p.strip() for p in description.split(\"\\n\\n\") if p.strip()]\n",
        "                if len(paragraphs) > 5:\n",
        "                    description = \"\\n\\n\".join(paragraphs[:5])\n",
        "                return description\n",
        "\n",
        "        # Si no se encuentra una descripción específica, usar los primeros párrafos\n",
        "        paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
        "        if paragraphs:\n",
        "            return paragraphs[0]\n",
        "\n",
        "        return \"No se encontró una descripción clara del curso.\"\n",
        "\n",
        "    def _extract_objectives(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extrae los objetivos de aprendizaje del texto del PDF.\n",
        "        Busca secciones como \"Objetivos\", \"Propósitos\", etc.\n",
        "        \"\"\"\n",
        "        patterns = [\n",
        "            r\"(?:Objetivos|Propósitos|Metas|Competencias|Resultados de aprendizaje)(?:\\s+del curso)?:(.*?)(?=\\n\\n\\s*(?:[A-Z]|Tema|Unidad|Bibliografía))\",\n",
        "            r\"(?:Objetivos|Propósitos|Metas|Competencias|Resultados de aprendizaje)(?:\\s+del curso)?(?:\\n+)(.*?)(?=\\n\\n\\s*(?:[A-Z]|Tema|Unidad|Bibliografía))\"\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "            if match:\n",
        "                objectives_text = match.group(1).strip()\n",
        "                # Buscar objetivos con viñetas o números\n",
        "                bullet_objectives = re.findall(r\"(?:•|\\*|\\-|\\d+\\.)\\s*(.*?)(?=\\n\\s*(?:•|\\*|\\-|\\d+\\.|$)|\\Z)\", objectives_text, re.DOTALL)\n",
        "                if bullet_objectives:\n",
        "                    return [obj.strip() for obj in bullet_objectives if obj.strip()]\n",
        "\n",
        "                # Si no hay viñetas, dividir por oraciones\n",
        "                sentences = sent_tokenize(objectives_text)\n",
        "                return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "        return [\"No se encontraron objetivos específicos del curso.\"]\n",
        "\n",
        "    def _extract_topics(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Extrae los temas y subtemas del texto del PDF.\n",
        "        Busca secciones como \"Temario\", \"Contenido\", \"Unidades\", etc.\n",
        "        \"\"\"\n",
        "        # Patrones para encontrar la sección de temas\n",
        "        section_patterns = [\n",
        "            r\"(?:Temario|Contenido|Unidades|Programa|Temas|Contenidos)(?:\\s+del curso)?:(.*?)(?=\\n\\n\\s*(?:Bibliografía|Referencias|Evaluación|Metodología|$))\",\n",
        "            r\"(?:Temario|Contenido|Unidades|Programa|Temas|Contenidos)(?:\\s+del curso)?(?:\\n+)(.*?)(?=\\n\\n\\s*(?:Bibliografía|Referencias|Evaluación|Metodología|$))\"\n",
        "        ]\n",
        "\n",
        "        topics_section = \"\"\n",
        "        for pattern in section_patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "            if match:\n",
        "                topics_section = match.group(1).strip()\n",
        "                break\n",
        "\n",
        "        if not topics_section:\n",
        "            # Si no se encuentra una sección específica, intentar extraer por estructura\n",
        "            # (Esto es un respaldo y puede no ser preciso)\n",
        "            paragraphs = text.split(\"\\n\\n\")\n",
        "            middle_index = len(paragraphs) // 2\n",
        "            topics_section = \"\\n\\n\".join(paragraphs[middle_index-3:middle_index+3])\n",
        "\n",
        "        # Extraer temas principales (pueden estar numerados o con formato especial)\n",
        "        topics = []\n",
        "        topic_pattern = r\"(?:Tema|Unidad|Módulo)?\\s*(\\d+|[IVX]+)[\\.\\:]\\s*(.*?)(?=\\n)\"\n",
        "\n",
        "        # Buscar temas con formato específico\n",
        "        topic_matches = re.finditer(topic_pattern, topics_section, re.IGNORECASE)\n",
        "\n",
        "        has_structured_topics = False\n",
        "        for i, match in enumerate(topic_matches):\n",
        "            has_structured_topics = True\n",
        "            topic_id = f\"topic_{i+1}\"\n",
        "            topic_title = match.group(2).strip()\n",
        "\n",
        "            # Encontrar el texto hasta el siguiente tema o fin de sección\n",
        "            end_pos = topics_section.find(f\"Tema {i+2}\", match.end())\n",
        "            end_pos = topics_section.find(f\"Unidad {i+2}\", match.end()) if end_pos == -1 else end_pos\n",
        "            end_pos = len(topics_section) if end_pos == -1 else end_pos\n",
        "\n",
        "            topic_text = topics_section[match.end():end_pos].strip()\n",
        "\n",
        "            # Extraer subtemas (pueden estar con viñetas, letras o números)\n",
        "            subtopics = re.findall(r\"(?:•|\\*|\\-|[a-z]\\.|\\d+\\.\\d+)\\s*(.*?)(?=\\n\\s*(?:•|\\*|\\-|[a-z]\\.|\\d+\\.\\d+|$)|\\Z)\", topic_text, re.DOTALL)\n",
        "\n",
        "            # Si no hay subtemas con formato, dividir por líneas no vacías\n",
        "            if not subtopics:\n",
        "                subtopics = [line.strip() for line in topic_text.split(\"\\n\") if line.strip() and not line.startswith(\"Tema\") and not line.startswith(\"Unidad\")]\n",
        "\n",
        "            topics.append({\n",
        "                \"id\": topic_id,\n",
        "                \"title\": topic_title,\n",
        "                \"subtopics\": [s.strip() for s in subtopics if s.strip()]\n",
        "            })\n",
        "\n",
        "        # Si no se encuentran temas estructurados, hacer una división básica\n",
        "        if not has_structured_topics:\n",
        "            paragraphs = [p.strip() for p in topics_section.split(\"\\n\\n\") if p.strip()]\n",
        "            for i, paragraph in enumerate(paragraphs[:5]):  # Limitar a 5 temas\n",
        "                topic_title = paragraph.split(\"\\n\")[0].strip()\n",
        "                subtopics = []\n",
        "\n",
        "                # Tomar líneas del párrafo (excepto la primera) como subtemas\n",
        "                lines = paragraph.split(\"\\n\")[1:]\n",
        "                if lines:\n",
        "                    subtopics = [line.strip() for line in lines if line.strip()]\n",
        "                else:\n",
        "                    # Si no hay líneas, tomar la primera oración como título y las demás como subtemas\n",
        "                    sentences = sent_tokenize(topic_title)\n",
        "                    if len(sentences) > 1:\n",
        "                        topic_title = sentences[0]\n",
        "                        subtopics = sentences[1:]\n",
        "\n",
        "                topics.append({\n",
        "                    \"id\": f\"topic_{i+1}\",\n",
        "                    \"title\": topic_title,\n",
        "                    \"subtopics\": subtopics\n",
        "                })\n",
        "\n",
        "        # Asegurar que al menos hay un tema\n",
        "        if not topics:\n",
        "            topics = [{\n",
        "                \"id\": \"topic_1\",\n",
        "                \"title\": \"Tema principal del curso\",\n",
        "                \"subtopics\": [\"Subtema 1\", \"Subtema 2\", \"Subtema 3\"]\n",
        "            }]\n",
        "\n",
        "        return topics\n",
        "\n",
        "    def _extract_bibliography(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extrae la bibliografía del texto del PDF.\n",
        "        Busca secciones como \"Bibliografía\", \"Referencias\", etc.\n",
        "        \"\"\"\n",
        "        patterns = [\n",
        "            r\"(?:Bibliografía|Referencias|Fuentes|Material de referencia)(?:\\s+del curso)?:(.*?)(?=\\n\\n\\s*(?:[A-Z]|$))\",\n",
        "            r\"(?:Bibliografía|Referencias|Fuentes|Material de referencia)(?:\\s+del curso)?(?:\\n+)(.*?)(?=\\n\\n\\s*(?:[A-Z]|$))\"\n",
        "        ]\n",
        "\n",
        "        for pattern in patterns:\n",
        "            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n",
        "            if match:\n",
        "                bibliography_text = match.group(1).strip()\n",
        "                # Buscar referencias con viñetas o números\n",
        "                bullet_refs = re.findall(r\"(?:•|\\*|\\-|\\d+\\.)\\s*(.*?)(?=\\n\\s*(?:•|\\*|\\-|\\d+\\.|$)|\\Z)\", bibliography_text, re.DOTALL)\n",
        "                if bullet_refs:\n",
        "                    return [ref.strip() for ref in bullet_refs if ref.strip()]\n",
        "\n",
        "                # Si no hay viñetas, dividir por líneas y filtrar vacías\n",
        "                lines = [line.strip() for line in bibliography_text.split(\"\\n\") if line.strip()]\n",
        "                return lines\n",
        "\n",
        "        return [\"No se encontraron referencias bibliográficas específicas.\"]"
      ],
      "metadata": {
        "id": "i-JTdR0CiZe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WR3BUxlle7o",
        "outputId": "446c2598-e238-4e83-e217-988f2d42c4a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.5 pypdfium2-4.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extrae el texto de un documento PDF.\n",
        "\n",
        "    Args:\n",
        "        pdf_path: Ruta al archivo PDF.\n",
        "\n",
        "    Returns:\n",
        "        Texto extraído del PDF.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text += page.extract_text() or \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error al extraer texto del PDF: {str(e)}\")\n",
        "    return text"
      ],
      "metadata": {
        "id": "U7SYcTvsleEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración del generador\n",
        "config = Config()\n",
        "generator = EducationalContentGenerator(config)\n",
        "\n",
        "# Procesar un PDF\n",
        "pdf_path = \"PROGRAMA_DE_CURSO.pdf\"\n",
        "materials = generator.generate_content_from_pdf(\n",
        "    pdf_path=pdf_path,\n",
        "    content_types=[\"lecture_notes\", \"practice_problems\", \"discussion_questions\"],\n",
        "    level=\"undergraduate\"\n",
        ")\n",
        "\n",
        "# Imprimir o guardar los resultados\n",
        "for content_type, content in materials.items():\n",
        "    print(f\"\\n--- {content_type.upper()} ---\\n\")\n",
        "    print(content[:500] + \"...\")  # Mostrar vista previa\n",
        "\n",
        "    # Opcional: guardar en archivos\n",
        "    with open(f\"{content_type}.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I4apdGi2li49",
        "outputId": "b268f2b3-340c-4637-b4c0-df660ad46180"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:educational_agent.llm_engine:No se pudieron cargar las plantillas desde templates/prompts.json: [Errno 2] No such file or directory: 'templates/prompts.json'. Se usarán plantillas por defecto.\n",
            "ERROR:educational_agent.llm_engine:Error al generar contenido con Gemini: HTTPConnectionPool(host='localhost', port=34507): Read timed out. (read timeout=600.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-44-be0f6a6dc766>\", line 437, in generate_content\n",
            "    response = self.model.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\", line 331, in generate_content\n",
            "    response = self._client.generate_content(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 293, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 153, in retry_target\n",
            "    _retry_error_helper(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_base.py\", line 212, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\", line 144, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\", line 76, in error_remapped_callable\n",
            "    return callable_(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1148, in __call__\n",
            "    response = GenerativeServiceRestTransport._GenerateContent._get_response(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\", line 1048, in _get_response\n",
            "    response = getattr(session, method)(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\", line 537, in request\n",
            "    response = super(AuthorizedSession, self).request(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/requests/adapters.py\", line 713, in send\n",
            "    raise ReadTimeout(e, request=request)\n",
            "requests.exceptions.ReadTimeout: HTTPConnectionPool(host='localhost', port=34507): Read timed out. (read timeout=600.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-145eabf0848b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Procesar un PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpdf_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"PROGRAMA_DE_CURSO.pdf\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m materials = generator.generate_content_from_pdf(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mpdf_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcontent_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lecture_notes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"practice_problems\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"discussion_questions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-be0f6a6dc766>\u001b[0m in \u001b[0;36mgenerate_content_from_pdf\u001b[0;34m(self, pdf_path, content_types, level, max_tokens_per_type, max_retries, retry_delay)\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m               \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                   \u001b[0manalysis_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content_analysis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m                   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m               \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_post_retry_check_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RetryCallState\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_run_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-be0f6a6dc766>\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, prompt, content_type, max_tokens)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Llamando a la API de Gemini con la configuración definida.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             response = self.model.generate_content(\n\u001b[0m\u001b[1;32m    438\u001b[0m                 \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"parts\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfull_prompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             )\n\u001b[0;32m--> 293\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msleep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msleep_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m             response = GenerativeServiceRestTransport._GenerateContent._get_response(\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_host\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(host, metadata, query_params, session, timeout, transcoded_request, body)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             response = getattr(session, method)(\n\u001b[0m\u001b[1;32m   1049\u001b[0m                 \u001b[0;34m\"{host}{uri}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \"\"\"\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    538\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fpdf"
      ],
      "metadata": {
        "id": "_7TBYXKi2I84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = FPDF()\n",
        "pdf.add_page()\n",
        "pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "# Encabezado\n",
        "pdf.cell(0, 10, txt=\"Contenido educativo generado\", ln=1, align=\"C\")\n",
        "\n",
        "# Agregar cada bloque de texto al PDF\n",
        "for content_type, content in materials.items():\n",
        "    pdf.ln(10)  # Espacio vertical\n",
        "    pdf.set_font(\"Arial\", \"B\", 14)\n",
        "    pdf.cell(0, 10, txt=content_type.upper(), ln=1)\n",
        "    pdf.set_font(\"Arial\", size=12)\n",
        "\n",
        "    # Para manejar saltos de línea en PDF, podemos usar multi_cell:\n",
        "    pdf.multi_cell(0, 10, content)\n",
        "\n",
        "# Guardar en un archivo PDF\n",
        "pdf.output(\"contenido_generado.pdf\")\n",
        "print(\"Se ha creado el archivo 'contenido_generado.pdf' con los materiales.\")"
      ],
      "metadata": {
        "id": "y8QsYWpt2Fhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración del generador\n",
        "config = Config()\n",
        "generator = EducationalContentGenerator(config)\n",
        "evaluator = Evaluator(config)\n",
        "\n",
        "# Procesar un PDF\n",
        "pdf_path = \"curso.pdf\"\n",
        "\n",
        "# Extraer el syllabus primero\n",
        "syllabus_data = generator.extract_syllabus(pdf_path)\n",
        "\n",
        "# Generar el contenido educativo\n",
        "materials = generator.generate_content_from_pdf(\n",
        "    pdf_path=pdf_path,\n",
        "    content_types=[\"lecture_notes\", \"practice_problems\", \"discussion_questions\",\n",
        "                   \"learning_objectives\", \"suggested_resources\"],\n",
        "    level=\"undergraduate\"\n",
        ")\n",
        "\n",
        "# Reestructurar los materiales para la evaluación\n",
        "# Asumiendo que los materiales se generan como diccionarios:\n",
        "# {content_type: content_text, ...}\n",
        "# Necesitamos convertirlos a la estructura:\n",
        "# {topic_id: {content_type: content_text, ...}, ...}\n",
        "\n",
        "structured_materials = {}\n",
        "\n",
        "# Opción 1: Si los materiales ya están organizados por temas\n",
        "if isinstance(materials, dict) and any(isinstance(v, dict) for v in materials.values()):\n",
        "    structured_materials = materials\n",
        "else:\n",
        "    # Opción 2: Si los materiales están organizados por tipo de contenido,\n",
        "    # necesitamos dividirlos por temas\n",
        "\n",
        "    # Primero, crear una estructura vacía para cada tema\n",
        "    for topic in syllabus_data[\"topics\"]:\n",
        "        structured_materials[topic[\"id\"]] = {}\n",
        "\n",
        "    # Función para identificar el tema al que pertenece un contenido\n",
        "    def identify_topic(content_text, topics):\n",
        "        best_match = None\n",
        "        best_score = 0\n",
        "\n",
        "        for topic in topics:\n",
        "            # Calcular cuántas palabras clave del tema aparecen en el contenido\n",
        "            topic_words = topic[\"title\"].lower().split()\n",
        "            count = sum(1 for word in topic_words if word in content_text.lower())\n",
        "\n",
        "            # Verificar subtemas también\n",
        "            for subtopic in topic[\"subtopics\"]:\n",
        "                subtopic_words = subtopic.lower().split()\n",
        "                count += sum(1 for word in subtopic_words if word in content_text.lower())\n",
        "\n",
        "            if count > best_score:\n",
        "                best_score = count\n",
        "                best_match = topic[\"id\"]\n",
        "\n",
        "        return best_match\n",
        "\n",
        "    # Para cada tipo de contenido, dividir por temas\n",
        "    for content_type, content in materials.items():\n",
        "        # Si el contenido ya está dividido por temas\n",
        "        if isinstance(content, dict):\n",
        "            for topic_id, topic_content in content.items():\n",
        "                if topic_id in structured_materials:\n",
        "                    structured_materials[topic_id][content_type] = topic_content\n",
        "        else:\n",
        "            # Para cada tipo de contenido, dividir por temas\n",
        "          for content_type, content in materials.items():\n",
        "              # Si el contenido ya está dividido por temas\n",
        "              if isinstance(content, dict):\n",
        "                  for topic_id, topic_content in content.items():\n",
        "                      if topic_id in structured_materials:\n",
        "                          structured_materials[topic_id][content_type] = topic_content\n",
        "              else:\n",
        "                  # Si el contenido es un solo texto largo, intentar dividirlo por temas\n",
        "                  content_sections = self._divide_content_by_topics(content, syllabus_data[\"topics\"])\n",
        "\n",
        "                  for topic_id, topic_content in content_sections.items():\n",
        "                      structured_materials[topic_id][content_type] = topic_content\n",
        "\n",
        "def divide_content_by_topics(content: str, topics: List[Dict[str, Any]]) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Divide un contenido largo en secciones por tema.\n",
        "\n",
        "    Args:\n",
        "        content: Texto completo del contenido\n",
        "        topics: Lista de temas del syllabus\n",
        "\n",
        "    Returns:\n",
        "        Diccionario con contenido dividido por topic_id\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "\n",
        "    # Si hay marcadores explícitos de temas en el contenido\n",
        "    topic_markers = {}\n",
        "    for topic in topics:\n",
        "        # Crear patrones de búsqueda para cada tema\n",
        "        title_words = topic[\"title\"].lower().split()\n",
        "        if title_words:\n",
        "            pattern = r\"(?:Tema|Unidad|Módulo)?\\s*(?:\\d+|[IVX]+)?[\\.\\:]?\\s*(\" + \"|\".join(title_words) + r\")\"\n",
        "\n",
        "            matches = list(re.finditer(pattern, content, re.IGNORECASE))\n",
        "            if matches:\n",
        "                topic_markers[topic[\"id\"]] = matches[0].start()\n",
        "\n",
        "    # Si encontramos marcadores, dividir el contenido\n",
        "    if topic_markers:\n",
        "        sorted_markers = sorted(topic_markers.items(), key=lambda x: x[1])\n",
        "\n",
        "        for i, (topic_id, start_pos) in enumerate(sorted_markers):\n",
        "            end_pos = sorted_markers[i+1][1] if i < len(sorted_markers)-1 else len(content)\n",
        "            result[topic_id] = content[start_pos:end_pos].strip()\n",
        "    else:\n",
        "        # Si no hay marcadores, dividir el contenido en partes iguales\n",
        "        num_topics = len(topics)\n",
        "        if num_topics > 0:\n",
        "            chunk_size = len(content) // num_topics\n",
        "\n",
        "            for i, topic in enumerate(topics):\n",
        "                start_pos = i * chunk_size\n",
        "                end_pos = (i+1) * chunk_size if i < num_topics-1 else len(content)\n",
        "                result[topic[\"id\"]] = content[start_pos:end_pos].strip()\n",
        "        else:\n",
        "            # Si no hay temas, devolver todo el contenido bajo un ID genérico\n",
        "            result[\"topic_1\"] = content\n",
        "\n",
        "    return result\n",
        "\n",
        "def extract_syllabus_from_text(text_content):\n",
        "    \"\"\"\n",
        "    Extrae un syllabus simplificado de un texto para pruebas.\n",
        "    \"\"\"\n",
        "    # Syllabus básico de ejemplo\n",
        "    return {\n",
        "        \"description\": \"Este curso aborda los fundamentos de la programación educativa.\",\n",
        "        \"objectives\": [\n",
        "            \"Entender los principios básicos de la programación\",\n",
        "            \"Aplicar conceptos a problemas del mundo real\",\n",
        "            \"Desarrollar habilidades de resolución de problemas\"\n",
        "        ],\n",
        "        \"topics\": [\n",
        "            {\n",
        "                \"id\": \"topic_1\",\n",
        "                \"title\": \"Introducción a la programación\",\n",
        "                \"subtopics\": [\"Variables y tipos de datos\", \"Estructuras de control\", \"Funciones\"]\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"topic_2\",\n",
        "                \"title\": \"Estructuras de datos\",\n",
        "                \"subtopics\": [\"Listas y arrays\", \"Diccionarios\", \"Conjuntos\"]\n",
        "            },\n",
        "            {\n",
        "                \"id\": \"topic_3\",\n",
        "                \"title\": \"Programación orientada a objetos\",\n",
        "                \"subtopics\": [\"Clases y objetos\", \"Herencia\", \"Polimorfismo\"]\n",
        "            }\n",
        "        ],\n",
        "        \"bibliography\": [\n",
        "            \"Python for Everybody, Charles R. Severance\",\n",
        "            \"Clean Code, Robert C. Martin\",\n",
        "            \"Fluent Python, Luciano Ramalho\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Ejecutar la evaluación\n",
        "evaluation_results = evaluator.evaluate_content(\n",
        "    generated_content=structured_materials,\n",
        "    syllabus_data=syllabus_data\n",
        ")\n",
        "\n",
        "def generate_sample_materials():\n",
        "    \"\"\"\n",
        "    Genera materiales educativos de ejemplo para pruebas.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"lecture_notes\": \"# Introducción a la programación\\n\\nEn esta unidad vamos a estudiar los conceptos básicos...\",\n",
        "        \"practice_problems\": \"1. Crea un programa que calcule el promedio de tres números.\\n2. Implementa un algoritmo para determinar si un número es primo.\",\n",
        "        \"discussion_questions\": \"¿Cómo afecta la elección del lenguaje de programación al diseño de un sistema?\",\n",
        "        \"learning_objectives\": \"Al finalizar esta unidad, el estudiante será capaz de implementar algoritmos básicos.\",\n",
        "        \"suggested_resources\": \"1. Python Documentation\\n2. Curso online: CS50\"\n",
        "    }\n",
        "\n",
        "# Ejecución principal\n",
        "try:\n",
        "    # Intentar leer un PDF (esto probablemente fallará si no existe el archivo)\n",
        "    pdf_path = \"curso.pdf\"\n",
        "\n",
        "    try:\n",
        "        # Intentamos abrir el PDF\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text_content = \"\"\n",
        "        for page in doc:\n",
        "            text_content += page.get_text()\n",
        "        syllabus_data = extract_syllabus_from_text(text_content)\n",
        "    except Exception as e:\n",
        "        # Si falla, usamos datos de ejemplo\n",
        "        print(f\"Error al leer el PDF: {e}\")\n",
        "        print(\"Usando datos de ejemplo para la demostración...\")\n",
        "        syllabus_data = extract_syllabus_from_text(\"\")\n",
        "\n",
        "    # Generar materiales (usar datos de ejemplo para la demostración)\n",
        "    materials = generate_sample_materials()\n",
        "\n",
        "    # Reestructurar los materiales para la evaluación\n",
        "    structured_materials = {}\n",
        "\n",
        "    # Opción 1: Si los materiales ya están organizados por temas\n",
        "    if isinstance(materials, dict) and any(isinstance(v, dict) for v in materials.values()):\n",
        "        structured_materials = materials\n",
        "    else:\n",
        "        # Opción 2: Si los materiales están organizados por tipo de contenido,\n",
        "        # dividirlos por temas\n",
        "\n",
        "        # Primero, crear una estructura vacía para cada tema\n",
        "        for topic in syllabus_data[\"topics\"]:\n",
        "            structured_materials[topic[\"id\"]] = {}\n",
        "\n",
        "        # Para cada tipo de contenido, dividir por temas\n",
        "        for content_type, content in materials.items():\n",
        "            # Si el contenido ya está dividido por temas\n",
        "            if isinstance(content, dict):\n",
        "                for topic_id, topic_content in content.items():\n",
        "                    if topic_id in structured_materials:\n",
        "                        structured_materials[topic_id][content_type] = topic_content\n",
        "            else:\n",
        "                # Si el contenido es un solo texto largo, intentar dividirlo por temas\n",
        "                content_sections = divide_content_by_topics(content, syllabus_data[\"topics\"])\n",
        "\n",
        "                for topic_id, topic_content in content_sections.items():\n",
        "                    structured_materials[topic_id][content_type] = topic_content\n",
        "\n",
        "    # Inicializar el evaluador\n",
        "    config = Config()\n",
        "    evaluator = Evaluator(config)\n",
        "\n",
        "    # Ejecutar la evaluación\n",
        "    evaluation_results = evaluator.evaluate_content(\n",
        "        generated_content=structured_materials,\n",
        "        syllabus_data=syllabus_data\n",
        "    )\n",
        "\n",
        "    # Imprimir los resultados de evaluación\n",
        "    print(\"\\n--- RESULTADOS DE EVALUACIÓN ---\\n\")\n",
        "    print(f\"Puntuación global: {evaluation_results['average_score']:.2f}\")\n",
        "    print(\"\\nPuntuaciones por tipo de contenido:\")\n",
        "    for content_type, score in evaluation_results[\"content_type_scores\"].items():\n",
        "        print(f\"- {content_type}: {score:.2f}\")\n",
        "\n",
        "    print(\"\\nMétricas globales:\")\n",
        "    for metric, score in evaluation_results[\"overall_metrics\"].items():\n",
        "        print(f\"- {metric}: {score:.2f}\")\n",
        "\n",
        "    print(\"\\nPuntuaciones por tema:\")\n",
        "    for topic_id, scores in evaluation_results[\"topic_scores\"].items():\n",
        "        print(f\"\\nTema {topic_id}:\")\n",
        "        for content_type, score in scores.items():\n",
        "            print(f\"  - {content_type}: {score:.2f}\")\n",
        "\n",
        "    # Guardar los resultados en un archivo JSON\n",
        "    with open(\"evaluation_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(evaluation_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error durante la ejecución: {e}\")\n",
        "\n",
        "# Imprimir los resultados de evaluación\n",
        "print(\"\\n--- RESULTADOS DE EVALUACIÓN ---\\n\")\n",
        "print(f\"Puntuación global: {evaluation_results['average_score']:.2f}\")\n",
        "print(\"\\nPuntuaciones por tipo de contenido:\")\n",
        "for content_type, score in evaluation_results[\"content_type_scores\"].items():\n",
        "    print(f\"- {content_type}: {score:.2f}\")\n",
        "\n",
        "print(\"\\nMétricas globales:\")\n",
        "for metric, score in evaluation_results[\"overall_metrics\"].items():\n",
        "    print(f\"- {metric}: {score:.2f}\")\n",
        "\n",
        "print(\"\\nPuntuaciones por tema:\")\n",
        "for topic_id, scores in evaluation_results[\"topic_scores\"].items():\n",
        "    print(f\"\\nTema {topic_id}:\")\n",
        "    for content_type, score in scores.items():\n",
        "        print(f\"  - {content_type}: {score:.2f}\")\n",
        "\n",
        "# Guardar los resultados en un archivo JSON\n",
        "import json\n",
        "with open(\"evaluation_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(evaluation_results, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Generar un informe detallado de la evaluación\n",
        "def generate_evaluation_report(evaluation_results, syllabus_data):\n",
        "    \"\"\"\n",
        "    Genera un informe detallado de la evaluación en formato Markdown.\n",
        "    \"\"\"\n",
        "    report = \"# Informe de Evaluación del Contenido Educativo\\n\\n\"\n",
        "\n",
        "    # Información general del curso\n",
        "    report += \"## Información del Curso\\n\\n\"\n",
        "    report += f\"**Descripción:** {syllabus_data['description'][:200]}...\\n\\n\"\n",
        "    report += f\"**Número de temas:** {len(syllabus_data['topics'])}\\n\\n\"\n",
        "\n",
        "    # Resultados generales\n",
        "    report += \"## Resultados Generales\\n\\n\"\n",
        "    report += f\"**Puntuación global:** {evaluation_results['average_score']:.2f}/1.00\\n\\n\"\n",
        "\n",
        "    # Gráfico de puntuaciones (representado como texto)\n",
        "    report += \"### Puntuaciones por Tipo de Contenido\\n\\n\"\n",
        "    report += \"```\\n\"\n",
        "    for content_type, score in evaluation_results[\"content_type_scores\"].items():\n",
        "        bar = \"█\" * int(score * 20)\n",
        "        report += f\"{content_type.ljust(25)}: {bar} {score:.2f}\\n\"\n",
        "    report += \"```\\n\\n\"\n",
        "\n",
        "    # Métricas globales\n",
        "    report += \"### Métricas Globales\\n\\n\"\n",
        "    report += \"| Métrica | Puntuación |\\n\"\n",
        "    report += \"|---------|------------|\\n\"\n",
        "    for metric, score in evaluation_results[\"overall_metrics\"].items():\n",
        "        report += f\"| {metric.replace('_', ' ').title()} | {score:.2f} |\\n\"\n",
        "    report += \"\\n\"\n",
        "\n",
        "    # Resultados por tema\n",
        "    report += \"## Resultados por Tema\\n\\n\"\n",
        "    for topic_id, scores in evaluation_results[\"topic_scores\"].items():\n",
        "        # Encontrar el título del tema\n",
        "        topic_title = next((t[\"title\"] for t in syllabus_data[\"topics\"] if t[\"id\"] == topic_id), topic_id)\n",
        "        report += f\"### {topic_title}\\n\\n\"\n",
        "\n",
        "        report += \"| Tipo de Contenido | Puntuación |\\n\"\n",
        "        report += \"|-------------------|------------|\\n\"\n",
        "        for content_type, score in scores.items():\n",
        "            if content_type != \"average\":\n",
        "                report += f\"| {content_type.replace('_', ' ').title()} | {score:.2f} |\\n\"\n",
        "        report += f\"| **Promedio del tema** | **{scores.get('average', 0):.2f}** |\\n\\n\"\n",
        "\n",
        "    # Recomendaciones para mejora\n",
        "    report += \"## Recomendaciones para Mejora\\n\\n\"\n",
        "\n",
        "    # Identificar áreas más débiles\n",
        "    weak_areas = []\n",
        "    for content_type, score in evaluation_results[\"content_type_scores\"].items():\n",
        "        if score < 0.7:\n",
        "            weak_areas.append((content_type, score))\n",
        "\n",
        "    if weak_areas:\n",
        "        report += \"### Áreas de mejora prioritarias\\n\\n\"\n",
        "        for content_type, score in sorted(weak_areas, key=lambda x: x[1]):\n",
        "            report += f\"- **{content_type.replace('_', ' ').title()}** ({score:.2f}): \"\n",
        "\n",
        "            if content_type == \"lecture_notes\":\n",
        "                report += \"Mejorar la cobertura de subtemas y aumentar el uso de ejemplos prácticos.\\n\"\n",
        "            elif content_type == \"practice_problems\":\n",
        "                report += \"Incluir más problemas con diferentes niveles de dificultad y asegurarse de incluir soluciones detalladas.\\n\"\n",
        "            elif content_type == \"discussion_questions\":\n",
        "                report += \"Formular preguntas más abiertas que fomenten el pensamiento crítico.\\n\"\n",
        "            elif content_type == \"learning_objectives\":\n",
        "                report += \"Usar verbos de la taxonomía de Bloom y hacer los objetivos más medibles.\\n\"\n",
        "            elif content_type == \"suggested_resources\":\n",
        "                report += \"Diversificar los tipos de recursos y proporcionar más detalles sobre cada uno.\\n\"\n",
        "    else:\n",
        "        report += \"El contenido generado tiene una buena calidad general. Para mejorar aún más:\\n\\n\"\n",
        "        report += \"- Considerar la inclusión de más ejemplos prácticos en las notas de clase\\n\"\n",
        "        report += \"- Diversificar los tipos de problemas de práctica\\n\"\n",
        "        report += \"- Reforzar las conexiones entre los diferentes temas\\n\"\n",
        "\n",
        "    return report\n",
        "\n",
        "# Generar y guardar el informe\n",
        "report = generate_evaluation_report(evaluation_results, syllabus_data)\n",
        "with open(\"evaluation_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(report)\n",
        "print(f\"\\nInforme de evaluación guardado en evaluation_report.md\")\n",
        "\n",
        "# Imprimir o guardar los materiales generados\n",
        "for content_type, content in materials.items():\n",
        "    print(f\"\\n--- {content_type.upper()} ---\\n\")\n",
        "    print(content[:500] + \"...\")  # Mostrar vista previa\n",
        "\n",
        "    # Opcional: guardar en archivos\n",
        "    with open(f\"{content_type}.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "7coTQN6Dq78_",
        "outputId": "792670c1-4d50-4342-b661-682c401fb88b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:educational_agent.llm_engine:No se pudieron cargar las plantillas desde templates/prompts.json: [Errno 2] No such file or directory: 'templates/prompts.json'. Se usarán plantillas por defecto.\n",
            "ERROR:root:Error al extraer texto del PDF curso.pdf: name 'fitz' is not defined\n",
            "ERROR:educational_agent.content_generator:Error al procesar el PDF: [Errno 2] No such file or directory: 'curso.pdf'\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'self' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-9aed4ea98792>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m               \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                   \u001b[0;31m# Si el contenido es un solo texto largo, intentar dividirlo por temas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                   \u001b[0mcontent_sections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_divide_content_by_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyllabus_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"topics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                   \u001b[0;32mfor\u001b[0m \u001b[0mtopic_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_content\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontent_sections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from typing import Dict, List, Any\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Asegurarse de que NLTK cuente con los recursos necesarios\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Configurar logging a nivel de módulo\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# =============================================================================\n",
        "# Clase Evaluator: Evaluación del contenido educativo generado\n",
        "# =============================================================================\n",
        "class Evaluator:\n",
        "    \"\"\"Evalúa la calidad del contenido educativo generado utilizando diversas métricas.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Any) -> None:\n",
        "        \"\"\"\n",
        "        Inicializa el evaluador con la configuración dada.\n",
        "\n",
        "        Args:\n",
        "            config: Configuración general (se puede ampliar según las necesidades).\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.logger = logging.getLogger(\"educational_agent.evaluator\")\n",
        "\n",
        "    def evaluate_content(\n",
        "        self,\n",
        "        generated_content: Dict[str, Dict[str, str]],\n",
        "        syllabus_data: Dict[str, Any]\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evalúa el contenido generado mediante varias métricas:\n",
        "          - Relevancia (cobertura de subtemas)\n",
        "          - Consistencia (similitud de coseno entre secciones)\n",
        "          - Legibilidad (longitud promedio de oraciones)\n",
        "          - Uso de terminología específica (basada en TF-IDF)\n",
        "\n",
        "        Args:\n",
        "            generated_content: Diccionario con materiales generados, organizados por tema y tipo.\n",
        "            syllabus_data: Datos estructurados del syllabus del curso.\n",
        "\n",
        "        Returns:\n",
        "            Diccionario con los resultados de la evaluación.\n",
        "        \"\"\"\n",
        "        self.logger.info(\"Iniciando evaluación del contenido generado\")\n",
        "\n",
        "        evaluation_results: Dict[str, Any] = {\n",
        "            \"topic_scores\": {},\n",
        "            \"content_type_scores\": {\n",
        "                \"lecture_notes\": 0,\n",
        "                \"practice_problems\": 0,\n",
        "                \"discussion_questions\": 0,\n",
        "                \"learning_objectives\": 0,\n",
        "                \"suggested_resources\": 0\n",
        "            },\n",
        "            \"overall_metrics\": {\n",
        "                \"relevance_score\": 0,\n",
        "                \"consistency_score\": 0,\n",
        "                \"readability_score\": 0,\n",
        "                \"domain_terminology_score\": 0\n",
        "            },\n",
        "            \"average_score\": 0\n",
        "        }\n",
        "\n",
        "        # Extraer terminología clave del syllabus para evaluar su uso en el contenido\n",
        "        course_terminology = self._extract_domain_terminology(syllabus_data)\n",
        "\n",
        "        # Evaluar cada tema individualmente\n",
        "        for topic_id, topic_content in generated_content.items():\n",
        "            # Buscar información del tema en el syllabus (por ID)\n",
        "            topic_info = next((t for t in syllabus_data[\"topics\"] if t[\"id\"] == topic_id), None)\n",
        "            if not topic_info:\n",
        "                self.logger.warning(f\"No se encontró información para el tema {topic_id} en el syllabus\")\n",
        "                continue\n",
        "\n",
        "            # Extraer textos por tipo de contenido\n",
        "            lecture = topic_content.get(\"lecture_notes\", \"\")\n",
        "            practice = topic_content.get(\"practice_problems\", \"\")\n",
        "            discussion = topic_content.get(\"discussion_questions\", \"\")\n",
        "            objectives = topic_content.get(\"learning_objectives\", \"\")\n",
        "            resources = topic_content.get(\"suggested_resources\", \"\")\n",
        "\n",
        "            # Calcular puntuaciones para cada tipo de contenido\n",
        "            topic_scores: Dict[str, float] = {\n",
        "                \"lecture_notes\": self._evaluate_lecture_notes(lecture, topic_info, course_terminology),\n",
        "                \"practice_problems\": self._evaluate_practice_problems(practice, topic_info),\n",
        "                \"discussion_questions\": self._evaluate_discussion_questions(discussion, topic_info),\n",
        "                \"learning_objectives\": self._evaluate_learning_objectives(objectives, topic_info),\n",
        "                \"suggested_resources\": self._evaluate_suggested_resources(resources, topic_info)\n",
        "            }\n",
        "\n",
        "            # Calcular la puntuación promedio para el tema\n",
        "            topic_avg = sum(topic_scores.values()) / len(topic_scores)\n",
        "            topic_scores[\"average\"] = topic_avg\n",
        "            evaluation_results[\"topic_scores\"][topic_id] = topic_scores\n",
        "\n",
        "            # Acumular las puntuaciones para cada tipo de contenido\n",
        "            for key, score in topic_scores.items():\n",
        "                if key != \"average\" and key in evaluation_results[\"content_type_scores\"]:\n",
        "                    evaluation_results[\"content_type_scores\"][key] += score\n",
        "\n",
        "        # Promediar las puntuaciones por tipo de contenido\n",
        "        num_topics = len(generated_content)\n",
        "        if num_topics > 0:\n",
        "            for key in evaluation_results[\"content_type_scores\"]:\n",
        "                evaluation_results[\"content_type_scores\"][key] /= num_topics\n",
        "\n",
        "        # Calcular métricas globales basadas en todo el contenido generado\n",
        "        all_texts = self._join_all_texts(generated_content)\n",
        "        overall_consistency = self._evaluate_consistency(generated_content)\n",
        "        overall_readability = self._calculate_readability(all_texts)\n",
        "        overall_terminology = self._calculate_terminology_usage(all_texts, course_terminology)\n",
        "\n",
        "        evaluation_results[\"overall_metrics\"][\"consistency_score\"] = overall_consistency\n",
        "        evaluation_results[\"overall_metrics\"][\"readability_score\"] = overall_readability\n",
        "        evaluation_results[\"overall_metrics\"][\"domain_terminology_score\"] = overall_terminology\n",
        "\n",
        "        # Relevancia: promedio de cobertura de subtemas en las notas de clase\n",
        "        overall_relevance = np.mean([\n",
        "            self._calculate_subtopic_coverage(topic_content.get(\"lecture_notes\", \"\"), topic_info[\"subtopics\"])\n",
        "            for topic_id, topic_content in generated_content.items()\n",
        "            if topic_id in [t[\"id\"] for t in syllabus_data[\"topics\"]]\n",
        "        ])\n",
        "        evaluation_results[\"overall_metrics\"][\"relevance_score\"] = overall_relevance\n",
        "\n",
        "        # Calcular la puntuación global promedio\n",
        "        content_type_avg = np.mean(list(evaluation_results[\"content_type_scores\"].values()))\n",
        "        overall_metrics_avg = np.mean(list(evaluation_results[\"overall_metrics\"].values()))\n",
        "        evaluation_results[\"average_score\"] = (content_type_avg + overall_metrics_avg) / 2\n",
        "\n",
        "        self.logger.info(f\"Evaluación completada. Puntuación promedio: {evaluation_results['average_score']:.2f}\")\n",
        "        return evaluation_results\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Métodos auxiliares para análisis global del contenido\n",
        "    # -------------------------------------------------------------------------\n",
        "    def _join_all_texts(self, generated_content: Dict[str, Dict[str, str]]) -> str:\n",
        "        \"\"\"Une todo el texto de todos los temas y secciones para análisis global.\"\"\"\n",
        "        texts: List[str] = []\n",
        "        for topic_content in generated_content.values():\n",
        "            texts.extend(topic_content.values())\n",
        "        return \"\\n\".join(texts)\n",
        "\n",
        "    def _calculate_readability(self, text: str) -> float:\n",
        "        \"\"\"\n",
        "        Calcula la legibilidad basada en la longitud promedio de las oraciones.\n",
        "        Se asume que oraciones más cortas facilitan la lectura.\n",
        "        \"\"\"\n",
        "        sentences = sent_tokenize(text)\n",
        "        words = word_tokenize(text)\n",
        "        if not sentences:\n",
        "            return 0.0\n",
        "        avg_sentence_length = len(words) / len(sentences)\n",
        "        if avg_sentence_length <= 15:\n",
        "            return 1.0\n",
        "        elif avg_sentence_length >= 30:\n",
        "            return 0.0\n",
        "        else:\n",
        "            return (30 - avg_sentence_length) / 15\n",
        "\n",
        "    def _extract_domain_terminology(self, syllabus_data: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extrae términos clave del syllabus combinando la descripción, objetivos, temario y bibliografía.\n",
        "\n",
        "        Returns:\n",
        "            Lista de términos extraídos mediante TF-IDF.\n",
        "        \"\"\"\n",
        "        combined_text = syllabus_data.get(\"description\", \"\")\n",
        "        for obj in syllabus_data.get(\"objectives\", []):\n",
        "            combined_text += \" \" + obj\n",
        "        for topic in syllabus_data.get(\"topics\", []):\n",
        "            combined_text += \" \" + topic.get(\"title\", \"\")\n",
        "            for subtopic in topic.get(\"subtopics\", []):\n",
        "                combined_text += \" \" + subtopic\n",
        "        for bib in syllabus_data.get(\"bibliography\", []):\n",
        "            combined_text += \" \" + bib\n",
        "\n",
        "        vectorizer = TfidfVectorizer(max_features=50, stop_words='english', ngram_range=(1, 2))\n",
        "        try:\n",
        "            tfidf_matrix = vectorizer.fit_transform([combined_text])\n",
        "            feature_names = vectorizer.get_feature_names_out()\n",
        "            return list(feature_names)\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"Error extrayendo terminología: {e}\")\n",
        "            return []\n",
        "\n",
        "    def _calculate_terminology_usage(self, text: str, terminology: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calcula la densidad de uso de la terminología específica en el contenido.\n",
        "\n",
        "        Returns:\n",
        "            Valor entre 0 y 1 indicando la densidad.\n",
        "        \"\"\"\n",
        "        content_lower = text.lower()\n",
        "        term_count = sum(1 for term in terminology if term.lower() in content_lower)\n",
        "        word_count = len(word_tokenize(text))\n",
        "        term_density = (term_count * 1000) / word_count if word_count > 0 else 0\n",
        "        # Se asume que 5 términos por cada 1000 palabras es ideal\n",
        "        return min(1.0, term_density / 5)\n",
        "\n",
        "    def _calculate_subtopic_coverage(self, content: str, subtopics: List[str]) -> float:\n",
        "        \"\"\"\n",
        "        Calcula la proporción de subtemas cubiertos en el contenido.\n",
        "\n",
        "        Returns:\n",
        "            Valor entre 0 y 1 indicando la cobertura.\n",
        "        \"\"\"\n",
        "        content_lower = content.lower()\n",
        "        covered = 0\n",
        "        for sub in subtopics:\n",
        "            sub_terms = [w.lower() for w in word_tokenize(sub) if len(w) > 3]\n",
        "            if sub_terms and (sum(1 for term in sub_terms if term in content_lower) / len(sub_terms)) >= 0.5:\n",
        "                covered += 1\n",
        "        return covered / len(subtopics) if subtopics else 0\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Métodos de evaluación específicos para cada tipo de contenido\n",
        "    # -------------------------------------------------------------------------\n",
        "    def _evaluate_lecture_notes(self, lecture_notes: str, topic_info: Dict[str, Any], course_terminology: List[str]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(lecture_notes, topic_info.get(\"subtopics\", [])),\n",
        "            self._calculate_terminology_usage(lecture_notes, course_terminology),\n",
        "            self._calculate_readability(lecture_notes),\n",
        "            self._evaluate_content_structure(lecture_notes),\n",
        "            self._evaluate_examples_presence(lecture_notes)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    def _evaluate_practice_problems(self, practice_problems: str, topic_info: Dict[str, Any]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(practice_problems, topic_info.get(\"subtopics\", [])),\n",
        "            self._evaluate_solutions_presence(practice_problems),\n",
        "            self._evaluate_difficulty_variety(practice_problems),\n",
        "            self._evaluate_problem_clarity(practice_problems)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    def _evaluate_discussion_questions(self, discussion_questions: str, topic_info: Dict[str, Any]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(discussion_questions, topic_info.get(\"subtopics\", [])),\n",
        "            self._evaluate_critical_thinking(discussion_questions),\n",
        "            self._evaluate_open_ended_questions(discussion_questions)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    def _evaluate_learning_objectives(self, learning_objectives: str, topic_info: Dict[str, Any]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(learning_objectives, topic_info.get(\"subtopics\", [])),\n",
        "            self._evaluate_bloom_taxonomy_usage(learning_objectives),\n",
        "            self._evaluate_measurable_objectives(learning_objectives)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    def _evaluate_suggested_resources(self, suggested_resources: str, topic_info: Dict[str, Any]) -> float:\n",
        "        scores = [\n",
        "            self._calculate_subtopic_coverage(suggested_resources, topic_info.get(\"subtopics\", [])),\n",
        "            self._evaluate_resource_variety(suggested_resources),\n",
        "            self._evaluate_resource_detail(suggested_resources)\n",
        "        ]\n",
        "        return sum(scores) / len(scores)\n",
        "\n",
        "    # -------------------------------------------------------------------------\n",
        "    # Métodos \"stub\" o simples para evaluación adicional\n",
        "    # -------------------------------------------------------------------------\n",
        "    def _evaluate_content_structure(self, text: str) -> float:\n",
        "        paragraphs = [p for p in text.split(\"\\n\\n\") if p.strip()]\n",
        "        num_paragraphs = len(paragraphs)\n",
        "        return 1.0 if num_paragraphs >= 5 else (num_paragraphs / 5.0 if num_paragraphs else 0.0)\n",
        "\n",
        "    def _evaluate_examples_presence(self, text: str) -> float:\n",
        "        return 1.0 if \"ejemplo\" in text.lower() else 0.0\n",
        "\n",
        "    def _evaluate_solutions_presence(self, text: str) -> float:\n",
        "        return 1.0 if (\"solución\" in text.lower() or \"solución:\" in text.lower()) else 0.0\n",
        "\n",
        "    def _evaluate_difficulty_variety(self, text: str) -> float:\n",
        "        # Valor fijo de ejemplo; puede ser mejorado con un análisis más detallado.\n",
        "        return 0.8\n",
        "\n",
        "    def _evaluate_problem_clarity(self, text: str) -> float:\n",
        "        # Ejemplo simple: se asume claridad si el texto tiene cierta puntuación.\n",
        "        return 0.8\n",
        "\n",
        "    def _evaluate_critical_thinking(self, text: str) -> float:\n",
        "        keywords = [\"analiza\", \"discute\", \"reflexiona\", \"argumenta\"]\n",
        "        count = sum(1 for kw in keywords if kw in text.lower())\n",
        "        return min(1.0, count / len(keywords))\n",
        "\n",
        "    def _evaluate_open_ended_questions(self, text: str) -> float:\n",
        "        questions = [line for line in text.splitlines() if \"?\" in line]\n",
        "        if not questions:\n",
        "            return 0.0\n",
        "        open_count = sum(1 for q in questions if not re.search(r'\\b(?:sí|no)\\b', q.lower()))\n",
        "        return open_count / len(questions)\n",
        "\n",
        "    def _evaluate_bloom_taxonomy_usage(self, text: str) -> float:\n",
        "        bloom_verbs = [\"analiza\", \"aplica\", \"compara\", \"evalúa\", \"crea\", \"sintetiza\", \"interpreta\"]\n",
        "        count = sum(1 for verb in bloom_verbs if verb in text.lower())\n",
        "        return min(1.0, count / len(bloom_verbs))\n",
        "\n",
        "    def _evaluate_measurable_objectives(self, text: str) -> float:\n",
        "        return 0.8 if any(kw in text.lower() for kw in [\"porcentaje\", \"número\", \"cuantifica\"]) else 0.5\n",
        "\n",
        "    def _evaluate_resource_variety(self, text: str) -> float:\n",
        "        lines = [l for l in text.splitlines() if l.strip()]\n",
        "        return min(1.0, len(lines) / 5.0)\n",
        "\n",
        "    def _evaluate_resource_detail(self, text: str) -> float:\n",
        "        return 0.8\n",
        "\n",
        "    def _evaluate_consistency(self, generated_content: Dict[str, Dict[str, str]]) -> float:\n",
        "        \"\"\"\n",
        "        Evalúa la consistencia semántica entre temas usando TF-IDF y similitud de coseno.\n",
        "        Se asume que mayor similitud entre temas indica mayor consistencia.\n",
        "        \"\"\"\n",
        "        texts: List[str] = []\n",
        "        for topic_content in generated_content.values():\n",
        "            combined = \" \".join(topic_content.values())\n",
        "            texts.append(combined)\n",
        "        if len(texts) < 2:\n",
        "            return 1.0\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "        tfidf = vectorizer.fit_transform(texts)\n",
        "        similarity_matrix = cosine_similarity(tfidf)\n",
        "        n = similarity_matrix.shape[0]\n",
        "        sum_sim = np.sum(similarity_matrix) - n  # Excluir la diagonal\n",
        "        num_elements = n * (n - 1)\n",
        "        avg_similarity = sum_sim / num_elements if num_elements > 0 else 1.0\n",
        "        return avg_similarity\n",
        "\n",
        "# =============================================================================\n",
        "# Ejemplo de evaluación (se asume que 'config', 'contenido' y 'syllabus_data' están definidos)\n",
        "# =============================================================================\n",
        "# Crear una instancia del evaluador (la variable 'config' debe estar definida en otro módulo)\n",
        "evaluator = Evaluator(config)\n",
        "evaluacion = evaluator.evaluate_content(content, syllabus_data)\n",
        "print(evaluacion)"
      ],
      "metadata": {
        "id": "qsc04YLforeg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}